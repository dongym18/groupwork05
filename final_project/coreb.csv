457c899653991 kernel/sched/core.c (Thomas Gleixner            2019-05-19 13:08:55 +0100    1) // SPDX-License-Identifier: GPL-2.0-only
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700    2) /*
391e43da797a9 kernel/sched/core.c (Peter Zijlstra             2011-11-15 17:14:39 +0100    3)  *  kernel/sched/core.c
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700    4)  *
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100    5)  *  Core kernel scheduler code and related syscalls
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700    6)  *
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700    7)  *  Copyright (C) 1991-2002  Linus Torvalds
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700    8)  */
325ea10c08094 kernel/sched/core.c (Ingo Molnar                2018-03-03 12:20:47 +0100    9) #include "sched.h"
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700   10) 
7281c8dec8a87 kernel/sched/core.c (Peter Zijlstra             2018-04-20 14:29:51 +0200   11) #include <linux/nospec.h>
85f1abe0019fc kernel/sched/core.c (Peter Zijlstra             2018-05-01 18:14:45 +0200   12) 
0ed557aa81392 kernel/sched/core.c (Mark Rutland               2018-06-14 15:27:41 -0700   13) #include <linux/kcov.h>
0ed557aa81392 kernel/sched/core.c (Mark Rutland               2018-06-14 15:27:41 -0700   14) 
96f951edb1f1b kernel/sched/core.c (David Howells              2012-03-28 18:30:03 +0100   15) #include <asm/switch_to.h>
5517d86bea237 kernel/sched.c      (Eric Dumazet               2007-05-08 00:32:57 -0700   16) #include <asm/tlb.h>
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700   17) 
ea138446e51f7 kernel/sched/core.c (Tejun Heo                  2013-01-18 14:05:55 -0800   18) #include "../workqueue_internal.h"
771b53d033e86 kernel/sched/core.c (Jens Axboe                 2019-10-22 10:25:58 -0600   19) #include "../../fs/io-wq.h"
29d5e0476e1c4 kernel/sched/core.c (Thomas Gleixner            2012-04-20 13:05:45 +0000   20) #include "../smpboot.h"
6e0534f278199 kernel/sched.c      (Gregory Haskins            2008-05-12 21:21:01 +0200   21) 
91c27493e78df kernel/sched/core.c (Vincent Guittot            2018-06-28 17:45:09 +0200   22) #include "pelt.h"
91c27493e78df kernel/sched/core.c (Vincent Guittot            2018-06-28 17:45:09 +0200   23) 
a8d154b009168 kernel/sched.c      (Steven Rostedt             2009-04-10 09:36:00 -0400   24) #define CREATE_TRACE_POINTS
ad8d75fff811a kernel/sched.c      (Steven Rostedt             2009-04-14 19:39:12 -0400   25) #include <trace/events/sched.h>
a8d154b009168 kernel/sched.c      (Steven Rostedt             2009-04-10 09:36:00 -0400   26) 
a056a5bed7fa6 kernel/sched/core.c (Qais Yousef                2019-06-04 12:14:59 +0100   27) /*
a056a5bed7fa6 kernel/sched/core.c (Qais Yousef                2019-06-04 12:14:59 +0100   28)  * Export tracepoints that act as a bare tracehook (ie: have no trace event
a056a5bed7fa6 kernel/sched/core.c (Qais Yousef                2019-06-04 12:14:59 +0100   29)  * associated with them) to allow external modules to probe them.
a056a5bed7fa6 kernel/sched/core.c (Qais Yousef                2019-06-04 12:14:59 +0100   30)  */
a056a5bed7fa6 kernel/sched/core.c (Qais Yousef                2019-06-04 12:14:59 +0100   31) EXPORT_TRACEPOINT_SYMBOL_GPL(pelt_cfs_tp);
a056a5bed7fa6 kernel/sched/core.c (Qais Yousef                2019-06-04 12:14:59 +0100   32) EXPORT_TRACEPOINT_SYMBOL_GPL(pelt_rt_tp);
a056a5bed7fa6 kernel/sched/core.c (Qais Yousef                2019-06-04 12:14:59 +0100   33) EXPORT_TRACEPOINT_SYMBOL_GPL(pelt_dl_tp);
a056a5bed7fa6 kernel/sched/core.c (Qais Yousef                2019-06-04 12:14:59 +0100   34) EXPORT_TRACEPOINT_SYMBOL_GPL(pelt_irq_tp);
a056a5bed7fa6 kernel/sched/core.c (Qais Yousef                2019-06-04 12:14:59 +0100   35) EXPORT_TRACEPOINT_SYMBOL_GPL(pelt_se_tp);
a056a5bed7fa6 kernel/sched/core.c (Qais Yousef                2019-06-04 12:14:59 +0100   36) EXPORT_TRACEPOINT_SYMBOL_GPL(sched_overutilized_tp);
a056a5bed7fa6 kernel/sched/core.c (Qais Yousef                2019-06-04 12:14:59 +0100   37) 
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200   38) DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
dc61b1d65e353 kernel/sched.c      (Peter Zijlstra             2010-06-08 11:40:42 +0200   39) 
e9666d10a5677 kernel/sched/core.c (Masahiro Yamada            2018-12-31 00:14:15 +0900   40) #if defined(CONFIG_SCHED_DEBUG) && defined(CONFIG_JUMP_LABEL)
bf5c91ba8c629 kernel/sched.c      (Ingo Molnar                2007-10-15 17:00:04 +0200   41) /*
bf5c91ba8c629 kernel/sched.c      (Ingo Molnar                2007-10-15 17:00:04 +0200   42)  * Debugging: various feature bits
765cc3a4b224e kernel/sched/core.c (Patrick Bellasi            2017-11-08 18:41:01 +0000   43)  *
765cc3a4b224e kernel/sched/core.c (Patrick Bellasi            2017-11-08 18:41:01 +0000   44)  * If SCHED_DEBUG is disabled, each compilation unit has its own copy of
765cc3a4b224e kernel/sched/core.c (Patrick Bellasi            2017-11-08 18:41:01 +0000   45)  * sysctl_sched_features, defined in sched.h, to allow constants propagation
765cc3a4b224e kernel/sched/core.c (Patrick Bellasi            2017-11-08 18:41:01 +0000   46)  * at compile time and compiler optimization based on features default.
bf5c91ba8c629 kernel/sched.c      (Ingo Molnar                2007-10-15 17:00:04 +0200   47)  */
f00b45c145981 kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200   48) #define SCHED_FEAT(name, enabled)	\
f00b45c145981 kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200   49) 	(1UL << __SCHED_FEAT_##name) * enabled |
bf5c91ba8c629 kernel/sched.c      (Ingo Molnar                2007-10-15 17:00:04 +0200   50) const_debug unsigned int sysctl_sched_features =
391e43da797a9 kernel/sched/core.c (Peter Zijlstra             2011-11-15 17:14:39 +0100   51) #include "features.h"
f00b45c145981 kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200   52) 	0;
f00b45c145981 kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200   53) #undef SCHED_FEAT
765cc3a4b224e kernel/sched/core.c (Patrick Bellasi            2017-11-08 18:41:01 +0000   54) #endif
f00b45c145981 kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200   55) 
b82d9fdd848ab kernel/sched.c      (Peter Zijlstra             2007-11-09 22:39:39 +0100   56) /*
b82d9fdd848ab kernel/sched.c      (Peter Zijlstra             2007-11-09 22:39:39 +0100   57)  * Number of tasks to iterate in a single balance run.
b82d9fdd848ab kernel/sched.c      (Peter Zijlstra             2007-11-09 22:39:39 +0100   58)  * Limited because this is done with IRQs disabled.
b82d9fdd848ab kernel/sched.c      (Peter Zijlstra             2007-11-09 22:39:39 +0100   59)  */
b82d9fdd848ab kernel/sched.c      (Peter Zijlstra             2007-11-09 22:39:39 +0100   60) const_debug unsigned int sysctl_sched_nr_migrate = 32;
b82d9fdd848ab kernel/sched.c      (Peter Zijlstra             2007-11-09 22:39:39 +0100   61) 
fa85ae2418e68 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100   62) /*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100   63)  * period over which we measure -rt task CPU usage in us.
fa85ae2418e68 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100   64)  * default: 1s
fa85ae2418e68 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100   65)  */
9f0c1e560c433 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:39 +0100   66) unsigned int sysctl_sched_rt_period = 1000000;
fa85ae2418e68 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100   67) 
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200   68) __read_mostly int scheduler_running;
6892b75e60557 kernel/sched.c      (Ingo Molnar                2008-02-13 14:02:36 +0100   69) 
9f0c1e560c433 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:39 +0100   70) /*
9f0c1e560c433 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:39 +0100   71)  * part of the period that we allow rt tasks to run in us.
9f0c1e560c433 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:39 +0100   72)  * default: 0.95s
9f0c1e560c433 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:39 +0100   73)  */
9f0c1e560c433 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:39 +0100   74) int sysctl_sched_rt_runtime = 950000;
fa85ae2418e68 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100   75) 
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   76) /*
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   77)  * __task_rq_lock - lock the rq @p resides on.
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   78)  */
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200   79) struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   80) 	__acquires(rq->lock)
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   81) {
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   82) 	struct rq *rq;
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   83) 
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   84) 	lockdep_assert_held(&p->pi_lock);
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   85) 
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   86) 	for (;;) {
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   87) 		rq = task_rq(p);
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   88) 		raw_spin_lock(&rq->lock);
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   89) 		if (likely(rq == task_rq(p) && !task_on_rq_migrating(p))) {
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100   90) 			rq_pin_lock(rq, rf);
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   91) 			return rq;
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   92) 		}
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   93) 		raw_spin_unlock(&rq->lock);
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   94) 
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   95) 		while (unlikely(task_on_rq_migrating(p)))
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   96) 			cpu_relax();
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   97) 	}
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   98) }
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200   99) 
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  100) /*
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  101)  * task_rq_lock - lock p->pi_lock and lock the rq @p resides on.
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  102)  */
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200  103) struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  104) 	__acquires(p->pi_lock)
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  105) 	__acquires(rq->lock)
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  106) {
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  107) 	struct rq *rq;
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  108) 
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  109) 	for (;;) {
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200  110) 		raw_spin_lock_irqsave(&p->pi_lock, rf->flags);
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  111) 		rq = task_rq(p);
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  112) 		raw_spin_lock(&rq->lock);
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  113) 		/*
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  114) 		 *	move_queued_task()		task_rq_lock()
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  115) 		 *
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  116) 		 *	ACQUIRE (rq->lock)
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  117) 		 *	[S] ->on_rq = MIGRATING		[L] rq = task_rq()
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  118) 		 *	WMB (__set_task_cpu())		ACQUIRE (rq->lock);
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  119) 		 *	[S] ->cpu = new_cpu		[L] task_rq()
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  120) 		 *					[L] ->on_rq
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  121) 		 *	RELEASE (rq->lock)
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  122) 		 *
c546951d9c930 kernel/sched/core.c (Andrea Parri               2019-01-21 16:52:40 +0100  123) 		 * If we observe the old CPU in task_rq_lock(), the acquire of
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  124) 		 * the old rq->lock will fully serialize against the stores.
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  125) 		 *
c546951d9c930 kernel/sched/core.c (Andrea Parri               2019-01-21 16:52:40 +0100  126) 		 * If we observe the new CPU in task_rq_lock(), the address
c546951d9c930 kernel/sched/core.c (Andrea Parri               2019-01-21 16:52:40 +0100  127) 		 * dependency headed by '[L] rq = task_rq()' and the acquire
c546951d9c930 kernel/sched/core.c (Andrea Parri               2019-01-21 16:52:40 +0100  128) 		 * will pair with the WMB to ensure we then also see migrating.
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  129) 		 */
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  130) 		if (likely(rq == task_rq(p) && !task_on_rq_migrating(p))) {
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100  131) 			rq_pin_lock(rq, rf);
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  132) 			return rq;
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  133) 		}
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  134) 		raw_spin_unlock(&rq->lock);
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200  135) 		raw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  136) 
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  137) 		while (unlikely(task_on_rq_migrating(p)))
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  138) 			cpu_relax();
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  139) 	}
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  140) }
3e71a462dd483 kernel/sched/core.c (Peter Zijlstra             2016-04-28 16:16:33 +0200  141) 
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  142) /*
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  143)  * RQ-clock updating methods:
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  144)  */
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  145) 
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  146) static void update_rq_clock_task(struct rq *rq, s64 delta)
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  147) {
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  148) /*
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  149)  * In theory, the compile should just see 0 here, and optimize out the call
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  150)  * to sched_rt_avg_update. But I don't trust it...
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  151)  */
11d4afd4ff667 kernel/sched/core.c (Vincent Guittot            2018-09-25 11:17:42 +0200  152) 	s64 __maybe_unused steal = 0, irq_delta = 0;
11d4afd4ff667 kernel/sched/core.c (Vincent Guittot            2018-09-25 11:17:42 +0200  153) 
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  154) #ifdef CONFIG_IRQ_TIME_ACCOUNTING
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  155) 	irq_delta = irq_time_read(cpu_of(rq)) - rq->prev_irq_time;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  156) 
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  157) 	/*
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  158) 	 * Since irq_time is only updated on {soft,}irq_exit, we might run into
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  159) 	 * this case when a previous update_rq_clock() happened inside a
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  160) 	 * {soft,}irq region.
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  161) 	 *
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  162) 	 * When this happens, we stop ->clock_task and only update the
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  163) 	 * prev_irq_time stamp to account for the part that fit, so that a next
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  164) 	 * update will consume the rest. This ensures ->clock_task is
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  165) 	 * monotonic.
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  166) 	 *
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  167) 	 * It does however cause some slight miss-attribution of {soft,}irq
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  168) 	 * time, a more accurate solution would be to update the irq_time using
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  169) 	 * the current rq->clock timestamp, except that would require using
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  170) 	 * atomic ops.
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  171) 	 */
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  172) 	if (irq_delta > delta)
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  173) 		irq_delta = delta;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  174) 
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  175) 	rq->prev_irq_time += irq_delta;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  176) 	delta -= irq_delta;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  177) #endif
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  178) #ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  179) 	if (static_key_false((&paravirt_steal_rq_enabled))) {
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  180) 		steal = paravirt_steal_clock(cpu_of(rq));
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  181) 		steal -= rq->prev_steal_time_rq;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  182) 
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  183) 		if (unlikely(steal > delta))
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  184) 			steal = delta;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  185) 
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  186) 		rq->prev_steal_time_rq += steal;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  187) 		delta -= steal;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  188) 	}
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  189) #endif
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  190) 
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  191) 	rq->clock_task += delta;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  192) 
11d4afd4ff667 kernel/sched/core.c (Vincent Guittot            2018-09-25 11:17:42 +0200  193) #ifdef CONFIG_HAVE_SCHED_AVG_IRQ
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  194) 	if ((irq_delta + steal) && sched_feat(NONTASK_CAPACITY))
91c27493e78df kernel/sched/core.c (Vincent Guittot            2018-06-28 17:45:09 +0200  195) 		update_irq_load_avg(rq, irq_delta + steal);
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  196) #endif
23127296889fe kernel/sched/core.c (Vincent Guittot            2019-01-23 16:26:53 +0100  197) 	update_rq_clock_pelt(rq, delta);
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  198) }
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  199) 
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  200) void update_rq_clock(struct rq *rq)
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  201) {
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  202) 	s64 delta;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  203) 
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  204) 	lockdep_assert_held(&rq->lock);
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  205) 
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  206) 	if (rq->clock_update_flags & RQCF_ACT_SKIP)
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  207) 		return;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  208) 
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  209) #ifdef CONFIG_SCHED_DEBUG
26ae58d23b94a kernel/sched/core.c (Peter Zijlstra             2016-10-03 16:53:49 +0200  210) 	if (sched_feat(WARN_DOUBLE_CLOCK))
26ae58d23b94a kernel/sched/core.c (Peter Zijlstra             2016-10-03 16:53:49 +0200  211) 		SCHED_WARN_ON(rq->clock_update_flags & RQCF_UPDATED);
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  212) 	rq->clock_update_flags |= RQCF_UPDATED;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  213) #endif
26ae58d23b94a kernel/sched/core.c (Peter Zijlstra             2016-10-03 16:53:49 +0200  214) 
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  215) 	delta = sched_clock_cpu(cpu_of(rq)) - rq->clock;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  216) 	if (delta < 0)
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  217) 		return;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  218) 	rq->clock += delta;
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  219) 	update_rq_clock_task(rq, delta);
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  220) }
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  221) 
535b9552bb81e kernel/sched/core.c (Ingo Molnar                2017-02-01 12:29:21 +0100  222) 
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  223) #ifdef CONFIG_SCHED_HRTICK
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  224) /*
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  225)  * Use HR-timers to deliver accurate preemption points.
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  226)  */
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  227) 
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  228) static void hrtick_clear(struct rq *rq)
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  229) {
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  230) 	if (hrtimer_active(&rq->hrtick_timer))
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  231) 		hrtimer_cancel(&rq->hrtick_timer);
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  232) }
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  233) 
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  234) /*
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  235)  * High-resolution timer tick.
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  236)  * Runs from hardirq context with interrupts disabled.
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  237)  */
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  238) static enum hrtimer_restart hrtick(struct hrtimer *timer)
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  239) {
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  240) 	struct rq *rq = container_of(timer, struct rq, hrtick_timer);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200  241) 	struct rq_flags rf;
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  242) 
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  243) 	WARN_ON_ONCE(cpu_of(rq) != smp_processor_id());
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  244) 
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200  245) 	rq_lock(rq, &rf);
3e51f33fcc7f5 kernel/sched.c      (Peter Zijlstra             2008-05-03 18:29:28 +0200  246) 	update_rq_clock(rq);
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  247) 	rq->curr->sched_class->task_tick(rq, rq->curr, 1);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200  248) 	rq_unlock(rq, &rf);
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  249) 
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  250) 	return HRTIMER_NORESTART;
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  251) }
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  252) 
95e904c7da715 kernel/sched.c      (Rabin Vincent              2008-05-11 05:55:33 +0530  253) #ifdef CONFIG_SMP
971ee28cbd1cc kernel/sched/core.c (Peter Zijlstra             2013-06-28 11:18:53 +0200  254) 
4961b6e11825c kernel/sched/core.c (Thomas Gleixner            2015-04-14 21:09:05 +0000  255) static void __hrtick_restart(struct rq *rq)
971ee28cbd1cc kernel/sched/core.c (Peter Zijlstra             2013-06-28 11:18:53 +0200  256) {
971ee28cbd1cc kernel/sched/core.c (Peter Zijlstra             2013-06-28 11:18:53 +0200  257) 	struct hrtimer *timer = &rq->hrtick_timer;
971ee28cbd1cc kernel/sched/core.c (Peter Zijlstra             2013-06-28 11:18:53 +0200  258) 
d5096aa65acd0 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-07-26 20:30:52 +0200  259) 	hrtimer_start_expires(timer, HRTIMER_MODE_ABS_PINNED_HARD);
971ee28cbd1cc kernel/sched/core.c (Peter Zijlstra             2013-06-28 11:18:53 +0200  260) }
971ee28cbd1cc kernel/sched/core.c (Peter Zijlstra             2013-06-28 11:18:53 +0200  261) 
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  262) /*
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  263)  * called from hardirq (IPI) context
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  264)  */
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  265) static void __hrtick_start(void *arg)
b328ca182f01c kernel/sched.c      (Peter Zijlstra             2008-04-29 10:02:46 +0200  266) {
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  267) 	struct rq *rq = arg;
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200  268) 	struct rq_flags rf;
b328ca182f01c kernel/sched.c      (Peter Zijlstra             2008-04-29 10:02:46 +0200  269) 
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200  270) 	rq_lock(rq, &rf);
971ee28cbd1cc kernel/sched/core.c (Peter Zijlstra             2013-06-28 11:18:53 +0200  271) 	__hrtick_restart(rq);
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  272) 	rq->hrtick_csd_pending = 0;
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200  273) 	rq_unlock(rq, &rf);
b328ca182f01c kernel/sched.c      (Peter Zijlstra             2008-04-29 10:02:46 +0200  274) }
b328ca182f01c kernel/sched.c      (Peter Zijlstra             2008-04-29 10:02:46 +0200  275) 
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  276) /*
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  277)  * Called to set the hrtick timer state.
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  278)  *
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  279)  * called with rq->lock held and irqs disabled
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  280)  */
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200  281) void hrtick_start(struct rq *rq, u64 delay)
b328ca182f01c kernel/sched.c      (Peter Zijlstra             2008-04-29 10:02:46 +0200  282) {
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  283) 	struct hrtimer *timer = &rq->hrtick_timer;
177ef2a6315ea kernel/sched/core.c (xiaofeng.yan               2014-08-26 03:15:41 +0000  284) 	ktime_t time;
177ef2a6315ea kernel/sched/core.c (xiaofeng.yan               2014-08-26 03:15:41 +0000  285) 	s64 delta;
177ef2a6315ea kernel/sched/core.c (xiaofeng.yan               2014-08-26 03:15:41 +0000  286) 
177ef2a6315ea kernel/sched/core.c (xiaofeng.yan               2014-08-26 03:15:41 +0000  287) 	/*
177ef2a6315ea kernel/sched/core.c (xiaofeng.yan               2014-08-26 03:15:41 +0000  288) 	 * Don't schedule slices shorter than 10000ns, that just
177ef2a6315ea kernel/sched/core.c (xiaofeng.yan               2014-08-26 03:15:41 +0000  289) 	 * doesn't make sense and can cause timer DoS.
177ef2a6315ea kernel/sched/core.c (xiaofeng.yan               2014-08-26 03:15:41 +0000  290) 	 */
177ef2a6315ea kernel/sched/core.c (xiaofeng.yan               2014-08-26 03:15:41 +0000  291) 	delta = max_t(s64, delay, 10000LL);
177ef2a6315ea kernel/sched/core.c (xiaofeng.yan               2014-08-26 03:15:41 +0000  292) 	time = ktime_add_ns(timer->base->get_time(), delta);
b328ca182f01c kernel/sched.c      (Peter Zijlstra             2008-04-29 10:02:46 +0200  293) 
cc584b213f252 kernel/sched.c      (Arjan van de Ven           2008-09-01 15:02:30 -0700  294) 	hrtimer_set_expires(timer, time);
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  295) 
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  296) 	if (rq == this_rq()) {
971ee28cbd1cc kernel/sched/core.c (Peter Zijlstra             2013-06-28 11:18:53 +0200  297) 		__hrtick_restart(rq);
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  298) 	} else if (!rq->hrtick_csd_pending) {
c46fff2a3b297 kernel/sched/core.c (Frederic Weisbecker        2014-02-24 16:40:02 +0100  299) 		smp_call_function_single_async(cpu_of(rq), &rq->hrtick_csd);
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  300) 		rq->hrtick_csd_pending = 1;
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  301) 	}
b328ca182f01c kernel/sched.c      (Peter Zijlstra             2008-04-29 10:02:46 +0200  302) }
b328ca182f01c kernel/sched.c      (Peter Zijlstra             2008-04-29 10:02:46 +0200  303) 
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  304) #else
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  305) /*
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  306)  * Called to set the hrtick timer state.
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  307)  *
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  308)  * called with rq->lock held and irqs disabled
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  309)  */
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200  310) void hrtick_start(struct rq *rq, u64 delay)
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  311) {
868933359a3bd kernel/sched/core.c (Wanpeng Li                 2014-11-26 08:44:06 +0800  312) 	/*
868933359a3bd kernel/sched/core.c (Wanpeng Li                 2014-11-26 08:44:06 +0800  313) 	 * Don't schedule slices shorter than 10000ns, that just
868933359a3bd kernel/sched/core.c (Wanpeng Li                 2014-11-26 08:44:06 +0800  314) 	 * doesn't make sense. Rely on vruntime for fairness.
868933359a3bd kernel/sched/core.c (Wanpeng Li                 2014-11-26 08:44:06 +0800  315) 	 */
868933359a3bd kernel/sched/core.c (Wanpeng Li                 2014-11-26 08:44:06 +0800  316) 	delay = max_t(u64, delay, 10000LL);
4961b6e11825c kernel/sched/core.c (Thomas Gleixner            2015-04-14 21:09:05 +0000  317) 	hrtimer_start(&rq->hrtick_timer, ns_to_ktime(delay),
d5096aa65acd0 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-07-26 20:30:52 +0200  318) 		      HRTIMER_MODE_REL_PINNED_HARD);
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  319) }
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  320) #endif /* CONFIG_SMP */
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  321) 
77a021be383eb kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:23 +0100  322) static void hrtick_rq_init(struct rq *rq)
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  323) {
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  324) #ifdef CONFIG_SMP
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  325) 	rq->hrtick_csd_pending = 0;
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  326) 
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  327) 	rq->hrtick_csd.flags = 0;
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  328) 	rq->hrtick_csd.func = __hrtick_start;
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  329) 	rq->hrtick_csd.info = rq;
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  330) #endif
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  331) 
d5096aa65acd0 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-07-26 20:30:52 +0200  332) 	hrtimer_init(&rq->hrtick_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD);
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200  333) 	rq->hrtick_timer.function = hrtick;
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  334) }
006c75f146e58 kernel/sched.c      (Andrew Morton              2008-09-22 14:55:46 -0700  335) #else	/* CONFIG_SCHED_HRTICK */
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  336) static inline void hrtick_clear(struct rq *rq)
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  337) {
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  338) }
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  339) 
77a021be383eb kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:23 +0100  340) static inline void hrtick_rq_init(struct rq *rq)
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  341) {
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  342) }
006c75f146e58 kernel/sched.c      (Andrew Morton              2008-09-22 14:55:46 -0700  343) #endif	/* CONFIG_SCHED_HRTICK */
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100  344) 
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  345) /*
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  346)  * cmpxchg based fetch_or, macro so it works for different integer types
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  347)  */
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  348) #define fetch_or(ptr, mask)						\
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  349) 	({								\
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  350) 		typeof(ptr) _ptr = (ptr);				\
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  351) 		typeof(mask) _mask = (mask);				\
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  352) 		typeof(*_ptr) _old, _val = *_ptr;			\
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  353) 									\
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  354) 		for (;;) {						\
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  355) 			_old = cmpxchg(_ptr, _val, _val | _mask);	\
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  356) 			if (_old == _val)				\
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  357) 				break;					\
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  358) 			_val = _old;					\
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  359) 		}							\
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  360) 	_old;								\
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  361) })
5529578a27288 kernel/sched/core.c (Frederic Weisbecker        2016-03-24 15:38:01 +0100  362) 
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  363) #if defined(CONFIG_SMP) && defined(TIF_POLLING_NRFLAG)
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  364) /*
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  365)  * Atomically set TIF_NEED_RESCHED and test for TIF_POLLING_NRFLAG,
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  366)  * this avoids any races wrt polling state changes and thereby avoids
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  367)  * spurious IPIs.
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  368)  */
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  369) static bool set_nr_and_not_polling(struct task_struct *p)
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  370) {
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  371) 	struct thread_info *ti = task_thread_info(p);
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  372) 	return !(fetch_or(&ti->flags, _TIF_NEED_RESCHED) & _TIF_POLLING_NRFLAG);
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  373) }
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  374) 
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  375) /*
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  376)  * Atomically set TIF_NEED_RESCHED if TIF_POLLING_NRFLAG is set.
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  377)  *
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  378)  * If this returns true, then the idle task promises to call
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  379)  * sched_ttwu_pending() and reschedule soon.
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  380)  */
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  381) static bool set_nr_if_polling(struct task_struct *p)
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  382) {
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  383) 	struct thread_info *ti = task_thread_info(p);
316c1608d15c7 kernel/sched/core.c (Jason Low                  2015-04-28 13:00:20 -0700  384) 	typeof(ti->flags) old, val = READ_ONCE(ti->flags);
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  385) 
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  386) 	for (;;) {
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  387) 		if (!(val & _TIF_POLLING_NRFLAG))
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  388) 			return false;
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  389) 		if (val & _TIF_NEED_RESCHED)
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  390) 			return true;
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  391) 		old = cmpxchg(&ti->flags, val, val | _TIF_NEED_RESCHED);
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  392) 		if (old == val)
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  393) 			break;
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  394) 		val = old;
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  395) 	}
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  396) 	return true;
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  397) }
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  398) 
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  399) #else
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  400) static bool set_nr_and_not_polling(struct task_struct *p)
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  401) {
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  402) 	set_tsk_need_resched(p);
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  403) 	return true;
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  404) }
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  405) 
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  406) #ifdef CONFIG_SMP
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  407) static bool set_nr_if_polling(struct task_struct *p)
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  408) {
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  409) 	return false;
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  410) }
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700  411) #endif
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  412) #endif
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  413) 
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  414) static bool __wake_q_add(struct wake_q_head *head, struct task_struct *task)
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  415) {
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  416) 	struct wake_q_node *node = &task->wake_q;
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  417) 
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  418) 	/*
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  419) 	 * Atomically grab the task, if ->wake_q is !nil already it means
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  420) 	 * its already queued (either by us or someone else) and will get the
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  421) 	 * wakeup due to that.
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  422) 	 *
4c4e3731564c8 kernel/sched/core.c (Peter Zijlstra             2018-12-17 10:14:53 +0100  423) 	 * In order to ensure that a pending wakeup will observe our pending
4c4e3731564c8 kernel/sched/core.c (Peter Zijlstra             2018-12-17 10:14:53 +0100  424) 	 * state, even in the failed case, an explicit smp_mb() must be used.
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  425) 	 */
4c4e3731564c8 kernel/sched/core.c (Peter Zijlstra             2018-12-17 10:14:53 +0100  426) 	smp_mb__before_atomic();
87ff19cb2f1aa kernel/sched/core.c (Davidlohr Bueso            2018-12-02 21:31:30 -0800  427) 	if (unlikely(cmpxchg_relaxed(&node->next, NULL, WAKE_Q_TAIL)))
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  428) 		return false;
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  429) 
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  430) 	/*
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  431) 	 * The head is context local, there can be no concurrency.
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  432) 	 */
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  433) 	*head->lastp = node;
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  434) 	head->lastp = &node->next;
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  435) 	return true;
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  436) }
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  437) 
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  438) /**
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  439)  * wake_q_add() - queue a wakeup for 'later' waking.
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  440)  * @head: the wake_q_head to add @task to
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  441)  * @task: the task to queue for 'later' wakeup
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  442)  *
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  443)  * Queue a task for later wakeup, most likely by the wake_up_q() call in the
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  444)  * same context, _HOWEVER_ this is not guaranteed, the wakeup can come
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  445)  * instantly.
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  446)  *
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  447)  * This function must be used as-if it were wake_up_process(); IOW the task
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  448)  * must be ready to be woken at this location.
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  449)  */
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  450) void wake_q_add(struct wake_q_head *head, struct task_struct *task)
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  451) {
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  452) 	if (__wake_q_add(head, task))
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  453) 		get_task_struct(task);
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  454) }
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  455) 
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  456) /**
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  457)  * wake_q_add_safe() - safely queue a wakeup for 'later' waking.
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  458)  * @head: the wake_q_head to add @task to
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  459)  * @task: the task to queue for 'later' wakeup
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  460)  *
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  461)  * Queue a task for later wakeup, most likely by the wake_up_q() call in the
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  462)  * same context, _HOWEVER_ this is not guaranteed, the wakeup can come
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  463)  * instantly.
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  464)  *
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  465)  * This function must be used as-if it were wake_up_process(); IOW the task
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  466)  * must be ready to be woken at this location.
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  467)  *
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  468)  * This function is essentially a task-safe equivalent to wake_q_add(). Callers
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  469)  * that already hold reference to @task can call the 'safe' version and trust
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  470)  * wake_q to do the right thing depending whether or not the @task is already
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  471)  * queued for wakeup.
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  472)  */
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  473) void wake_q_add_safe(struct wake_q_head *head, struct task_struct *task)
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  474) {
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  475) 	if (!__wake_q_add(head, task))
07879c6a3740f kernel/sched/core.c (Davidlohr Bueso            2018-12-18 11:53:52 -0800  476) 		put_task_struct(task);
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  477) }
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  478) 
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  479) void wake_up_q(struct wake_q_head *head)
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  480) {
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  481) 	struct wake_q_node *node = head->first;
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  482) 
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  483) 	while (node != WAKE_Q_TAIL) {
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  484) 		struct task_struct *task;
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  485) 
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  486) 		task = container_of(node, struct task_struct, wake_q);
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  487) 		BUG_ON(!task);
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100  488) 		/* Task can safely be re-inserted now: */
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  489) 		node = node->next;
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  490) 		task->wake_q.next = NULL;
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  491) 
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  492) 		/*
7696f9910a9a4 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:03 -0700  493) 		 * wake_up_process() executes a full barrier, which pairs with
7696f9910a9a4 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:03 -0700  494) 		 * the queueing in wake_q_add() so as not to miss wakeups.
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  495) 		 */
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  496) 		wake_up_process(task);
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  497) 		put_task_struct(task);
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  498) 	}
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  499) }
7675104990ed2 kernel/sched/core.c (Peter Zijlstra             2015-05-01 08:27:50 -0700  500) 
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  501) /*
8875125efe840 kernel/sched/core.c (Kirill Tkhai               2014-06-29 00:03:57 +0400  502)  * resched_curr - mark rq's current task 'to be rescheduled now'.
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  503)  *
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  504)  * On UP this means the setting of the need_resched flag, on SMP it
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  505)  * might also involve a cross-CPU call to trigger the scheduler on
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  506)  * the target CPU.
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  507)  */
8875125efe840 kernel/sched/core.c (Kirill Tkhai               2014-06-29 00:03:57 +0400  508) void resched_curr(struct rq *rq)
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  509) {
8875125efe840 kernel/sched/core.c (Kirill Tkhai               2014-06-29 00:03:57 +0400  510) 	struct task_struct *curr = rq->curr;
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  511) 	int cpu;
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  512) 
8875125efe840 kernel/sched/core.c (Kirill Tkhai               2014-06-29 00:03:57 +0400  513) 	lockdep_assert_held(&rq->lock);
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  514) 
8875125efe840 kernel/sched/core.c (Kirill Tkhai               2014-06-29 00:03:57 +0400  515) 	if (test_tsk_need_resched(curr))
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  516) 		return;
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  517) 
8875125efe840 kernel/sched/core.c (Kirill Tkhai               2014-06-29 00:03:57 +0400  518) 	cpu = cpu_of(rq);
fd99f91aa007b kernel/sched/core.c (Peter Zijlstra             2014-04-09 15:35:08 +0200  519) 
f27dde8deef33 kernel/sched/core.c (Peter Zijlstra             2013-08-14 14:55:31 +0200  520) 	if (cpu == smp_processor_id()) {
8875125efe840 kernel/sched/core.c (Kirill Tkhai               2014-06-29 00:03:57 +0400  521) 		set_tsk_need_resched(curr);
f27dde8deef33 kernel/sched/core.c (Peter Zijlstra             2013-08-14 14:55:31 +0200  522) 		set_preempt_need_resched();
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  523) 		return;
f27dde8deef33 kernel/sched/core.c (Peter Zijlstra             2013-08-14 14:55:31 +0200  524) 	}
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  525) 
8875125efe840 kernel/sched/core.c (Kirill Tkhai               2014-06-29 00:03:57 +0400  526) 	if (set_nr_and_not_polling(curr))
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  527) 		smp_send_reschedule(cpu);
dfc68f29ae67f kernel/sched/core.c (Andy Lutomirski            2014-06-04 10:31:15 -0700  528) 	else
dfc68f29ae67f kernel/sched/core.c (Andy Lutomirski            2014-06-04 10:31:15 -0700  529) 		trace_sched_wake_idle_without_ipi(cpu);
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  530) }
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  531) 
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200  532) void resched_cpu(int cpu)
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  533) {
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  534) 	struct rq *rq = cpu_rq(cpu);
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  535) 	unsigned long flags;
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  536) 
7c2102e56a3f7 kernel/sched/core.c (Paul E. McKenney           2017-09-18 08:54:40 -0700  537) 	raw_spin_lock_irqsave(&rq->lock, flags);
a0982dfa03efc kernel/sched/core.c (Paul E. McKenney           2017-10-13 16:24:28 -0700  538) 	if (cpu_online(cpu) || cpu == smp_processor_id())
a0982dfa03efc kernel/sched/core.c (Paul E. McKenney           2017-10-13 16:24:28 -0700  539) 		resched_curr(rq);
05fa785cf80c9 kernel/sched.c      (Thomas Gleixner            2009-11-17 14:28:38 +0100  540) 	raw_spin_unlock_irqrestore(&rq->lock, flags);
c24d20dbef948 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  541) }
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  542) 
b021fe3e25094 kernel/sched/core.c (Peter Zijlstra             2013-09-17 09:30:55 +0200  543) #ifdef CONFIG_SMP
3451d0243c3cd kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  544) #ifdef CONFIG_NO_HZ_COMMON
83cd4fe27ad84 kernel/sched.c      (Venkatesh Pallipadi        2010-05-21 17:09:41 -0700  545) /*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100  546)  * In the semi idle case, use the nearest busy CPU for migrating timers
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100  547)  * from an idle CPU.  This is good for power-savings.
83cd4fe27ad84 kernel/sched.c      (Venkatesh Pallipadi        2010-05-21 17:09:41 -0700  548)  *
83cd4fe27ad84 kernel/sched.c      (Venkatesh Pallipadi        2010-05-21 17:09:41 -0700  549)  * We don't do similar optimization for completely idle system, as
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100  550)  * selecting an idle CPU will add more delays to the timers than intended
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100  551)  * (as that CPU's timer base may not be uptodate wrt jiffies etc).
83cd4fe27ad84 kernel/sched.c      (Venkatesh Pallipadi        2010-05-21 17:09:41 -0700  552)  */
bc7a34b8b9ebf kernel/sched/core.c (Thomas Gleixner            2015-05-26 22:50:33 +0000  553) int get_nohz_timer_target(void)
83cd4fe27ad84 kernel/sched.c      (Venkatesh Pallipadi        2010-05-21 17:09:41 -0700  554) {
e938b9c94164e kernel/sched/core.c (Wanpeng Li                 2020-01-13 08:50:27 +0800  555) 	int i, cpu = smp_processor_id(), default_cpu = -1;
83cd4fe27ad84 kernel/sched.c      (Venkatesh Pallipadi        2010-05-21 17:09:41 -0700  556) 	struct sched_domain *sd;
83cd4fe27ad84 kernel/sched.c      (Venkatesh Pallipadi        2010-05-21 17:09:41 -0700  557) 
e938b9c94164e kernel/sched/core.c (Wanpeng Li                 2020-01-13 08:50:27 +0800  558) 	if (housekeeping_cpu(cpu, HK_FLAG_TIMER)) {
e938b9c94164e kernel/sched/core.c (Wanpeng Li                 2020-01-13 08:50:27 +0800  559) 		if (!idle_cpu(cpu))
e938b9c94164e kernel/sched/core.c (Wanpeng Li                 2020-01-13 08:50:27 +0800  560) 			return cpu;
e938b9c94164e kernel/sched/core.c (Wanpeng Li                 2020-01-13 08:50:27 +0800  561) 		default_cpu = cpu;
e938b9c94164e kernel/sched/core.c (Wanpeng Li                 2020-01-13 08:50:27 +0800  562) 	}
6201b4d61fbf1 kernel/sched/core.c (Viresh Kumar               2014-03-18 16:26:07 +0530  563) 
057f3fadb347e kernel/sched.c      (Peter Zijlstra             2011-04-18 11:24:34 +0200  564) 	rcu_read_lock();
83cd4fe27ad84 kernel/sched.c      (Venkatesh Pallipadi        2010-05-21 17:09:41 -0700  565) 	for_each_domain(cpu, sd) {
e938b9c94164e kernel/sched/core.c (Wanpeng Li                 2020-01-13 08:50:27 +0800  566) 		for_each_cpu_and(i, sched_domain_span(sd),
e938b9c94164e kernel/sched/core.c (Wanpeng Li                 2020-01-13 08:50:27 +0800  567) 			housekeeping_cpumask(HK_FLAG_TIMER)) {
444969223c81c kernel/sched/core.c (Wanpeng Li                 2016-05-04 14:45:34 +0800  568) 			if (cpu == i)
444969223c81c kernel/sched/core.c (Wanpeng Li                 2016-05-04 14:45:34 +0800  569) 				continue;
444969223c81c kernel/sched/core.c (Wanpeng Li                 2016-05-04 14:45:34 +0800  570) 
e938b9c94164e kernel/sched/core.c (Wanpeng Li                 2020-01-13 08:50:27 +0800  571) 			if (!idle_cpu(i)) {
057f3fadb347e kernel/sched.c      (Peter Zijlstra             2011-04-18 11:24:34 +0200  572) 				cpu = i;
057f3fadb347e kernel/sched.c      (Peter Zijlstra             2011-04-18 11:24:34 +0200  573) 				goto unlock;
057f3fadb347e kernel/sched.c      (Peter Zijlstra             2011-04-18 11:24:34 +0200  574) 			}
057f3fadb347e kernel/sched.c      (Peter Zijlstra             2011-04-18 11:24:34 +0200  575) 		}
83cd4fe27ad84 kernel/sched.c      (Venkatesh Pallipadi        2010-05-21 17:09:41 -0700  576) 	}
9642d18eee2cd kernel/sched/core.c (Vatika Harlalka            2015-09-01 16:50:59 +0200  577) 
e938b9c94164e kernel/sched/core.c (Wanpeng Li                 2020-01-13 08:50:27 +0800  578) 	if (default_cpu == -1)
e938b9c94164e kernel/sched/core.c (Wanpeng Li                 2020-01-13 08:50:27 +0800  579) 		default_cpu = housekeeping_any_cpu(HK_FLAG_TIMER);
e938b9c94164e kernel/sched/core.c (Wanpeng Li                 2020-01-13 08:50:27 +0800  580) 	cpu = default_cpu;
057f3fadb347e kernel/sched.c      (Peter Zijlstra             2011-04-18 11:24:34 +0200  581) unlock:
057f3fadb347e kernel/sched.c      (Peter Zijlstra             2011-04-18 11:24:34 +0200  582) 	rcu_read_unlock();
83cd4fe27ad84 kernel/sched.c      (Venkatesh Pallipadi        2010-05-21 17:09:41 -0700  583) 	return cpu;
83cd4fe27ad84 kernel/sched.c      (Venkatesh Pallipadi        2010-05-21 17:09:41 -0700  584) }
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100  585) 
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  586) /*
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  587)  * When add_timer_on() enqueues a timer into the timer wheel of an
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  588)  * idle CPU then this timer might expire before the next timer event
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  589)  * which is scheduled to wake up that CPU. In case of a completely
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  590)  * idle system the next event might even be infinite time into the
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  591)  * future. wake_up_idle_cpu() ensures that the CPU is woken up and
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  592)  * leaves the inner idle loop so the newly added timer is taken into
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  593)  * account when the CPU goes back to idle and evaluates the timer
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  594)  * wheel for the next timer event.
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  595)  */
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  596) static void wake_up_idle_cpu(int cpu)
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  597) {
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  598) 	struct rq *rq = cpu_rq(cpu);
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  599) 
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  600) 	if (cpu == smp_processor_id())
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  601) 		return;
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  602) 
67b9ca70c3030 kernel/sched/core.c (Andy Lutomirski            2014-06-04 10:31:17 -0700  603) 	if (set_nr_and_not_polling(rq->idle))
06d8308c61e54 kernel/sched.c      (Thomas Gleixner            2008-03-22 09:20:24 +0100  604) 		smp_send_reschedule(cpu);
dfc68f29ae67f kernel/sched/core.c (Andy Lutomirski            2014-06-04 10:31:15 -0700  605) 	else
dfc68f29ae67f kernel/sched/core.c (Andy Lutomirski            2014-06-04 10:31:15 -0700  606) 		trace_sched_wake_idle_without_ipi(cpu);
45bf76df4814a kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  607) }
45bf76df4814a kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  608) 
c5bfece2d6129 kernel/sched/core.c (Frederic Weisbecker        2013-04-12 16:45:34 +0200  609) static bool wake_up_full_nohz_cpu(int cpu)
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  610) {
53c5fa16b4c84 kernel/sched/core.c (Frederic Weisbecker        2014-06-04 16:20:21 +0200  611) 	/*
53c5fa16b4c84 kernel/sched/core.c (Frederic Weisbecker        2014-06-04 16:20:21 +0200  612) 	 * We just need the target to call irq_exit() and re-evaluate
53c5fa16b4c84 kernel/sched/core.c (Frederic Weisbecker        2014-06-04 16:20:21 +0200  613) 	 * the next tick. The nohz full kick at least implies that.
53c5fa16b4c84 kernel/sched/core.c (Frederic Weisbecker        2014-06-04 16:20:21 +0200  614) 	 * If needed we can still optimize that later with an
53c5fa16b4c84 kernel/sched/core.c (Frederic Weisbecker        2014-06-04 16:20:21 +0200  615) 	 * empty IRQ.
53c5fa16b4c84 kernel/sched/core.c (Frederic Weisbecker        2014-06-04 16:20:21 +0200  616) 	 */
379d9ecb3cc9d kernel/sched/core.c (Paul E. McKenney           2016-06-30 10:37:20 -0700  617) 	if (cpu_is_offline(cpu))
379d9ecb3cc9d kernel/sched/core.c (Paul E. McKenney           2016-06-30 10:37:20 -0700  618) 		return true;  /* Don't try to wake offline CPUs. */
c5bfece2d6129 kernel/sched/core.c (Frederic Weisbecker        2013-04-12 16:45:34 +0200  619) 	if (tick_nohz_full_cpu(cpu)) {
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  620) 		if (cpu != smp_processor_id() ||
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  621) 		    tick_nohz_tick_stopped())
53c5fa16b4c84 kernel/sched/core.c (Frederic Weisbecker        2014-06-04 16:20:21 +0200  622) 			tick_nohz_full_kick_cpu(cpu);
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  623) 		return true;
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  624) 	}
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  625) 
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  626) 	return false;
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  627) }
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  628) 
379d9ecb3cc9d kernel/sched/core.c (Paul E. McKenney           2016-06-30 10:37:20 -0700  629) /*
379d9ecb3cc9d kernel/sched/core.c (Paul E. McKenney           2016-06-30 10:37:20 -0700  630)  * Wake up the specified CPU.  If the CPU is going offline, it is the
379d9ecb3cc9d kernel/sched/core.c (Paul E. McKenney           2016-06-30 10:37:20 -0700  631)  * caller's responsibility to deal with the lost wakeup, for example,
379d9ecb3cc9d kernel/sched/core.c (Paul E. McKenney           2016-06-30 10:37:20 -0700  632)  * by hooking into the CPU_DEAD notifier like timers and hrtimers do.
379d9ecb3cc9d kernel/sched/core.c (Paul E. McKenney           2016-06-30 10:37:20 -0700  633)  */
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  634) void wake_up_nohz_cpu(int cpu)
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  635) {
c5bfece2d6129 kernel/sched/core.c (Frederic Weisbecker        2013-04-12 16:45:34 +0200  636) 	if (!wake_up_full_nohz_cpu(cpu))
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  637) 		wake_up_idle_cpu(cpu);
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  638) }
1c20091e77fc5 kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  639) 
ca38062e57e97 kernel/sched.c      (Suresh Siddha              2011-10-03 15:09:00 -0700  640) static inline bool got_nohz_idle_kick(void)
45bf76df4814a kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  641) {
1c792db7f7957 kernel/sched/core.c (Suresh Siddha              2011-12-01 17:07:32 -0800  642) 	int cpu = smp_processor_id();
873b4c65b519f kernel/sched/core.c (Vincent Guittot            2013-06-05 10:13:11 +0200  643) 
b7031a02ec753 kernel/sched/core.c (Peter Zijlstra             2017-12-21 10:11:09 +0100  644) 	if (!(atomic_read(nohz_flags(cpu)) & NOHZ_KICK_MASK))
873b4c65b519f kernel/sched/core.c (Vincent Guittot            2013-06-05 10:13:11 +0200  645) 		return false;
873b4c65b519f kernel/sched/core.c (Vincent Guittot            2013-06-05 10:13:11 +0200  646) 
873b4c65b519f kernel/sched/core.c (Vincent Guittot            2013-06-05 10:13:11 +0200  647) 	if (idle_cpu(cpu) && !need_resched())
873b4c65b519f kernel/sched/core.c (Vincent Guittot            2013-06-05 10:13:11 +0200  648) 		return true;
873b4c65b519f kernel/sched/core.c (Vincent Guittot            2013-06-05 10:13:11 +0200  649) 
873b4c65b519f kernel/sched/core.c (Vincent Guittot            2013-06-05 10:13:11 +0200  650) 	/*
873b4c65b519f kernel/sched/core.c (Vincent Guittot            2013-06-05 10:13:11 +0200  651) 	 * We can't run Idle Load Balance on this CPU for this time so we
873b4c65b519f kernel/sched/core.c (Vincent Guittot            2013-06-05 10:13:11 +0200  652) 	 * cancel it and clear NOHZ_BALANCE_KICK
873b4c65b519f kernel/sched/core.c (Vincent Guittot            2013-06-05 10:13:11 +0200  653) 	 */
b7031a02ec753 kernel/sched/core.c (Peter Zijlstra             2017-12-21 10:11:09 +0100  654) 	atomic_andnot(NOHZ_KICK_MASK, nohz_flags(cpu));
873b4c65b519f kernel/sched/core.c (Vincent Guittot            2013-06-05 10:13:11 +0200  655) 	return false;
45bf76df4814a kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  656) }
45bf76df4814a kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  657) 
3451d0243c3cd kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  658) #else /* CONFIG_NO_HZ_COMMON */
45bf76df4814a kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  659) 
ca38062e57e97 kernel/sched.c      (Suresh Siddha              2011-10-03 15:09:00 -0700  660) static inline bool got_nohz_idle_kick(void)
2069dd75c7d0f kernel/sched.c      (Peter Zijlstra             2010-11-15 15:47:00 -0800  661) {
ca38062e57e97 kernel/sched.c      (Suresh Siddha              2011-10-03 15:09:00 -0700  662) 	return false;
2069dd75c7d0f kernel/sched.c      (Peter Zijlstra             2010-11-15 15:47:00 -0800  663) }
2069dd75c7d0f kernel/sched.c      (Peter Zijlstra             2010-11-15 15:47:00 -0800  664) 
3451d0243c3cd kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200  665) #endif /* CONFIG_NO_HZ_COMMON */
d842de871c8c5 kernel/sched.c      (Srivatsa Vaddagiri         2007-12-02 20:04:49 +0100  666) 
ce831b38ca492 kernel/sched/core.c (Frederic Weisbecker        2013-04-20 15:15:35 +0200  667) #ifdef CONFIG_NO_HZ_FULL
76d92ac305f23 kernel/sched/core.c (Frederic Weisbecker        2015-07-17 22:25:49 +0200  668) bool sched_can_stop_tick(struct rq *rq)
ce831b38ca492 kernel/sched/core.c (Frederic Weisbecker        2013-04-20 15:15:35 +0200  669) {
76d92ac305f23 kernel/sched/core.c (Frederic Weisbecker        2015-07-17 22:25:49 +0200  670) 	int fifo_nr_running;
76d92ac305f23 kernel/sched/core.c (Frederic Weisbecker        2015-07-17 22:25:49 +0200  671) 
76d92ac305f23 kernel/sched/core.c (Frederic Weisbecker        2015-07-17 22:25:49 +0200  672) 	/* Deadline tasks, even if single, need the tick */
76d92ac305f23 kernel/sched/core.c (Frederic Weisbecker        2015-07-17 22:25:49 +0200  673) 	if (rq->dl.dl_nr_running)
76d92ac305f23 kernel/sched/core.c (Frederic Weisbecker        2015-07-17 22:25:49 +0200  674) 		return false;
76d92ac305f23 kernel/sched/core.c (Frederic Weisbecker        2015-07-17 22:25:49 +0200  675) 
1e78cdbd9b226 kernel/sched/core.c (Rik van Riel               2015-02-16 15:23:49 -0500  676) 	/*
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  677) 	 * If there are more than one RR tasks, we need the tick to effect the
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  678) 	 * actual RR behaviour.
1e78cdbd9b226 kernel/sched/core.c (Rik van Riel               2015-02-16 15:23:49 -0500  679) 	 */
76d92ac305f23 kernel/sched/core.c (Frederic Weisbecker        2015-07-17 22:25:49 +0200  680) 	if (rq->rt.rr_nr_running) {
76d92ac305f23 kernel/sched/core.c (Frederic Weisbecker        2015-07-17 22:25:49 +0200  681) 		if (rq->rt.rr_nr_running == 1)
76d92ac305f23 kernel/sched/core.c (Frederic Weisbecker        2015-07-17 22:25:49 +0200  682) 			return true;
76d92ac305f23 kernel/sched/core.c (Frederic Weisbecker        2015-07-17 22:25:49 +0200  683) 		else
76d92ac305f23 kernel/sched/core.c (Frederic Weisbecker        2015-07-17 22:25:49 +0200  684) 			return false;
1e78cdbd9b226 kernel/sched/core.c (Rik van Riel               2015-02-16 15:23:49 -0500  685) 	}
1e78cdbd9b226 kernel/sched/core.c (Rik van Riel               2015-02-16 15:23:49 -0500  686) 
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  687) 	/*
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  688) 	 * If there's no RR tasks, but FIFO tasks, we can skip the tick, no
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  689) 	 * forced preemption between FIFO tasks.
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  690) 	 */
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  691) 	fifo_nr_running = rq->rt.rt_nr_running - rq->rt.rr_nr_running;
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  692) 	if (fifo_nr_running)
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  693) 		return true;
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  694) 
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  695) 	/*
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  696) 	 * If there are no DL,RR/FIFO tasks, there must only be CFS tasks left;
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  697) 	 * if there's more than one we need the tick for involuntary
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  698) 	 * preemption.
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  699) 	 */
2548d546d40c0 kernel/sched/core.c (Peter Zijlstra             2016-04-21 18:03:15 +0200  700) 	if (rq->nr_running > 1)
541b82644d72c kernel/sched/core.c (Viresh Kumar               2014-06-24 14:04:12 +0530  701) 		return false;
ce831b38ca492 kernel/sched/core.c (Frederic Weisbecker        2013-04-20 15:15:35 +0200  702) 
541b82644d72c kernel/sched/core.c (Viresh Kumar               2014-06-24 14:04:12 +0530  703) 	return true;
ce831b38ca492 kernel/sched/core.c (Frederic Weisbecker        2013-04-20 15:15:35 +0200  704) }
ce831b38ca492 kernel/sched/core.c (Frederic Weisbecker        2013-04-20 15:15:35 +0200  705) #endif /* CONFIG_NO_HZ_FULL */
6d6bc0ad867c4 kernel/sched.c      (Dhaval Giani               2008-05-30 14:23:45 +0200  706) #endif /* CONFIG_SMP */
18d95a2832c13 kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200  707) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700  708) #if defined(CONFIG_RT_GROUP_SCHED) || (defined(CONFIG_FAIR_GROUP_SCHED) && \
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700  709) 			(defined(CONFIG_SMP) || defined(CONFIG_CFS_BANDWIDTH)))
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  710) /*
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700  711)  * Iterate task_group tree rooted at *from, calling @down when first entering a
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700  712)  * node and @up when leaving it for the final time.
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700  713)  *
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700  714)  * Caller must hold rcu_lock or sufficient equivalent.
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  715)  */
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200  716) int walk_tg_tree_from(struct task_group *from,
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700  717) 			     tg_visitor down, tg_visitor up, void *data)
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  718) {
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  719) 	struct task_group *parent, *child;
eb755805f21bd kernel/sched.c      (Peter Zijlstra             2008-08-19 12:33:05 +0200  720) 	int ret;
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  721) 
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700  722) 	parent = from;
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700  723) 
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  724) down:
eb755805f21bd kernel/sched.c      (Peter Zijlstra             2008-08-19 12:33:05 +0200  725) 	ret = (*down)(parent, data);
eb755805f21bd kernel/sched.c      (Peter Zijlstra             2008-08-19 12:33:05 +0200  726) 	if (ret)
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700  727) 		goto out;
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  728) 	list_for_each_entry_rcu(child, &parent->children, siblings) {
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  729) 		parent = child;
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  730) 		goto down;
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  731) 
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  732) up:
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  733) 		continue;
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  734) 	}
eb755805f21bd kernel/sched.c      (Peter Zijlstra             2008-08-19 12:33:05 +0200  735) 	ret = (*up)(parent, data);
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700  736) 	if (ret || parent == from)
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700  737) 		goto out;
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  738) 
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  739) 	child = parent;
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  740) 	parent = parent->parent;
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  741) 	if (parent)
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  742) 		goto up;
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700  743) out:
eb755805f21bd kernel/sched.c      (Peter Zijlstra             2008-08-19 12:33:05 +0200  744) 	return ret;
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  745) }
c09595f63bb19 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:14 +0200  746) 
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200  747) int tg_nop(struct task_group *tg, void *data)
eb755805f21bd kernel/sched.c      (Peter Zijlstra             2008-08-19 12:33:05 +0200  748) {
e2b245f89ee3f kernel/sched.c      (Jan H. Schnherr           2011-08-01 11:03:28 +0200  749) 	return 0;
eb755805f21bd kernel/sched.c      (Peter Zijlstra             2008-08-19 12:33:05 +0200  750) }
18d95a2832c13 kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200  751) #endif
18d95a2832c13 kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200  752) 
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200  753) static void set_load_weight(struct task_struct *p, bool update_load)
45bf76df4814a kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  754) {
f05998d4b8063 kernel/sched.c      (Nikhil Rao                 2011-05-18 10:09:38 -0700  755) 	int prio = p->static_prio - MAX_RT_PRIO;
f05998d4b8063 kernel/sched.c      (Nikhil Rao                 2011-05-18 10:09:38 -0700  756) 	struct load_weight *load = &p->se.load;
f05998d4b8063 kernel/sched.c      (Nikhil Rao                 2011-05-18 10:09:38 -0700  757) 
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  758) 	/*
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  759) 	 * SCHED_IDLE tasks get minimal weight:
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  760) 	 */
1da1843f9f033 kernel/sched/core.c (Viresh Kumar               2018-11-05 16:51:55 +0530  761) 	if (task_has_idle_policy(p)) {
c8b281161dfa4 kernel/sched.c      (Nikhil Rao                 2011-05-18 14:37:48 -0700  762) 		load->weight = scale_load(WEIGHT_IDLEPRIO);
f05998d4b8063 kernel/sched.c      (Nikhil Rao                 2011-05-18 10:09:38 -0700  763) 		load->inv_weight = WMULT_IDLEPRIO;
4a465e3ebbc80 kernel/sched/core.c (Dietmar Eggemann           2018-08-03 15:05:38 +0100  764) 		p->se.runnable_weight = load->weight;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  765) 		return;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  766) 	}
71f8bd4600521 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  767) 
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200  768) 	/*
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200  769) 	 * SCHED_OTHER tasks have to update their load when changing their
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200  770) 	 * weight
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200  771) 	 */
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200  772) 	if (update_load && p->sched_class == &fair_sched_class) {
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200  773) 		reweight_task(p, prio);
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200  774) 	} else {
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200  775) 		load->weight = scale_load(sched_prio_to_weight[prio]);
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200  776) 		load->inv_weight = sched_prio_to_wmult[prio];
4a465e3ebbc80 kernel/sched/core.c (Dietmar Eggemann           2018-08-03 15:05:38 +0100  777) 		p->se.runnable_weight = load->weight;
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200  778) 	}
71f8bd4600521 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  779) }
71f8bd4600521 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200  780) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  781) #ifdef CONFIG_UCLAMP_TASK
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100  782) /*
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100  783)  * Serializes updates of utilization clamp values
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100  784)  *
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100  785)  * The (slow-path) user-space triggers utilization clamp value updates which
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100  786)  * can require updates on (fast-path) scheduler's data structures used to
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100  787)  * support enqueue/dequeue operations.
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100  788)  * While the per-CPU rq lock protects fast-path update operations, user-space
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100  789)  * requests are serialized using a mutex to reduce the risk of conflicting
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100  790)  * updates or API abuses.
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100  791)  */
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100  792) static DEFINE_MUTEX(uclamp_mutex);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100  793) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  794) /* Max allowed minimum utilization */
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  795) unsigned int sysctl_sched_uclamp_util_min = SCHED_CAPACITY_SCALE;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  796) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  797) /* Max allowed maximum utilization */
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  798) unsigned int sysctl_sched_uclamp_util_max = SCHED_CAPACITY_SCALE;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  799) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  800) /* All clamps are required to be less or equal than these values */
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  801) static struct uclamp_se uclamp_default[UCLAMP_CNT];
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  802) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  803) /* Integer rounded range for each bucket */
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  804) #define UCLAMP_BUCKET_DELTA DIV_ROUND_CLOSEST(SCHED_CAPACITY_SCALE, UCLAMP_BUCKETS)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  805) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  806) #define for_each_clamp_id(clamp_id) \
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  807) 	for ((clamp_id) = 0; (clamp_id) < UCLAMP_CNT; (clamp_id)++)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  808) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  809) static inline unsigned int uclamp_bucket_id(unsigned int clamp_value)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  810) {
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  811) 	return clamp_value / UCLAMP_BUCKET_DELTA;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  812) }
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  813) 
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  814) static inline unsigned int uclamp_bucket_base_value(unsigned int clamp_value)
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  815) {
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  816) 	return UCLAMP_BUCKET_DELTA * uclamp_bucket_id(clamp_value);
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  817) }
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  818) 
7763baace1b73 kernel/sched/core.c (Valentin Schneider         2019-11-15 10:39:08 +0000  819) static inline unsigned int uclamp_none(enum uclamp_id clamp_id)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  820) {
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  821) 	if (clamp_id == UCLAMP_MIN)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  822) 		return 0;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  823) 	return SCHED_CAPACITY_SCALE;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  824) }
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  825) 
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100  826) static inline void uclamp_se_set(struct uclamp_se *uc_se,
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100  827) 				 unsigned int value, bool user_defined)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  828) {
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  829) 	uc_se->value = value;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  830) 	uc_se->bucket_id = uclamp_bucket_id(value);
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100  831) 	uc_se->user_defined = user_defined;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  832) }
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  833) 
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  834) static inline unsigned int
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100  835) uclamp_idle_value(struct rq *rq, enum uclamp_id clamp_id,
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  836) 		  unsigned int clamp_value)
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  837) {
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  838) 	/*
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  839) 	 * Avoid blocked utilization pushing up the frequency when we go
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  840) 	 * idle (which drops the max-clamp) by retaining the last known
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  841) 	 * max-clamp.
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  842) 	 */
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  843) 	if (clamp_id == UCLAMP_MAX) {
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  844) 		rq->uclamp_flags |= UCLAMP_FLAG_IDLE;
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  845) 		return clamp_value;
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  846) 	}
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  847) 
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  848) 	return uclamp_none(UCLAMP_MIN);
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  849) }
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  850) 
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100  851) static inline void uclamp_idle_reset(struct rq *rq, enum uclamp_id clamp_id,
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  852) 				     unsigned int clamp_value)
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  853) {
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  854) 	/* Reset max-clamp retention only on idle exit */
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  855) 	if (!(rq->uclamp_flags & UCLAMP_FLAG_IDLE))
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  856) 		return;
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  857) 
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  858) 	WRITE_ONCE(rq->uclamp[clamp_id].value, clamp_value);
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  859) }
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  860) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  861) static inline
7763baace1b73 kernel/sched/core.c (Valentin Schneider         2019-11-15 10:39:08 +0000  862) unsigned int uclamp_rq_max_value(struct rq *rq, enum uclamp_id clamp_id,
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100  863) 				   unsigned int clamp_value)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  864) {
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  865) 	struct uclamp_bucket *bucket = rq->uclamp[clamp_id].bucket;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  866) 	int bucket_id = UCLAMP_BUCKETS - 1;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  867) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  868) 	/*
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  869) 	 * Since both min and max clamps are max aggregated, find the
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  870) 	 * top most bucket with tasks in.
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  871) 	 */
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  872) 	for ( ; bucket_id >= 0; bucket_id--) {
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  873) 		if (!bucket[bucket_id].tasks)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  874) 			continue;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  875) 		return bucket[bucket_id].value;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  876) 	}
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  877) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  878) 	/* No tasks -- default clamp values */
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  879) 	return uclamp_idle_value(rq, clamp_id, clamp_value);
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  880) }
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  881) 
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  882) static inline struct uclamp_se
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100  883) uclamp_tg_restrict(struct task_struct *p, enum uclamp_id clamp_id)
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  884) {
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  885) 	struct uclamp_se uc_req = p->uclamp_req[clamp_id];
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  886) #ifdef CONFIG_UCLAMP_TASK_GROUP
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  887) 	struct uclamp_se uc_max;
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  888) 
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  889) 	/*
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  890) 	 * Tasks in autogroups or root task group will be
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  891) 	 * restricted by system defaults.
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  892) 	 */
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  893) 	if (task_group_is_autogroup(task_group(p)))
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  894) 		return uc_req;
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  895) 	if (task_group(p) == &root_task_group)
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  896) 		return uc_req;
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  897) 
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  898) 	uc_max = task_group(p)->uclamp[clamp_id];
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  899) 	if (uc_req.value > uc_max.value || !uc_req.user_defined)
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  900) 		return uc_max;
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  901) #endif
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  902) 
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  903) 	return uc_req;
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  904) }
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  905) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  906) /*
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  907)  * The effective clamp bucket index of a task depends on, by increasing
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  908)  * priority:
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  909)  * - the task specific clamp value, when explicitly requested from userspace
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  910)  * - the task group effective clamp value, for tasks not either in the root
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  911)  *   group or in an autogroup
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  912)  * - the system default clamp value, defined by the sysadmin
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  913)  */
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  914) static inline struct uclamp_se
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100  915) uclamp_eff_get(struct task_struct *p, enum uclamp_id clamp_id)
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  916) {
3eac870a32472 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:09 +0100  917) 	struct uclamp_se uc_req = uclamp_tg_restrict(p, clamp_id);
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  918) 	struct uclamp_se uc_max = uclamp_default[clamp_id];
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  919) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  920) 	/* System default restrictions always apply */
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  921) 	if (unlikely(uc_req.value > uc_max.value))
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  922) 		return uc_max;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  923) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  924) 	return uc_req;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  925) }
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  926) 
686516b55e98e kernel/sched/core.c (Valentin Schneider         2019-12-11 11:38:48 +0000  927) unsigned long uclamp_eff_value(struct task_struct *p, enum uclamp_id clamp_id)
9d20ad7dfc9a5 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:11 +0100  928) {
9d20ad7dfc9a5 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:11 +0100  929) 	struct uclamp_se uc_eff;
9d20ad7dfc9a5 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:11 +0100  930) 
9d20ad7dfc9a5 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:11 +0100  931) 	/* Task currently refcounted: use back-annotated (effective) value */
9d20ad7dfc9a5 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:11 +0100  932) 	if (p->uclamp[clamp_id].active)
686516b55e98e kernel/sched/core.c (Valentin Schneider         2019-12-11 11:38:48 +0000  933) 		return (unsigned long)p->uclamp[clamp_id].value;
9d20ad7dfc9a5 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:11 +0100  934) 
9d20ad7dfc9a5 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:11 +0100  935) 	uc_eff = uclamp_eff_get(p, clamp_id);
9d20ad7dfc9a5 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:11 +0100  936) 
686516b55e98e kernel/sched/core.c (Valentin Schneider         2019-12-11 11:38:48 +0000  937) 	return (unsigned long)uc_eff.value;
9d20ad7dfc9a5 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:11 +0100  938) }
9d20ad7dfc9a5 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:11 +0100  939) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  940) /*
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  941)  * When a task is enqueued on a rq, the clamp bucket currently defined by the
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  942)  * task's uclamp::bucket_id is refcounted on that rq. This also immediately
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  943)  * updates the rq's clamp value if required.
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  944)  *
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  945)  * Tasks can have a task-specific value requested from user-space, track
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  946)  * within each bucket the maximum value for tasks refcounted in it.
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  947)  * This "local max aggregation" allows to track the exact "requested" value
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  948)  * for each bucket when all its RUNNABLE tasks require the same clamp.
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  949)  */
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  950) static inline void uclamp_rq_inc_id(struct rq *rq, struct task_struct *p,
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100  951) 				    enum uclamp_id clamp_id)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  952) {
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  953) 	struct uclamp_rq *uc_rq = &rq->uclamp[clamp_id];
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  954) 	struct uclamp_se *uc_se = &p->uclamp[clamp_id];
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  955) 	struct uclamp_bucket *bucket;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  956) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  957) 	lockdep_assert_held(&rq->lock);
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  958) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  959) 	/* Update task effective clamp */
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  960) 	p->uclamp[clamp_id] = uclamp_eff_get(p, clamp_id);
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  961) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  962) 	bucket = &uc_rq->bucket[uc_se->bucket_id];
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  963) 	bucket->tasks++;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100  964) 	uc_se->active = true;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  965) 
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  966) 	uclamp_idle_reset(rq, clamp_id, uc_se->value);
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  967) 
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  968) 	/*
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  969) 	 * Local max aggregation: rq buckets always track the max
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  970) 	 * "requested" clamp value of its RUNNABLE tasks.
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  971) 	 */
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  972) 	if (bucket->tasks == 1 || uc_se->value > bucket->value)
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  973) 		bucket->value = uc_se->value;
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  974) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  975) 	if (uc_se->value > READ_ONCE(uc_rq->value))
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100  976) 		WRITE_ONCE(uc_rq->value, uc_se->value);
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  977) }
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  978) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  979) /*
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  980)  * When a task is dequeued from a rq, the clamp bucket refcounted by the task
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  981)  * is released. If this is the last task reference counting the rq's max
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  982)  * active clamp value, then the rq's clamp value is updated.
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  983)  *
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  984)  * Both refcounted tasks and rq's cached clamp values are expected to be
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  985)  * always valid. If it's detected they are not, as defensive programming,
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  986)  * enforce the expected state and warn.
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  987)  */
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  988) static inline void uclamp_rq_dec_id(struct rq *rq, struct task_struct *p,
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100  989) 				    enum uclamp_id clamp_id)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  990) {
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  991) 	struct uclamp_rq *uc_rq = &rq->uclamp[clamp_id];
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  992) 	struct uclamp_se *uc_se = &p->uclamp[clamp_id];
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  993) 	struct uclamp_bucket *bucket;
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100  994) 	unsigned int bkt_clamp;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  995) 	unsigned int rq_clamp;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  996) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  997) 	lockdep_assert_held(&rq->lock);
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  998) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100  999) 	bucket = &uc_rq->bucket[uc_se->bucket_id];
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1000) 	SCHED_WARN_ON(!bucket->tasks);
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1001) 	if (likely(bucket->tasks))
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1002) 		bucket->tasks--;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1003) 	uc_se->active = false;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1004) 
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100 1005) 	/*
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100 1006) 	 * Keep "local max aggregation" simple and accept to (possibly)
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100 1007) 	 * overboost some RUNNABLE tasks in the same bucket.
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100 1008) 	 * The rq clamp bucket value is reset to its base value whenever
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100 1009) 	 * there are no more RUNNABLE tasks refcounting it.
60daf9c194106 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:03 +0100 1010) 	 */
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1011) 	if (likely(bucket->tasks))
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1012) 		return;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1013) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1014) 	rq_clamp = READ_ONCE(uc_rq->value);
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1015) 	/*
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1016) 	 * Defensive programming: this should never happen. If it happens,
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1017) 	 * e.g. due to future modification, warn and fixup the expected value.
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1018) 	 */
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1019) 	SCHED_WARN_ON(bucket->value > rq_clamp);
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100 1020) 	if (bucket->value >= rq_clamp) {
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100 1021) 		bkt_clamp = uclamp_rq_max_value(rq, clamp_id, uc_se->value);
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100 1022) 		WRITE_ONCE(uc_rq->value, bkt_clamp);
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100 1023) 	}
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1024) }
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1025) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1026) static inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1027) {
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100 1028) 	enum uclamp_id clamp_id;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1029) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1030) 	if (unlikely(!p->sched_class->uclamp_enabled))
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1031) 		return;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1032) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1033) 	for_each_clamp_id(clamp_id)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1034) 		uclamp_rq_inc_id(rq, p, clamp_id);
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100 1035) 
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100 1036) 	/* Reset clamp idle holding when there is one RUNNABLE task */
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100 1037) 	if (rq->uclamp_flags & UCLAMP_FLAG_IDLE)
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100 1038) 		rq->uclamp_flags &= ~UCLAMP_FLAG_IDLE;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1039) }
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1040) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1041) static inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1042) {
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100 1043) 	enum uclamp_id clamp_id;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1044) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1045) 	if (unlikely(!p->sched_class->uclamp_enabled))
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1046) 		return;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1047) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1048) 	for_each_clamp_id(clamp_id)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1049) 		uclamp_rq_dec_id(rq, p, clamp_id);
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1050) }
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1051) 
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1052) static inline void
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100 1053) uclamp_update_active(struct task_struct *p, enum uclamp_id clamp_id)
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1054) {
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1055) 	struct rq_flags rf;
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1056) 	struct rq *rq;
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1057) 
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1058) 	/*
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1059) 	 * Lock the task and the rq where the task is (or was) queued.
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1060) 	 *
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1061) 	 * We might lock the (previous) rq of a !RUNNABLE task, but that's the
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1062) 	 * price to pay to safely serialize util_{min,max} updates with
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1063) 	 * enqueues, dequeues and migration operations.
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1064) 	 * This is the same locking schema used by __set_cpus_allowed_ptr().
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1065) 	 */
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1066) 	rq = task_rq_lock(p, &rf);
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1067) 
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1068) 	/*
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1069) 	 * Setting the clamp bucket is serialized by task_rq_lock().
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1070) 	 * If the task is not yet RUNNABLE and its task_struct is not
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1071) 	 * affecting a valid clamp bucket, the next time it's enqueued,
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1072) 	 * it will already see the updated clamp bucket value.
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1073) 	 */
6e1ff0773f49c kernel/sched/core.c (Qais Yousef                2019-11-14 21:10:52 +0000 1074) 	if (p->uclamp[clamp_id].active) {
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1075) 		uclamp_rq_dec_id(rq, p, clamp_id);
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1076) 		uclamp_rq_inc_id(rq, p, clamp_id);
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1077) 	}
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1078) 
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1079) 	task_rq_unlock(rq, p, &rf);
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1080) }
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1081) 
e3b8b6a0d12cc kernel/sched/core.c (Qais Yousef                2019-11-05 11:22:12 +0000 1082) #ifdef CONFIG_UCLAMP_TASK_GROUP
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1083) static inline void
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1084) uclamp_update_active_tasks(struct cgroup_subsys_state *css,
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1085) 			   unsigned int clamps)
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1086) {
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100 1087) 	enum uclamp_id clamp_id;
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1088) 	struct css_task_iter it;
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1089) 	struct task_struct *p;
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1090) 
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1091) 	css_task_iter_start(css, 0, &it);
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1092) 	while ((p = css_task_iter_next(&it))) {
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1093) 		for_each_clamp_id(clamp_id) {
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1094) 			if ((0x1 << clamp_id) & clamps)
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1095) 				uclamp_update_active(p, clamp_id);
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1096) 		}
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1097) 	}
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1098) 	css_task_iter_end(&it);
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1099) }
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 1100) 
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1101) static void cpu_util_update_eff(struct cgroup_subsys_state *css);
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1102) static void uclamp_update_root_tg(void)
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1103) {
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1104) 	struct task_group *tg = &root_task_group;
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1105) 
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1106) 	uclamp_se_set(&tg->uclamp_req[UCLAMP_MIN],
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1107) 		      sysctl_sched_uclamp_util_min, false);
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1108) 	uclamp_se_set(&tg->uclamp_req[UCLAMP_MAX],
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1109) 		      sysctl_sched_uclamp_util_max, false);
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1110) 
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1111) 	rcu_read_lock();
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1112) 	cpu_util_update_eff(&root_task_group.css);
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1113) 	rcu_read_unlock();
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1114) }
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1115) #else
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1116) static void uclamp_update_root_tg(void) { }
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1117) #endif
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1118) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1119) int sysctl_sched_uclamp_handler(struct ctl_table *table, int write,
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1120) 				void __user *buffer, size_t *lenp,
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1121) 				loff_t *ppos)
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1122) {
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1123) 	bool update_root_tg = false;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1124) 	int old_min, old_max;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1125) 	int result;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1126) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 1127) 	mutex_lock(&uclamp_mutex);
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1128) 	old_min = sysctl_sched_uclamp_util_min;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1129) 	old_max = sysctl_sched_uclamp_util_max;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1130) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1131) 	result = proc_dointvec(table, write, buffer, lenp, ppos);
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1132) 	if (result)
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1133) 		goto undo;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1134) 	if (!write)
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1135) 		goto done;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1136) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1137) 	if (sysctl_sched_uclamp_util_min > sysctl_sched_uclamp_util_max ||
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1138) 	    sysctl_sched_uclamp_util_max > SCHED_CAPACITY_SCALE) {
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1139) 		result = -EINVAL;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1140) 		goto undo;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1141) 	}
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1142) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1143) 	if (old_min != sysctl_sched_uclamp_util_min) {
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1144) 		uclamp_se_set(&uclamp_default[UCLAMP_MIN],
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1145) 			      sysctl_sched_uclamp_util_min, false);
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1146) 		update_root_tg = true;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1147) 	}
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1148) 	if (old_max != sysctl_sched_uclamp_util_max) {
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1149) 		uclamp_se_set(&uclamp_default[UCLAMP_MAX],
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1150) 			      sysctl_sched_uclamp_util_max, false);
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1151) 		update_root_tg = true;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1152) 	}
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1153) 
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1154) 	if (update_root_tg)
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1155) 		uclamp_update_root_tg();
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1156) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1157) 	/*
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1158) 	 * We update all RUNNABLE tasks only when task groups are in use.
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1159) 	 * Otherwise, keep it simple and do just a lazy update at each next
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1160) 	 * task enqueue time.
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1161) 	 */
7274a5c1bbec4 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:08 +0100 1162) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1163) 	goto done;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1164) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1165) undo:
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1166) 	sysctl_sched_uclamp_util_min = old_min;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1167) 	sysctl_sched_uclamp_util_max = old_max;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1168) done:
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 1169) 	mutex_unlock(&uclamp_mutex);
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1170) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1171) 	return result;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1172) }
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1173) 
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1174) static int uclamp_validate(struct task_struct *p,
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1175) 			   const struct sched_attr *attr)
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1176) {
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1177) 	unsigned int lower_bound = p->uclamp_req[UCLAMP_MIN].value;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1178) 	unsigned int upper_bound = p->uclamp_req[UCLAMP_MAX].value;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1179) 
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1180) 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN)
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1181) 		lower_bound = attr->sched_util_min;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1182) 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX)
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1183) 		upper_bound = attr->sched_util_max;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1184) 
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1185) 	if (lower_bound > upper_bound)
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1186) 		return -EINVAL;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1187) 	if (upper_bound > SCHED_CAPACITY_SCALE)
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1188) 		return -EINVAL;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1189) 
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1190) 	return 0;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1191) }
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1192) 
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1193) static void __setscheduler_uclamp(struct task_struct *p,
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1194) 				  const struct sched_attr *attr)
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1195) {
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100 1196) 	enum uclamp_id clamp_id;
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1197) 
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1198) 	/*
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1199) 	 * On scheduling class change, reset to default clamps for tasks
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1200) 	 * without a task-specific value.
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1201) 	 */
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1202) 	for_each_clamp_id(clamp_id) {
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1203) 		struct uclamp_se *uc_se = &p->uclamp_req[clamp_id];
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1204) 		unsigned int clamp_value = uclamp_none(clamp_id);
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1205) 
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1206) 		/* Keep using defined clamps across class changes */
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1207) 		if (uc_se->user_defined)
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1208) 			continue;
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1209) 
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1210) 		/* By default, RT tasks always get 100% boost */
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1211) 		if (unlikely(rt_task(p) && clamp_id == UCLAMP_MIN))
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1212) 			clamp_value = uclamp_none(UCLAMP_MAX);
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1213) 
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1214) 		uclamp_se_set(uc_se, clamp_value, false);
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1215) 	}
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1216) 
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1217) 	if (likely(!(attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)))
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1218) 		return;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1219) 
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1220) 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN) {
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1221) 		uclamp_se_set(&p->uclamp_req[UCLAMP_MIN],
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1222) 			      attr->sched_util_min, true);
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1223) 	}
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1224) 
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1225) 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX) {
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1226) 		uclamp_se_set(&p->uclamp_req[UCLAMP_MAX],
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1227) 			      attr->sched_util_max, true);
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1228) 	}
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1229) }
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1230) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1231) static void uclamp_fork(struct task_struct *p)
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1232) {
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100 1233) 	enum uclamp_id clamp_id;
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1234) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1235) 	for_each_clamp_id(clamp_id)
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1236) 		p->uclamp[clamp_id].active = false;
a87498ace58e2 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:08 +0100 1237) 
a87498ace58e2 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:08 +0100 1238) 	if (likely(!p->sched_reset_on_fork))
a87498ace58e2 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:08 +0100 1239) 		return;
a87498ace58e2 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:08 +0100 1240) 
a87498ace58e2 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:08 +0100 1241) 	for_each_clamp_id(clamp_id) {
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1242) 		unsigned int clamp_value = uclamp_none(clamp_id);
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1243) 
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1244) 		/* By default, RT tasks always get 100% boost */
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1245) 		if (unlikely(rt_task(p) && clamp_id == UCLAMP_MIN))
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1246) 			clamp_value = uclamp_none(UCLAMP_MAX);
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1247) 
1a00d999971c7 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:09 +0100 1248) 		uclamp_se_set(&p->uclamp_req[clamp_id], clamp_value, false);
a87498ace58e2 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:08 +0100 1249) 	}
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1250) }
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1251) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1252) static void __init init_uclamp(void)
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1253) {
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1254) 	struct uclamp_se uc_max = {};
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100 1255) 	enum uclamp_id clamp_id;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1256) 	int cpu;
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1257) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 1258) 	mutex_init(&uclamp_mutex);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 1259) 
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100 1260) 	for_each_possible_cpu(cpu) {
dcd6dffb0a757 kernel/sched/core.c (Li Guanglei                2019-12-25 15:44:04 +0800 1261) 		memset(&cpu_rq(cpu)->uclamp, 0,
dcd6dffb0a757 kernel/sched/core.c (Li Guanglei                2019-12-25 15:44:04 +0800 1262) 				sizeof(struct uclamp_rq)*UCLAMP_CNT);
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100 1263) 		cpu_rq(cpu)->uclamp_flags = 0;
e496187da7107 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:04 +0100 1264) 	}
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1265) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1266) 	for_each_clamp_id(clamp_id) {
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1267) 		uclamp_se_set(&init_task.uclamp_req[clamp_id],
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1268) 			      uclamp_none(clamp_id), false);
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1269) 	}
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1270) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1271) 	/* System defaults allow max clamp values for both indexes */
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1272) 	uclamp_se_set(&uc_max, uclamp_none(UCLAMP_MAX), false);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 1273) 	for_each_clamp_id(clamp_id) {
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1274) 		uclamp_default[clamp_id] = uc_max;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 1275) #ifdef CONFIG_UCLAMP_TASK_GROUP
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 1276) 		root_task_group.uclamp_req[clamp_id] = uc_max;
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 1277) 		root_task_group.uclamp[clamp_id] = uc_max;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 1278) #endif
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 1279) 	}
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1280) }
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1281) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1282) #else /* CONFIG_UCLAMP_TASK */
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1283) static inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p) { }
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1284) static inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p) { }
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1285) static inline int uclamp_validate(struct task_struct *p,
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1286) 				  const struct sched_attr *attr)
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1287) {
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1288) 	return -EOPNOTSUPP;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1289) }
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1290) static void __setscheduler_uclamp(struct task_struct *p,
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 1291) 				  const struct sched_attr *attr) { }
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 1292) static inline void uclamp_fork(struct task_struct *p) { }
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1293) static inline void init_uclamp(void) { }
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1294) #endif /* CONFIG_UCLAMP_TASK */
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1295) 
1de64443d755f kernel/sched/core.c (Peter Zijlstra             2015-09-30 17:44:13 +0200 1296) static inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
2087a1ad822cd kernel/sched.c      (Gregory Haskins            2008-06-27 14:30:00 -0600 1297) {
0a67d1ee30ef1 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:29:45 +0200 1298) 	if (!(flags & ENQUEUE_NOCLOCK))
0a67d1ee30ef1 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:29:45 +0200 1299) 		update_rq_clock(rq);
0a67d1ee30ef1 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:29:45 +0200 1300) 
eb414681d5a07 kernel/sched/core.c (Johannes Weiner            2018-10-26 15:06:27 -0700 1301) 	if (!(flags & ENQUEUE_RESTORE)) {
1de64443d755f kernel/sched/core.c (Peter Zijlstra             2015-09-30 17:44:13 +0200 1302) 		sched_info_queued(rq, p);
eb414681d5a07 kernel/sched/core.c (Johannes Weiner            2018-10-26 15:06:27 -0700 1303) 		psi_enqueue(p, flags & ENQUEUE_WAKEUP);
eb414681d5a07 kernel/sched/core.c (Johannes Weiner            2018-10-26 15:06:27 -0700 1304) 	}
0a67d1ee30ef1 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:29:45 +0200 1305) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1306) 	uclamp_rq_inc(rq, p);
371fd7e7a56a5 kernel/sched.c      (Peter Zijlstra             2010-03-24 16:38:48 +0100 1307) 	p->sched_class->enqueue_task(rq, p, flags);
71f8bd4600521 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1308) }
71f8bd4600521 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1309) 
1de64443d755f kernel/sched/core.c (Peter Zijlstra             2015-09-30 17:44:13 +0200 1310) static inline void dequeue_task(struct rq *rq, struct task_struct *p, int flags)
71f8bd4600521 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1311) {
0a67d1ee30ef1 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:29:45 +0200 1312) 	if (!(flags & DEQUEUE_NOCLOCK))
0a67d1ee30ef1 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:29:45 +0200 1313) 		update_rq_clock(rq);
0a67d1ee30ef1 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:29:45 +0200 1314) 
eb414681d5a07 kernel/sched/core.c (Johannes Weiner            2018-10-26 15:06:27 -0700 1315) 	if (!(flags & DEQUEUE_SAVE)) {
1de64443d755f kernel/sched/core.c (Peter Zijlstra             2015-09-30 17:44:13 +0200 1316) 		sched_info_dequeued(rq, p);
eb414681d5a07 kernel/sched/core.c (Johannes Weiner            2018-10-26 15:06:27 -0700 1317) 		psi_dequeue(p, flags & DEQUEUE_SLEEP);
eb414681d5a07 kernel/sched/core.c (Johannes Weiner            2018-10-26 15:06:27 -0700 1318) 	}
0a67d1ee30ef1 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:29:45 +0200 1319) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 1320) 	uclamp_rq_dec(rq, p);
371fd7e7a56a5 kernel/sched.c      (Peter Zijlstra             2010-03-24 16:38:48 +0100 1321) 	p->sched_class->dequeue_task(rq, p, flags);
71f8bd4600521 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1322) }
71f8bd4600521 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1323) 
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 1324) void activate_task(struct rq *rq, struct task_struct *p, int flags)
1e3c88bdeb126 kernel/sched.c      (Peter Zijlstra             2009-12-17 17:00:43 +0100 1325) {
1e3c88bdeb126 kernel/sched.c      (Peter Zijlstra             2009-12-17 17:00:43 +0100 1326) 	if (task_contributes_to_load(p))
1e3c88bdeb126 kernel/sched.c      (Peter Zijlstra             2009-12-17 17:00:43 +0100 1327) 		rq->nr_uninterruptible--;
1e3c88bdeb126 kernel/sched.c      (Peter Zijlstra             2009-12-17 17:00:43 +0100 1328) 
371fd7e7a56a5 kernel/sched.c      (Peter Zijlstra             2010-03-24 16:38:48 +0100 1329) 	enqueue_task(rq, p, flags);
7dd7788411646 kernel/sched/core.c (Peter Zijlstra             2019-04-09 09:59:05 +0200 1330) 
7dd7788411646 kernel/sched/core.c (Peter Zijlstra             2019-04-09 09:59:05 +0200 1331) 	p->on_rq = TASK_ON_RQ_QUEUED;
1e3c88bdeb126 kernel/sched.c      (Peter Zijlstra             2009-12-17 17:00:43 +0100 1332) }
1e3c88bdeb126 kernel/sched.c      (Peter Zijlstra             2009-12-17 17:00:43 +0100 1333) 
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 1334) void deactivate_task(struct rq *rq, struct task_struct *p, int flags)
1e3c88bdeb126 kernel/sched.c      (Peter Zijlstra             2009-12-17 17:00:43 +0100 1335) {
7dd7788411646 kernel/sched/core.c (Peter Zijlstra             2019-04-09 09:59:05 +0200 1336) 	p->on_rq = (flags & DEQUEUE_SLEEP) ? 0 : TASK_ON_RQ_MIGRATING;
7dd7788411646 kernel/sched/core.c (Peter Zijlstra             2019-04-09 09:59:05 +0200 1337) 
1e3c88bdeb126 kernel/sched.c      (Peter Zijlstra             2009-12-17 17:00:43 +0100 1338) 	if (task_contributes_to_load(p))
1e3c88bdeb126 kernel/sched.c      (Peter Zijlstra             2009-12-17 17:00:43 +0100 1339) 		rq->nr_uninterruptible++;
1e3c88bdeb126 kernel/sched.c      (Peter Zijlstra             2009-12-17 17:00:43 +0100 1340) 
371fd7e7a56a5 kernel/sched.c      (Peter Zijlstra             2010-03-24 16:38:48 +0100 1341) 	dequeue_task(rq, p, flags);
1e3c88bdeb126 kernel/sched.c      (Peter Zijlstra             2009-12-17 17:00:43 +0100 1342) }
1e3c88bdeb126 kernel/sched.c      (Peter Zijlstra             2009-12-17 17:00:43 +0100 1343) 
14531189f0a10 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1344) /*
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1345)  * __normal_prio - return the priority that is based on the static prio
14531189f0a10 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1346)  */
14531189f0a10 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1347) static inline int __normal_prio(struct task_struct *p)
14531189f0a10 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1348) {
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1349) 	return p->static_prio;
14531189f0a10 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1350) }
14531189f0a10 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1351) 
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1352) /*
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1353)  * Calculate the expected normal priority: i.e. priority
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1354)  * without taking RT-inheritance into account. Might be
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1355)  * boosted by interactivity modifiers. Changes upon fork,
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1356)  * setprio syscalls, and whenever the interactivity
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1357)  * estimator recalculates.
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1358)  */
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 1359) static inline int normal_prio(struct task_struct *p)
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1360) {
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1361) 	int prio;
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1362) 
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 1363) 	if (task_has_dl_policy(p))
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 1364) 		prio = MAX_DL_PRIO-1;
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 1365) 	else if (task_has_rt_policy(p))
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1366) 		prio = MAX_RT_PRIO-1 - p->rt_priority;
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1367) 	else
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1368) 		prio = __normal_prio(p);
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1369) 	return prio;
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1370) }
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1371) 
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1372) /*
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1373)  * Calculate the current priority, i.e. the priority
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1374)  * taken into account by the scheduler. This value might
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1375)  * be boosted by RT tasks, or might be boosted by
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1376)  * interactivity modifiers. Will be RT if the task got
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1377)  * RT-boosted. If not then it returns p->normal_prio.
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1378)  */
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 1379) static int effective_prio(struct task_struct *p)
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1380) {
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1381) 	p->normal_prio = normal_prio(p);
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1382) 	/*
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1383) 	 * If we are RT tasks or we were boosted to RT priority,
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1384) 	 * keep the priority unchanged. Otherwise, update priority
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1385) 	 * to the normal priority:
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1386) 	 */
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1387) 	if (!rt_prio(p->prio))
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1388) 		return p->normal_prio;
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1389) 	return p->prio;
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1390) }
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 1391) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1392) /**
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1393)  * task_curr - is this task currently executing on a CPU?
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1394)  * @p: the task in question.
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 1395)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 1396)  * Return: 1 if the task is currently executing. 0 otherwise.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1397)  */
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 1398) inline int task_curr(const struct task_struct *p)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1399) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1400) 	return cpu_curr(task_cpu(p)) == p;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1401) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1402) 
67dfa1b756f25 kernel/sched/core.c (Kirill Tkhai               2014-10-27 17:40:52 +0300 1403) /*
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 1404)  * switched_from, switched_to and prio_changed must _NOT_ drop rq->lock,
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 1405)  * use the balance_callback list if you want balancing.
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 1406)  *
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 1407)  * this means any call to check_class_changed() must be followed by a call to
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 1408)  * balance_callback().
67dfa1b756f25 kernel/sched/core.c (Kirill Tkhai               2014-10-27 17:40:52 +0300 1409)  */
cb46984504048 kernel/sched.c      (Steven Rostedt             2008-01-25 21:08:22 +0100 1410) static inline void check_class_changed(struct rq *rq, struct task_struct *p,
cb46984504048 kernel/sched.c      (Steven Rostedt             2008-01-25 21:08:22 +0100 1411) 				       const struct sched_class *prev_class,
da7a735e51f96 kernel/sched.c      (Peter Zijlstra             2011-01-17 17:03:27 +0100 1412) 				       int oldprio)
cb46984504048 kernel/sched.c      (Steven Rostedt             2008-01-25 21:08:22 +0100 1413) {
cb46984504048 kernel/sched.c      (Steven Rostedt             2008-01-25 21:08:22 +0100 1414) 	if (prev_class != p->sched_class) {
cb46984504048 kernel/sched.c      (Steven Rostedt             2008-01-25 21:08:22 +0100 1415) 		if (prev_class->switched_from)
da7a735e51f96 kernel/sched.c      (Peter Zijlstra             2011-01-17 17:03:27 +0100 1416) 			prev_class->switched_from(rq, p);
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 1417) 
da7a735e51f96 kernel/sched.c      (Peter Zijlstra             2011-01-17 17:03:27 +0100 1418) 		p->sched_class->switched_to(rq, p);
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 1419) 	} else if (oldprio != p->prio || dl_task(p))
da7a735e51f96 kernel/sched.c      (Peter Zijlstra             2011-01-17 17:03:27 +0100 1420) 		p->sched_class->prio_changed(rq, p, oldprio);
cb46984504048 kernel/sched.c      (Steven Rostedt             2008-01-25 21:08:22 +0100 1421) }
cb46984504048 kernel/sched.c      (Steven Rostedt             2008-01-25 21:08:22 +0100 1422) 
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 1423) void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags)
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1424) {
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1425) 	const struct sched_class *class;
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1426) 
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1427) 	if (p->sched_class == rq->curr->sched_class) {
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1428) 		rq->curr->sched_class->check_preempt_curr(rq, p, flags);
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1429) 	} else {
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1430) 		for_each_class(class) {
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1431) 			if (class == rq->curr->sched_class)
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1432) 				break;
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1433) 			if (class == p->sched_class) {
8875125efe840 kernel/sched/core.c (Kirill Tkhai               2014-06-29 00:03:57 +0400 1434) 				resched_curr(rq);
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1435) 				break;
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1436) 			}
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1437) 		}
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1438) 	}
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1439) 
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1440) 	/*
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1441) 	 * A queue event has occurred, and we're going to schedule.  In
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1442) 	 * this case, we can save a useless back to back clock update.
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1443) 	 */
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 1444) 	if (task_on_rq_queued(rq->curr) && test_tsk_need_resched(rq->curr))
adcc8da8859be kernel/sched/core.c (Davidlohr Bueso            2018-04-04 09:15:39 -0700 1445) 		rq_clock_skip_update(rq);
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1446) }
1e5a74059f905 kernel/sched.c      (Peter Zijlstra             2010-10-31 12:37:04 +0100 1447) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1448) #ifdef CONFIG_SMP
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1449) 
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1450) /*
bee9853932e90 kernel/sched/core.c (Joel Savitz                2019-03-06 20:13:33 -0500 1451)  * Per-CPU kthreads are allowed to run on !active && online CPUs, see
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1452)  * __set_cpus_allowed_ptr() and select_fallback_rq().
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1453)  */
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1454) static inline bool is_cpu_allowed(struct task_struct *p, int cpu)
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1455) {
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 1456) 	if (!cpumask_test_cpu(cpu, p->cpus_ptr))
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1457) 		return false;
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1458) 
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1459) 	if (is_per_cpu_kthread(p))
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1460) 		return cpu_online(cpu);
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1461) 
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1462) 	return cpu_active(cpu);
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1463) }
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1464) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1465) /*
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1466)  * This is how migration works:
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1467)  *
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1468)  * 1) we invoke migration_cpu_stop() on the target CPU using
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1469)  *    stop_one_cpu().
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1470)  * 2) stopper starts to run (implicitly forcing the migrated thread
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1471)  *    off the CPU)
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1472)  * 3) it checks whether the migrated task is still in the wrong runqueue.
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1473)  * 4) if it's in the wrong runqueue then the migration thread removes
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1474)  *    it and puts it into the right queue.
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1475)  * 5) stopper completes and stop_one_cpu() returns and the migration
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1476)  *    is done.
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1477)  */
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1478) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1479) /*
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1480)  * move_queued_task - move a queued task to new rq.
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1481)  *
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1482)  * Returns (locked) new rq. Old rq's lock is released.
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1483)  */
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1484) static struct rq *move_queued_task(struct rq *rq, struct rq_flags *rf,
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1485) 				   struct task_struct *p, int new_cpu)
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1486) {
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1487) 	lockdep_assert_held(&rq->lock);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1488) 
c546951d9c930 kernel/sched/core.c (Andrea Parri               2019-01-21 16:52:40 +0100 1489) 	WRITE_ONCE(p->on_rq, TASK_ON_RQ_MIGRATING);
15ff991e80475 kernel/sched/core.c (Peter Zijlstra             2016-10-05 17:59:32 +0200 1490) 	dequeue_task(rq, p, DEQUEUE_NOCLOCK);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1491) 	set_task_cpu(p, new_cpu);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1492) 	rq_unlock(rq, rf);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1493) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1494) 	rq = cpu_rq(new_cpu);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1495) 
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1496) 	rq_lock(rq, rf);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1497) 	BUG_ON(task_cpu(p) != new_cpu);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1498) 	enqueue_task(rq, p, 0);
3ea94de15ce9f kernel/sched/core.c (Joonwoo Park               2015-11-12 19:38:54 -0800 1499) 	p->on_rq = TASK_ON_RQ_QUEUED;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1500) 	check_preempt_curr(rq, p, 0);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1501) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1502) 	return rq;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1503) }
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1504) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1505) struct migration_arg {
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1506) 	struct task_struct *task;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1507) 	int dest_cpu;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1508) };
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1509) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1510) /*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 1511)  * Move (not current) task off this CPU, onto the destination CPU. We're doing
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1512)  * this because either it can't run here any more (set_cpus_allowed()
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1513)  * away from this CPU, or CPU going down), or because we're
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1514)  * attempting to rebalance this task on exec (sched_exec).
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1515)  *
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1516)  * So we race with normal scheduler movements, but that's OK, as long
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1517)  * as the task is no longer on this CPU.
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1518)  */
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1519) static struct rq *__migrate_task(struct rq *rq, struct rq_flags *rf,
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1520) 				 struct task_struct *p, int dest_cpu)
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1521) {
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1522) 	/* Affinity changed (again). */
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 1523) 	if (!is_cpu_allowed(p, dest_cpu))
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 1524) 		return rq;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1525) 
15ff991e80475 kernel/sched/core.c (Peter Zijlstra             2016-10-05 17:59:32 +0200 1526) 	update_rq_clock(rq);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1527) 	rq = move_queued_task(rq, rf, p, dest_cpu);
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 1528) 
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 1529) 	return rq;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1530) }
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1531) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1532) /*
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1533)  * migration_cpu_stop - this will be executed by a highprio stopper thread
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1534)  * and performs thread migration by bumping thread off CPU then
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1535)  * 'pushing' onto another runqueue.
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1536)  */
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1537) static int migration_cpu_stop(void *data)
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1538) {
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1539) 	struct migration_arg *arg = data;
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 1540) 	struct task_struct *p = arg->task;
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 1541) 	struct rq *rq = this_rq();
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1542) 	struct rq_flags rf;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1543) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1544) 	/*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 1545) 	 * The original target CPU might have gone down and we might
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 1546) 	 * be on another CPU but it doesn't matter.
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1547) 	 */
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1548) 	local_irq_disable();
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1549) 	/*
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1550) 	 * We need to explicitly wake pending tasks before running
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 1551) 	 * __migrate_task() such that we will not miss enforcing cpus_ptr
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1552) 	 * during wakeups, see set_cpus_allowed_ptr()'s TASK_WAKING test.
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1553) 	 */
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1554) 	sched_ttwu_pending();
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 1555) 
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 1556) 	raw_spin_lock(&p->pi_lock);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1557) 	rq_lock(rq, &rf);
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 1558) 	/*
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 1559) 	 * If task_rq(p) != rq, it cannot be migrated here, because we're
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 1560) 	 * holding rq->lock, if p->on_rq == 0 it cannot get enqueued because
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 1561) 	 * we're holding p->pi_lock.
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 1562) 	 */
bf89a304722f6 kernel/sched/core.c (Cheng Chao                 2016-09-14 10:01:50 +0800 1563) 	if (task_rq(p) == rq) {
bf89a304722f6 kernel/sched/core.c (Cheng Chao                 2016-09-14 10:01:50 +0800 1564) 		if (task_on_rq_queued(p))
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1565) 			rq = __migrate_task(rq, &rf, p, arg->dest_cpu);
bf89a304722f6 kernel/sched/core.c (Cheng Chao                 2016-09-14 10:01:50 +0800 1566) 		else
bf89a304722f6 kernel/sched/core.c (Cheng Chao                 2016-09-14 10:01:50 +0800 1567) 			p->wake_cpu = arg->dest_cpu;
bf89a304722f6 kernel/sched/core.c (Cheng Chao                 2016-09-14 10:01:50 +0800 1568) 	}
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1569) 	rq_unlock(rq, &rf);
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 1570) 	raw_spin_unlock(&p->pi_lock);
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 1571) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1572) 	local_irq_enable();
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1573) 	return 0;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1574) }
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1575) 
c5b2803840817 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:35 +0200 1576) /*
c5b2803840817 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:35 +0200 1577)  * sched_class::set_cpus_allowed must do the below, but is not required to
c5b2803840817 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:35 +0200 1578)  * actually call this function.
c5b2803840817 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:35 +0200 1579)  */
c5b2803840817 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:35 +0200 1580) void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask)
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1581) {
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 1582) 	cpumask_copy(&p->cpus_mask, new_mask);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1583) 	p->nr_cpus_allowed = cpumask_weight(new_mask);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1584) }
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1585) 
c5b2803840817 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:35 +0200 1586) void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
c5b2803840817 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:35 +0200 1587) {
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1588) 	struct rq *rq = task_rq(p);
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1589) 	bool queued, running;
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1590) 
c5b2803840817 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:35 +0200 1591) 	lockdep_assert_held(&p->pi_lock);
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1592) 
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1593) 	queued = task_on_rq_queued(p);
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1594) 	running = task_current(rq, p);
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1595) 
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1596) 	if (queued) {
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1597) 		/*
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1598) 		 * Because __kthread_bind() calls this on blocked tasks without
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1599) 		 * holding rq->lock.
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1600) 		 */
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1601) 		lockdep_assert_held(&rq->lock);
7a57f32a4d5c8 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:47:02 +0100 1602) 		dequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK);
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1603) 	}
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1604) 	if (running)
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1605) 		put_prev_task(rq, p);
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1606) 
c5b2803840817 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:35 +0200 1607) 	p->sched_class->set_cpus_allowed(p, new_mask);
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1608) 
6c37067e27867 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:36 +0200 1609) 	if (queued)
7134b3e941613 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:23:38 +0100 1610) 		enqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);
a399d233078ed kernel/sched/core.c (Vincent Guittot            2016-09-12 09:47:52 +0200 1611) 	if (running)
03b7fad167efc kernel/sched/core.c (Peter Zijlstra             2019-05-29 20:36:41 +0000 1612) 		set_next_task(rq, p);
c5b2803840817 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:35 +0200 1613) }
c5b2803840817 kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:35 +0200 1614) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1615) /*
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1616)  * Change a given task's CPU affinity. Migrate the thread to a
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1617)  * proper CPU and schedule it away if the CPU it's executing on
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1618)  * is removed from the allowed bitmask.
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1619)  *
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1620)  * NOTE: the caller must have a valid reference to the task, the
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1621)  * task must not exit() & deallocate itself prematurely. The
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1622)  * call is not atomic; no spinlocks may be held.
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1623)  */
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1624) static int __set_cpus_allowed_ptr(struct task_struct *p,
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1625) 				  const struct cpumask *new_mask, bool check)
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1626) {
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1627) 	const struct cpumask *cpu_valid_mask = cpu_active_mask;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1628) 	unsigned int dest_cpu;
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 1629) 	struct rq_flags rf;
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 1630) 	struct rq *rq;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1631) 	int ret = 0;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1632) 
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 1633) 	rq = task_rq_lock(p, &rf);
a499c3ead88cc kernel/sched/core.c (Wanpeng Li                 2017-02-21 23:52:55 -0800 1634) 	update_rq_clock(rq);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1635) 
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1636) 	if (p->flags & PF_KTHREAD) {
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1637) 		/*
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1638) 		 * Kernel threads are allowed on online && !active CPUs
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1639) 		 */
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1640) 		cpu_valid_mask = cpu_online_mask;
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1641) 	}
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1642) 
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1643) 	/*
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1644) 	 * Must re-check here, to close a race against __kthread_bind(),
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1645) 	 * sched_setaffinity() is not guaranteed to observe the flag.
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1646) 	 */
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1647) 	if (check && (p->flags & PF_NO_SETAFFINITY)) {
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1648) 		ret = -EINVAL;
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1649) 		goto out;
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1650) 	}
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1651) 
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 1652) 	if (cpumask_equal(p->cpus_ptr, new_mask))
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1653) 		goto out;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1654) 
714e501e16cd4 kernel/sched/core.c (KeMeng Shi                 2019-09-16 06:53:28 +0000 1655) 	dest_cpu = cpumask_any_and(cpu_valid_mask, new_mask);
714e501e16cd4 kernel/sched/core.c (KeMeng Shi                 2019-09-16 06:53:28 +0000 1656) 	if (dest_cpu >= nr_cpu_ids) {
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1657) 		ret = -EINVAL;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1658) 		goto out;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1659) 	}
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1660) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1661) 	do_set_cpus_allowed(p, new_mask);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1662) 
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1663) 	if (p->flags & PF_KTHREAD) {
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1664) 		/*
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1665) 		 * For kernel threads that do indeed end up on online &&
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 1666) 		 * !active we want to ensure they are strict per-CPU threads.
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1667) 		 */
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1668) 		WARN_ON(cpumask_intersects(new_mask, cpu_online_mask) &&
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1669) 			!cpumask_intersects(new_mask, cpu_active_mask) &&
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1670) 			p->nr_cpus_allowed != 1);
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1671) 	}
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 1672) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1673) 	/* Can the task run on the task's current CPU? If so, we're done */
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1674) 	if (cpumask_test_cpu(task_cpu(p), new_mask))
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1675) 		goto out;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1676) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1677) 	if (task_running(rq, p) || p->state == TASK_WAKING) {
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1678) 		struct migration_arg arg = { p, dest_cpu };
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1679) 		/* Need help from migration thread: drop lock and wait. */
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 1680) 		task_rq_unlock(rq, p, &rf);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1681) 		stop_one_cpu(cpu_of(rq), migration_cpu_stop, &arg);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1682) 		return 0;
cbce1a6867005 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:54 +0200 1683) 	} else if (task_on_rq_queued(p)) {
cbce1a6867005 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:54 +0200 1684) 		/*
cbce1a6867005 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:54 +0200 1685) 		 * OK, since we're going to drop the lock immediately
cbce1a6867005 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:54 +0200 1686) 		 * afterwards anyway.
cbce1a6867005 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:54 +0200 1687) 		 */
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1688) 		rq = move_queued_task(rq, &rf, p, dest_cpu);
cbce1a6867005 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:54 +0200 1689) 	}
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1690) out:
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 1691) 	task_rq_unlock(rq, p, &rf);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1692) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1693) 	return ret;
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1694) }
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1695) 
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1696) int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1697) {
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1698) 	return __set_cpus_allowed_ptr(p, new_mask, false);
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 1699) }
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1700) EXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 1701) 
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1702) void set_task_cpu(struct task_struct *p, unsigned int new_cpu)
c65cc8705256a kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:58 +0200 1703) {
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 1704) #ifdef CONFIG_SCHED_DEBUG
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 1705) 	/*
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 1706) 	 * We should never call set_task_cpu() on a blocked task,
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 1707) 	 * ttwu() will sort out the placement.
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 1708) 	 */
077614ee1e932 kernel/sched.c      (Peter Zijlstra             2009-12-17 13:16:31 +0100 1709) 	WARN_ON_ONCE(p->state != TASK_RUNNING && p->state != TASK_WAKING &&
e2336f6e51edd kernel/sched/core.c (Oleg Nesterov              2014-10-08 20:33:48 +0200 1710) 			!p->on_rq);
0122ec5b02f76 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:51 +0200 1711) 
3ea94de15ce9f kernel/sched/core.c (Joonwoo Park               2015-11-12 19:38:54 -0800 1712) 	/*
3ea94de15ce9f kernel/sched/core.c (Joonwoo Park               2015-11-12 19:38:54 -0800 1713) 	 * Migrating fair class task must have p->on_rq = TASK_ON_RQ_MIGRATING,
3ea94de15ce9f kernel/sched/core.c (Joonwoo Park               2015-11-12 19:38:54 -0800 1714) 	 * because schedstat_wait_{start,end} rebase migrating task's wait_start
3ea94de15ce9f kernel/sched/core.c (Joonwoo Park               2015-11-12 19:38:54 -0800 1715) 	 * time relying on p->on_rq.
3ea94de15ce9f kernel/sched/core.c (Joonwoo Park               2015-11-12 19:38:54 -0800 1716) 	 */
3ea94de15ce9f kernel/sched/core.c (Joonwoo Park               2015-11-12 19:38:54 -0800 1717) 	WARN_ON_ONCE(p->state == TASK_RUNNING &&
3ea94de15ce9f kernel/sched/core.c (Joonwoo Park               2015-11-12 19:38:54 -0800 1718) 		     p->sched_class == &fair_sched_class &&
3ea94de15ce9f kernel/sched/core.c (Joonwoo Park               2015-11-12 19:38:54 -0800 1719) 		     (p->on_rq && !task_on_rq_migrating(p)));
3ea94de15ce9f kernel/sched/core.c (Joonwoo Park               2015-11-12 19:38:54 -0800 1720) 
0122ec5b02f76 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:51 +0200 1721) #ifdef CONFIG_LOCKDEP
6c6c54e1807fa kernel/sched.c      (Peter Zijlstra             2011-06-03 17:37:07 +0200 1722) 	/*
6c6c54e1807fa kernel/sched.c      (Peter Zijlstra             2011-06-03 17:37:07 +0200 1723) 	 * The caller should hold either p->pi_lock or rq->lock, when changing
6c6c54e1807fa kernel/sched.c      (Peter Zijlstra             2011-06-03 17:37:07 +0200 1724) 	 * a task's CPU. ->pi_lock for waking tasks, rq->lock for runnable tasks.
6c6c54e1807fa kernel/sched.c      (Peter Zijlstra             2011-06-03 17:37:07 +0200 1725) 	 *
6c6c54e1807fa kernel/sched.c      (Peter Zijlstra             2011-06-03 17:37:07 +0200 1726) 	 * sched_move_task() holds both and thus holding either pins the cgroup,
8323f26ce3425 kernel/sched/core.c (Peter Zijlstra             2012-06-22 13:36:05 +0200 1727) 	 * see task_group().
6c6c54e1807fa kernel/sched.c      (Peter Zijlstra             2011-06-03 17:37:07 +0200 1728) 	 *
6c6c54e1807fa kernel/sched.c      (Peter Zijlstra             2011-06-03 17:37:07 +0200 1729) 	 * Furthermore, all task_rq users should acquire both locks, see
6c6c54e1807fa kernel/sched.c      (Peter Zijlstra             2011-06-03 17:37:07 +0200 1730) 	 * task_rq_lock().
6c6c54e1807fa kernel/sched.c      (Peter Zijlstra             2011-06-03 17:37:07 +0200 1731) 	 */
0122ec5b02f76 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:51 +0200 1732) 	WARN_ON_ONCE(debug_locks && !(lockdep_is_held(&p->pi_lock) ||
0122ec5b02f76 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:51 +0200 1733) 				      lockdep_is_held(&task_rq(p)->lock)));
0122ec5b02f76 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:51 +0200 1734) #endif
4ff9083b8a9a8 kernel/sched/core.c (Peter Zijlstra             2017-09-07 17:03:52 +0200 1735) 	/*
4ff9083b8a9a8 kernel/sched/core.c (Peter Zijlstra             2017-09-07 17:03:52 +0200 1736) 	 * Clearly, migrating tasks to offline CPUs is a fairly daft thing.
4ff9083b8a9a8 kernel/sched/core.c (Peter Zijlstra             2017-09-07 17:03:52 +0200 1737) 	 */
4ff9083b8a9a8 kernel/sched/core.c (Peter Zijlstra             2017-09-07 17:03:52 +0200 1738) 	WARN_ON_ONCE(!cpu_online(new_cpu));
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 1739) #endif
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 1740) 
de1d728606043 kernel/sched.c      (Mathieu Desnoyers          2009-05-05 16:49:59 +0800 1741) 	trace_sched_migrate_task(p, new_cpu);
cbc34ed1ac366 kernel/sched.c      (Peter Zijlstra             2008-12-10 08:08:22 +0100 1742) 
0c69774e6ce94 kernel/sched.c      (Peter Zijlstra             2009-12-22 15:43:19 +0100 1743) 	if (task_cpu(p) != new_cpu) {
0a74bef8bed18 kernel/sched/core.c (Paul Turner                2012-10-04 13:18:30 +0200 1744) 		if (p->sched_class->migrate_task_rq)
1327237a5978b kernel/sched/core.c (Srikar Dronamraju          2018-09-21 23:18:57 +0530 1745) 			p->sched_class->migrate_task_rq(p, new_cpu);
0c69774e6ce94 kernel/sched.c      (Peter Zijlstra             2009-12-22 15:43:19 +0100 1746) 		p->se.nr_migrations++;
d7822b1e24f2d kernel/sched/core.c (Mathieu Desnoyers          2018-06-02 08:43:54 -0400 1747) 		rseq_migrate(p);
ff303e66c240b kernel/sched/core.c (Peter Zijlstra             2015-04-17 20:05:30 +0200 1748) 		perf_event_task_migrate(p);
0c69774e6ce94 kernel/sched.c      (Peter Zijlstra             2009-12-22 15:43:19 +0100 1749) 	}
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1750) 
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 1751) 	__set_task_cpu(p, new_cpu);
c65cc8705256a kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:58 +0200 1752) }
c65cc8705256a kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:58 +0200 1753) 
0ad4e3dfe6cf3 kernel/sched/core.c (Srikar Dronamraju          2018-06-20 22:32:50 +0530 1754) #ifdef CONFIG_NUMA_BALANCING
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1755) static void __migrate_swap_task(struct task_struct *p, int cpu)
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1756) {
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 1757) 	if (task_on_rq_queued(p)) {
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1758) 		struct rq *src_rq, *dst_rq;
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1759) 		struct rq_flags srf, drf;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1760) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1761) 		src_rq = task_rq(p);
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1762) 		dst_rq = cpu_rq(cpu);
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1763) 
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1764) 		rq_pin_lock(src_rq, &srf);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1765) 		rq_pin_lock(dst_rq, &drf);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1766) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1767) 		deactivate_task(src_rq, p, 0);
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1768) 		set_task_cpu(p, cpu);
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1769) 		activate_task(dst_rq, p, 0);
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1770) 		check_preempt_curr(dst_rq, p, 0);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1771) 
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1772) 		rq_unpin_lock(dst_rq, &drf);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1773) 		rq_unpin_lock(src_rq, &srf);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 1774) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1775) 	} else {
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1776) 		/*
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1777) 		 * Task isn't running anymore; make it appear like we migrated
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1778) 		 * it before it went to sleep. This means on wakeup we make the
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 1779) 		 * previous CPU our target instead of where it really is.
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1780) 		 */
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1781) 		p->wake_cpu = cpu;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1782) 	}
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1783) }
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1784) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1785) struct migration_swap_arg {
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1786) 	struct task_struct *src_task, *dst_task;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1787) 	int src_cpu, dst_cpu;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1788) };
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1789) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1790) static int migrate_swap_stop(void *data)
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1791) {
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1792) 	struct migration_swap_arg *arg = data;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1793) 	struct rq *src_rq, *dst_rq;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1794) 	int ret = -EAGAIN;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1795) 
62694cd513222 kernel/sched/core.c (Peter Zijlstra             2015-10-09 18:36:29 +0200 1796) 	if (!cpu_active(arg->src_cpu) || !cpu_active(arg->dst_cpu))
62694cd513222 kernel/sched/core.c (Peter Zijlstra             2015-10-09 18:36:29 +0200 1797) 		return -EAGAIN;
62694cd513222 kernel/sched/core.c (Peter Zijlstra             2015-10-09 18:36:29 +0200 1798) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1799) 	src_rq = cpu_rq(arg->src_cpu);
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1800) 	dst_rq = cpu_rq(arg->dst_cpu);
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1801) 
746023159c40c kernel/sched/core.c (Peter Zijlstra             2013-10-10 20:17:22 +0200 1802) 	double_raw_lock(&arg->src_task->pi_lock,
746023159c40c kernel/sched/core.c (Peter Zijlstra             2013-10-10 20:17:22 +0200 1803) 			&arg->dst_task->pi_lock);
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1804) 	double_rq_lock(src_rq, dst_rq);
62694cd513222 kernel/sched/core.c (Peter Zijlstra             2015-10-09 18:36:29 +0200 1805) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1806) 	if (task_cpu(arg->dst_task) != arg->dst_cpu)
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1807) 		goto unlock;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1808) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1809) 	if (task_cpu(arg->src_task) != arg->src_cpu)
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1810) 		goto unlock;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1811) 
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 1812) 	if (!cpumask_test_cpu(arg->dst_cpu, arg->src_task->cpus_ptr))
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1813) 		goto unlock;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1814) 
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 1815) 	if (!cpumask_test_cpu(arg->src_cpu, arg->dst_task->cpus_ptr))
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1816) 		goto unlock;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1817) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1818) 	__migrate_swap_task(arg->src_task, arg->dst_cpu);
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1819) 	__migrate_swap_task(arg->dst_task, arg->src_cpu);
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1820) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1821) 	ret = 0;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1822) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1823) unlock:
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1824) 	double_rq_unlock(src_rq, dst_rq);
746023159c40c kernel/sched/core.c (Peter Zijlstra             2013-10-10 20:17:22 +0200 1825) 	raw_spin_unlock(&arg->dst_task->pi_lock);
746023159c40c kernel/sched/core.c (Peter Zijlstra             2013-10-10 20:17:22 +0200 1826) 	raw_spin_unlock(&arg->src_task->pi_lock);
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1827) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1828) 	return ret;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1829) }
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1830) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1831) /*
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1832)  * Cross migrate two tasks
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1833)  */
0ad4e3dfe6cf3 kernel/sched/core.c (Srikar Dronamraju          2018-06-20 22:32:50 +0530 1834) int migrate_swap(struct task_struct *cur, struct task_struct *p,
0ad4e3dfe6cf3 kernel/sched/core.c (Srikar Dronamraju          2018-06-20 22:32:50 +0530 1835) 		int target_cpu, int curr_cpu)
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1836) {
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1837) 	struct migration_swap_arg arg;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1838) 	int ret = -EINVAL;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1839) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1840) 	arg = (struct migration_swap_arg){
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1841) 		.src_task = cur,
0ad4e3dfe6cf3 kernel/sched/core.c (Srikar Dronamraju          2018-06-20 22:32:50 +0530 1842) 		.src_cpu = curr_cpu,
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1843) 		.dst_task = p,
0ad4e3dfe6cf3 kernel/sched/core.c (Srikar Dronamraju          2018-06-20 22:32:50 +0530 1844) 		.dst_cpu = target_cpu,
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1845) 	};
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1846) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1847) 	if (arg.src_cpu == arg.dst_cpu)
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1848) 		goto out;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1849) 
6acce3ef84520 kernel/sched/core.c (Peter Zijlstra             2013-10-11 14:38:20 +0200 1850) 	/*
6acce3ef84520 kernel/sched/core.c (Peter Zijlstra             2013-10-11 14:38:20 +0200 1851) 	 * These three tests are all lockless; this is OK since all of them
6acce3ef84520 kernel/sched/core.c (Peter Zijlstra             2013-10-11 14:38:20 +0200 1852) 	 * will be re-checked with proper locks held further down the line.
6acce3ef84520 kernel/sched/core.c (Peter Zijlstra             2013-10-11 14:38:20 +0200 1853) 	 */
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1854) 	if (!cpu_active(arg.src_cpu) || !cpu_active(arg.dst_cpu))
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1855) 		goto out;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1856) 
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 1857) 	if (!cpumask_test_cpu(arg.dst_cpu, arg.src_task->cpus_ptr))
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1858) 		goto out;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1859) 
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 1860) 	if (!cpumask_test_cpu(arg.src_cpu, arg.dst_task->cpus_ptr))
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1861) 		goto out;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1862) 
286549dcaf4f1 kernel/sched/core.c (Mel Gorman                 2014-01-21 15:51:03 -0800 1863) 	trace_sched_swap_numa(cur, arg.src_cpu, p, arg.dst_cpu);
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1864) 	ret = stop_two_cpus(arg.dst_cpu, arg.src_cpu, migrate_swap_stop, &arg);
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1865) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1866) out:
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1867) 	return ret;
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1868) }
0ad4e3dfe6cf3 kernel/sched/core.c (Srikar Dronamraju          2018-06-20 22:32:50 +0530 1869) #endif /* CONFIG_NUMA_BALANCING */
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 1870) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1871) /*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1872)  * wait_task_inactive - wait for a thread to unschedule.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1873)  *
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1874)  * If @match_state is nonzero, it's the @p->state value just checked and
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1875)  * not expected to change.  If it changes, i.e. @p might have woken up,
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1876)  * then return zero.  When we succeed in waiting for @p to be off its CPU,
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1877)  * we return a positive number (its total switch count).  If a second call
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1878)  * a short while later returns the same number, the caller can be sure that
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1879)  * @p has remained unscheduled the whole time.
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1880)  *
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1881)  * The caller must ensure that the task *will* unschedule sometime soon,
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1882)  * else this function might spin for a *long* time. This function can't
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1883)  * be called with interrupts off, or it may introduce deadlock with
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1884)  * smp_call_function() if an IPI is sent by the same process we are
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1885)  * waiting to become inactive.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1886)  */
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1887) unsigned long wait_task_inactive(struct task_struct *p, long match_state)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1888) {
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 1889) 	int running, queued;
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 1890) 	struct rq_flags rf;
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1891) 	unsigned long ncsw;
70b97a7f0b19c kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:42 -0700 1892) 	struct rq *rq;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1893) 
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1894) 	for (;;) {
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1895) 		/*
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1896) 		 * We do the initial early heuristics without holding
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1897) 		 * any task-queue locks at all. We'll only try to get
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1898) 		 * the runqueue lock when things look like they will
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1899) 		 * work out!
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1900) 		 */
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1901) 		rq = task_rq(p);
fa490cfd15d7c kernel/sched.c      (Linus Torvalds             2007-06-18 09:34:40 -0700 1902) 
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1903) 		/*
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1904) 		 * If the task is actively running on another CPU
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1905) 		 * still, just relax and busy-wait without holding
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1906) 		 * any locks.
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1907) 		 *
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1908) 		 * NOTE! Since we don't hold any locks, it's not
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1909) 		 * even sure that "rq" stays as the right runqueue!
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1910) 		 * But we don't care, since "task_running()" will
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1911) 		 * return false if the runqueue has changed and p
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1912) 		 * is actually now running somewhere else!
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1913) 		 */
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1914) 		while (task_running(rq, p)) {
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1915) 			if (match_state && unlikely(p->state != match_state))
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1916) 				return 0;
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1917) 			cpu_relax();
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1918) 		}
fa490cfd15d7c kernel/sched.c      (Linus Torvalds             2007-06-18 09:34:40 -0700 1919) 
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1920) 		/*
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1921) 		 * Ok, time to look more closely! We need the rq
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1922) 		 * lock now, to be *sure*. If we're wrong, we'll
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1923) 		 * just go back and repeat.
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1924) 		 */
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 1925) 		rq = task_rq_lock(p, &rf);
27a9da6538ee1 kernel/sched.c      (Peter Zijlstra             2010-05-04 20:36:56 +0200 1926) 		trace_sched_wait_task(p);
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1927) 		running = task_running(rq, p);
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 1928) 		queued = task_on_rq_queued(p);
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1929) 		ncsw = 0;
f31e11d87a5d7 kernel/sched.c      (Oleg Nesterov              2008-08-20 16:54:44 -0700 1930) 		if (!match_state || p->state == match_state)
93dcf55f828b0 kernel/sched.c      (Oleg Nesterov              2008-08-20 16:54:44 -0700 1931) 			ncsw = p->nvcsw | LONG_MIN; /* sets MSB */
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 1932) 		task_rq_unlock(rq, p, &rf);
fa490cfd15d7c kernel/sched.c      (Linus Torvalds             2007-06-18 09:34:40 -0700 1933) 
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1934) 		/*
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1935) 		 * If it changed from the expected state, bail out now.
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1936) 		 */
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1937) 		if (unlikely(!ncsw))
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1938) 			break;
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1939) 
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1940) 		/*
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1941) 		 * Was it really running after all now that we
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1942) 		 * checked with the proper locks actually held?
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1943) 		 *
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1944) 		 * Oops. Go back and try again..
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1945) 		 */
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1946) 		if (unlikely(running)) {
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1947) 			cpu_relax();
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1948) 			continue;
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1949) 		}
fa490cfd15d7c kernel/sched.c      (Linus Torvalds             2007-06-18 09:34:40 -0700 1950) 
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1951) 		/*
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1952) 		 * It's not enough that it's not actively running,
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1953) 		 * it must be off the runqueue _entirely_, and not
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1954) 		 * preempted!
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1955) 		 *
80dd99b368cf6 kernel/sched.c      (Luis Henriques             2009-03-16 19:58:09 +0000 1956) 		 * So if it was still runnable (but just not actively
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1957) 		 * running right now), it's preempted, and we should
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1958) 		 * yield - it could be a while.
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1959) 		 */
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 1960) 		if (unlikely(queued)) {
8b0e195314fab kernel/sched/core.c (Thomas Gleixner            2016-12-25 12:30:41 +0100 1961) 			ktime_t to = NSEC_PER_SEC / HZ;
8eb90c30e0e81 kernel/sched.c      (Thomas Gleixner            2011-02-23 23:52:21 +0000 1962) 
8eb90c30e0e81 kernel/sched.c      (Thomas Gleixner            2011-02-23 23:52:21 +0000 1963) 			set_current_state(TASK_UNINTERRUPTIBLE);
8eb90c30e0e81 kernel/sched.c      (Thomas Gleixner            2011-02-23 23:52:21 +0000 1964) 			schedule_hrtimeout(&to, HRTIMER_MODE_REL);
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1965) 			continue;
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1966) 		}
fa490cfd15d7c kernel/sched.c      (Linus Torvalds             2007-06-18 09:34:40 -0700 1967) 
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1968) 		/*
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1969) 		 * Ahh, all good. It wasn't running, and it wasn't
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1970) 		 * runnable, which means that it will never become
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1971) 		 * running in the future either. We're all done!
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1972) 		 */
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1973) 		break;
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 1974) 	}
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1975) 
85ba2d862e521 kernel/sched.c      (Roland McGrath             2008-07-25 19:45:58 -0700 1976) 	return ncsw;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1977) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1978) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1979) /***
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1980)  * kick_process - kick a running thread to enter/exit the kernel
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1981)  * @p: the to-be-kicked thread
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1982)  *
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1983)  * Cause a process which is running on another CPU to enter
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1984)  * kernel-mode, without any delay. (to get signals handled.)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1985)  *
25985edcedea6 kernel/sched.c      (Lucas De Marchi            2011-03-30 22:57:33 -0300 1986)  * NOTE: this function doesn't have to take the runqueue lock,
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1987)  * because all it wants to ensure is that the remote task enters
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1988)  * the kernel. If the IPI races and the task has been migrated
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1989)  * to another CPU then no harm is done and the purpose has been
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1990)  * achieved as well.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1991)  */
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 1992) void kick_process(struct task_struct *p)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1993) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1994) 	int cpu;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1995) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1996) 	preempt_disable();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1997) 	cpu = task_cpu(p);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1998) 	if ((cpu != smp_processor_id()) && task_curr(p))
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 1999) 		smp_send_reschedule(cpu);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2000) 	preempt_enable();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2001) }
b43e352139f51 kernel/sched.c      (Rusty Russell              2009-06-12 22:27:00 -0600 2002) EXPORT_SYMBOL_GPL(kick_process);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2003) 
30da688ef6b76 kernel/sched.c      (Oleg Nesterov              2010-03-15 10:10:19 +0100 2004) /*
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 2005)  * ->cpus_ptr is protected by both rq->lock and p->pi_lock
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2006)  *
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2007)  * A few notes on cpu_active vs cpu_online:
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2008)  *
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2009)  *  - cpu_active must be a subset of cpu_online
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2010)  *
97fb7a0a8944b kernel/sched/core.c (Ingo Molnar                2018-03-03 14:01:12 +0100 2011)  *  - on CPU-up we allow per-CPU kthreads on the online && !active CPU,
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2012)  *    see __set_cpus_allowed_ptr(). At this point the newly online
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2013)  *    CPU isn't yet part of the sched domains, and balancing will not
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2014)  *    see it.
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2015)  *
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2016)  *  - on CPU-down we clear cpu_active() to mask the sched domains and
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2017)  *    avoid the load balancer to place new tasks on the to be removed
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2018)  *    CPU. Existing tasks will remain running there and will be taken
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2019)  *    off.
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2020)  *
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2021)  * This means that fallback selection must not select !active CPUs.
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2022)  * And can assume that any active CPU must be online. Conversely
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2023)  * select_task_rq() below may allow selection of !active CPUs in order
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2024)  * to satisfy the above rules.
30da688ef6b76 kernel/sched.c      (Oleg Nesterov              2010-03-15 10:10:19 +0100 2025)  */
5da9a0fb673a0 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:38 +0100 2026) static int select_fallback_rq(int cpu, struct task_struct *p)
5da9a0fb673a0 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:38 +0100 2027) {
aa00d89c2780d kernel/sched/core.c (Tang Chen                  2013-02-22 16:33:33 -0800 2028) 	int nid = cpu_to_node(cpu);
aa00d89c2780d kernel/sched/core.c (Tang Chen                  2013-02-22 16:33:33 -0800 2029) 	const struct cpumask *nodemask = NULL;
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2030) 	enum { cpuset, possible, fail } state = cpuset;
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2031) 	int dest_cpu;
5da9a0fb673a0 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:38 +0100 2032) 
aa00d89c2780d kernel/sched/core.c (Tang Chen                  2013-02-22 16:33:33 -0800 2033) 	/*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2034) 	 * If the node that the CPU is on has been offlined, cpu_to_node()
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2035) 	 * will return -1. There is no CPU on the node, and we should
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2036) 	 * select the CPU on the other node.
aa00d89c2780d kernel/sched/core.c (Tang Chen                  2013-02-22 16:33:33 -0800 2037) 	 */
aa00d89c2780d kernel/sched/core.c (Tang Chen                  2013-02-22 16:33:33 -0800 2038) 	if (nid != -1) {
aa00d89c2780d kernel/sched/core.c (Tang Chen                  2013-02-22 16:33:33 -0800 2039) 		nodemask = cpumask_of_node(nid);
aa00d89c2780d kernel/sched/core.c (Tang Chen                  2013-02-22 16:33:33 -0800 2040) 
aa00d89c2780d kernel/sched/core.c (Tang Chen                  2013-02-22 16:33:33 -0800 2041) 		/* Look for allowed, online CPU in same node. */
aa00d89c2780d kernel/sched/core.c (Tang Chen                  2013-02-22 16:33:33 -0800 2042) 		for_each_cpu(dest_cpu, nodemask) {
aa00d89c2780d kernel/sched/core.c (Tang Chen                  2013-02-22 16:33:33 -0800 2043) 			if (!cpu_active(dest_cpu))
aa00d89c2780d kernel/sched/core.c (Tang Chen                  2013-02-22 16:33:33 -0800 2044) 				continue;
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 2045) 			if (cpumask_test_cpu(dest_cpu, p->cpus_ptr))
aa00d89c2780d kernel/sched/core.c (Tang Chen                  2013-02-22 16:33:33 -0800 2046) 				return dest_cpu;
aa00d89c2780d kernel/sched/core.c (Tang Chen                  2013-02-22 16:33:33 -0800 2047) 		}
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2048) 	}
5da9a0fb673a0 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:38 +0100 2049) 
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2050) 	for (;;) {
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2051) 		/* Any allowed, online CPU? */
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 2052) 		for_each_cpu(dest_cpu, p->cpus_ptr) {
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 2053) 			if (!is_cpu_allowed(p, dest_cpu))
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2054) 				continue;
175f0e25abeaa kernel/sched/core.c (Peter Zijlstra             2017-07-25 18:58:21 +0200 2055) 
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2056) 			goto out;
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2057) 		}
5da9a0fb673a0 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:38 +0100 2058) 
e73e85f059383 kernel/sched/core.c (Oleg Nesterov              2015-10-10 20:53:15 +0200 2059) 		/* No more Mr. Nice Guy. */
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2060) 		switch (state) {
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2061) 		case cpuset:
e73e85f059383 kernel/sched/core.c (Oleg Nesterov              2015-10-10 20:53:15 +0200 2062) 			if (IS_ENABLED(CONFIG_CPUSETS)) {
e73e85f059383 kernel/sched/core.c (Oleg Nesterov              2015-10-10 20:53:15 +0200 2063) 				cpuset_cpus_allowed_fallback(p);
e73e85f059383 kernel/sched/core.c (Oleg Nesterov              2015-10-10 20:53:15 +0200 2064) 				state = possible;
e73e85f059383 kernel/sched/core.c (Oleg Nesterov              2015-10-10 20:53:15 +0200 2065) 				break;
e73e85f059383 kernel/sched/core.c (Oleg Nesterov              2015-10-10 20:53:15 +0200 2066) 			}
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2067) 			/* Fall-through */
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2068) 		case possible:
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2069) 			do_set_cpus_allowed(p, cpu_possible_mask);
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2070) 			state = fail;
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2071) 			break;
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2072) 
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2073) 		case fail:
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2074) 			BUG();
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2075) 			break;
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2076) 		}
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2077) 	}
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2078) 
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2079) out:
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2080) 	if (state != cpuset) {
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2081) 		/*
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2082) 		 * Don't tell them about moving exiting tasks or
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2083) 		 * kernel threads (both mm NULL), since they never
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2084) 		 * leave kernel.
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2085) 		 */
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2086) 		if (p->mm && printk_ratelimit()) {
aac74dc495456 kernel/sched/core.c (John Stultz                2014-06-04 16:11:40 -0700 2087) 			printk_deferred("process %d (%s) no longer affine to cpu%d\n",
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2088) 					task_pid_nr(p), p->comm, cpu);
2baab4e90495e kernel/sched/core.c (Peter Zijlstra             2012-03-20 15:57:01 +0100 2089) 		}
5da9a0fb673a0 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:38 +0100 2090) 	}
5da9a0fb673a0 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:38 +0100 2091) 
5da9a0fb673a0 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:38 +0100 2092) 	return dest_cpu;
5da9a0fb673a0 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:38 +0100 2093) }
5da9a0fb673a0 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:38 +0100 2094) 
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 2095) /*
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 2096)  * The caller (fork, wakeup) owns p->pi_lock, ->cpus_ptr is stable.
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 2097)  */
970b13bacba14 kernel/sched.c      (Peter Zijlstra             2009-11-25 13:31:39 +0100 2098) static inline
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 2099) int select_task_rq(struct task_struct *p, int cpu, int sd_flags, int wake_flags)
970b13bacba14 kernel/sched.c      (Peter Zijlstra             2009-11-25 13:31:39 +0100 2100) {
cbce1a6867005 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:54 +0200 2101) 	lockdep_assert_held(&p->pi_lock);
cbce1a6867005 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:54 +0200 2102) 
4b53a3412d666 kernel/sched/core.c (Ingo Molnar                2017-02-05 15:41:03 +0100 2103) 	if (p->nr_cpus_allowed > 1)
6c1d9410f007a kernel/sched/core.c (Wanpeng Li                 2014-11-05 09:14:37 +0800 2104) 		cpu = p->sched_class->select_task_rq(p, cpu, sd_flags, wake_flags);
e9d867a67fd03 kernel/sched/core.c (Peter Zijlstra (Intel)     2016-03-10 12:54:08 +0100 2105) 	else
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 2106) 		cpu = cpumask_any(p->cpus_ptr);
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 2107) 
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 2108) 	/*
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 2109) 	 * In order not to call set_task_cpu() on a blocking task we need
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 2110) 	 * to rely on ttwu() to place the task on a valid ->cpus_ptr
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2111) 	 * CPU.
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 2112) 	 *
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 2113) 	 * Since this is common to all placement strategies, this lives here.
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 2114) 	 *
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 2115) 	 * [ this allows ->select_task() to simply return task_cpu(p) and
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 2116) 	 *   not worry about this generic constraint ]
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 2117) 	 */
7af443ee16976 kernel/sched/core.c (Paul Burton                2018-05-26 08:46:47 -0700 2118) 	if (unlikely(!is_cpu_allowed(p, cpu)))
5da9a0fb673a0 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:38 +0100 2119) 		cpu = select_fallback_rq(task_cpu(p), p);
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 2120) 
e2912009fb7b7 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:36 +0100 2121) 	return cpu;
970b13bacba14 kernel/sched.c      (Peter Zijlstra             2009-11-25 13:31:39 +0100 2122) }
09a40af5240de kernel/sched.c      (Mike Galbraith             2010-04-15 07:29:59 +0200 2123) 
09a40af5240de kernel/sched.c      (Mike Galbraith             2010-04-15 07:29:59 +0200 2124) static void update_avg(u64 *avg, u64 sample)
09a40af5240de kernel/sched.c      (Mike Galbraith             2010-04-15 07:29:59 +0200 2125) {
09a40af5240de kernel/sched.c      (Mike Galbraith             2010-04-15 07:29:59 +0200 2126) 	s64 diff = sample - *avg;
09a40af5240de kernel/sched.c      (Mike Galbraith             2010-04-15 07:29:59 +0200 2127) 	*avg += diff >> 3;
09a40af5240de kernel/sched.c      (Mike Galbraith             2010-04-15 07:29:59 +0200 2128) }
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 2129) 
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2130) void sched_set_stop_task(int cpu, struct task_struct *stop)
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2131) {
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2132) 	struct sched_param param = { .sched_priority = MAX_RT_PRIO - 1 };
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2133) 	struct task_struct *old_stop = cpu_rq(cpu)->stop;
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2134) 
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2135) 	if (stop) {
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2136) 		/*
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2137) 		 * Make it appear like a SCHED_FIFO task, its something
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2138) 		 * userspace knows about and won't get confused about.
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2139) 		 *
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2140) 		 * Also, it will make PI more or less work without too
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2141) 		 * much confusion -- but then, stop work should not
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2142) 		 * rely on PI working anyway.
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2143) 		 */
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2144) 		sched_setscheduler_nocheck(stop, SCHED_FIFO, &param);
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2145) 
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2146) 		stop->sched_class = &stop_sched_class;
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2147) 	}
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2148) 
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2149) 	cpu_rq(cpu)->stop = stop;
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2150) 
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2151) 	if (old_stop) {
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2152) 		/*
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2153) 		 * Reset it back to a normal scheduling class so that
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2154) 		 * it can die in pieces.
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2155) 		 */
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2156) 		old_stop->sched_class = &rt_sched_class;
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2157) 	}
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2158) }
f5832c1998af2 kernel/sched/core.c (Nicolas Pitre              2017-05-29 17:02:57 -0400 2159) 
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 2160) #else
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 2161) 
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 2162) static inline int __set_cpus_allowed_ptr(struct task_struct *p,
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 2163) 					 const struct cpumask *new_mask, bool check)
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 2164) {
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 2165) 	return set_cpus_allowed_ptr(p, new_mask);
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 2166) }
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 2167) 
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 2168) #endif /* CONFIG_SMP */
970b13bacba14 kernel/sched.c      (Peter Zijlstra             2009-11-25 13:31:39 +0100 2169) 
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2170) static void
b84cb5df1f9ad kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:55 +0200 2171) ttwu_stat(struct task_struct *p, int cpu, int wake_flags)
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2172) {
4fa8d299b43a9 kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:26 -0500 2173) 	struct rq *rq;
b84cb5df1f9ad kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:55 +0200 2174) 
4fa8d299b43a9 kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:26 -0500 2175) 	if (!schedstat_enabled())
4fa8d299b43a9 kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:26 -0500 2176) 		return;
4fa8d299b43a9 kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:26 -0500 2177) 
4fa8d299b43a9 kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:26 -0500 2178) 	rq = this_rq();
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2179) 
4fa8d299b43a9 kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:26 -0500 2180) #ifdef CONFIG_SMP
4fa8d299b43a9 kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:26 -0500 2181) 	if (cpu == rq->cpu) {
b85c8b71bf8de kernel/sched/core.c (Peter Zijlstra             2018-01-16 20:51:06 +0100 2182) 		__schedstat_inc(rq->ttwu_local);
b85c8b71bf8de kernel/sched/core.c (Peter Zijlstra             2018-01-16 20:51:06 +0100 2183) 		__schedstat_inc(p->se.statistics.nr_wakeups_local);
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2184) 	} else {
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2185) 		struct sched_domain *sd;
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2186) 
b85c8b71bf8de kernel/sched/core.c (Peter Zijlstra             2018-01-16 20:51:06 +0100 2187) 		__schedstat_inc(p->se.statistics.nr_wakeups_remote);
057f3fadb347e kernel/sched.c      (Peter Zijlstra             2011-04-18 11:24:34 +0200 2188) 		rcu_read_lock();
4fa8d299b43a9 kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:26 -0500 2189) 		for_each_domain(rq->cpu, sd) {
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2190) 			if (cpumask_test_cpu(cpu, sched_domain_span(sd))) {
b85c8b71bf8de kernel/sched/core.c (Peter Zijlstra             2018-01-16 20:51:06 +0100 2191) 				__schedstat_inc(sd->ttwu_wake_remote);
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2192) 				break;
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2193) 			}
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2194) 		}
057f3fadb347e kernel/sched.c      (Peter Zijlstra             2011-04-18 11:24:34 +0200 2195) 		rcu_read_unlock();
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2196) 	}
f339b9dc1f035 kernel/sched.c      (Peter Zijlstra             2011-05-31 10:49:20 +0200 2197) 
f339b9dc1f035 kernel/sched.c      (Peter Zijlstra             2011-05-31 10:49:20 +0200 2198) 	if (wake_flags & WF_MIGRATED)
b85c8b71bf8de kernel/sched/core.c (Peter Zijlstra             2018-01-16 20:51:06 +0100 2199) 		__schedstat_inc(p->se.statistics.nr_wakeups_migrate);
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2200) #endif /* CONFIG_SMP */
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2201) 
b85c8b71bf8de kernel/sched/core.c (Peter Zijlstra             2018-01-16 20:51:06 +0100 2202) 	__schedstat_inc(rq->ttwu_count);
b85c8b71bf8de kernel/sched/core.c (Peter Zijlstra             2018-01-16 20:51:06 +0100 2203) 	__schedstat_inc(p->se.statistics.nr_wakeups);
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2204) 
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2205) 	if (wake_flags & WF_SYNC)
b85c8b71bf8de kernel/sched/core.c (Peter Zijlstra             2018-01-16 20:51:06 +0100 2206) 		__schedstat_inc(p->se.statistics.nr_wakeups_sync);
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2207) }
d7c01d27ab767 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:43 +0200 2208) 
23f41eeb42ce7 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:56 +0200 2209) /*
23f41eeb42ce7 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:56 +0200 2210)  * Mark the task runnable and perform wakeup-preemption.
23f41eeb42ce7 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:56 +0200 2211)  */
e7904a28f5331 kernel/sched/core.c (Peter Zijlstra             2015-08-01 19:25:08 +0200 2212) static void ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags,
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 2213) 			   struct rq_flags *rf)
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2214) {
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2215) 	check_preempt_curr(rq, p, wake_flags);
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2216) 	p->state = TASK_RUNNING;
fbd705a0c6184 kernel/sched/core.c (Peter Zijlstra             2015-06-09 11:13:36 +0200 2217) 	trace_sched_wakeup(p);
fbd705a0c6184 kernel/sched/core.c (Peter Zijlstra             2015-06-09 11:13:36 +0200 2218) 
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2219) #ifdef CONFIG_SMP
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 2220) 	if (p->sched_class->task_woken) {
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 2221) 		/*
cbce1a6867005 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:54 +0200 2222) 		 * Our task @p is fully woken up and running; so its safe to
cbce1a6867005 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:54 +0200 2223) 		 * drop the rq->lock, hereafter rq is only used for statistics.
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 2224) 		 */
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 2225) 		rq_unpin_lock(rq, rf);
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2226) 		p->sched_class->task_woken(rq, p);
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 2227) 		rq_repin_lock(rq, rf);
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 2228) 	}
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2229) 
e69c634190dc7 kernel/sched.c      (Steven Rostedt             2010-12-06 17:10:31 -0500 2230) 	if (rq->idle_stamp) {
78becc2709758 kernel/sched/core.c (Frederic Weisbecker        2013-04-12 01:51:02 +0200 2231) 		u64 delta = rq_clock(rq) - rq->idle_stamp;
9bd721c55c8a8 kernel/sched/core.c (Jason Low                  2013-09-13 11:26:52 -0700 2232) 		u64 max = 2*rq->max_idle_balance_cost;
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2233) 
abfafa54db9ab kernel/sched/core.c (Jason Low                  2013-09-13 11:26:51 -0700 2234) 		update_avg(&rq->avg_idle, delta);
abfafa54db9ab kernel/sched/core.c (Jason Low                  2013-09-13 11:26:51 -0700 2235) 
abfafa54db9ab kernel/sched/core.c (Jason Low                  2013-09-13 11:26:51 -0700 2236) 		if (rq->avg_idle > max)
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2237) 			rq->avg_idle = max;
abfafa54db9ab kernel/sched/core.c (Jason Low                  2013-09-13 11:26:51 -0700 2238) 
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2239) 		rq->idle_stamp = 0;
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2240) 	}
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2241) #endif
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2242) }
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2243) 
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2244) static void
e7904a28f5331 kernel/sched/core.c (Peter Zijlstra             2015-08-01 19:25:08 +0200 2245) ttwu_do_activate(struct rq *rq, struct task_struct *p, int wake_flags,
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 2246) 		 struct rq_flags *rf)
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2247) {
77558e4d01ac0 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:36:23 +0100 2248) 	int en_flags = ENQUEUE_WAKEUP | ENQUEUE_NOCLOCK;
b5179ac70de85 kernel/sched/core.c (Peter Zijlstra             2016-05-11 16:10:34 +0200 2249) 
cbce1a6867005 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:54 +0200 2250) 	lockdep_assert_held(&rq->lock);
cbce1a6867005 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:54 +0200 2251) 
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2252) #ifdef CONFIG_SMP
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2253) 	if (p->sched_contributes_to_load)
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2254) 		rq->nr_uninterruptible--;
b5179ac70de85 kernel/sched/core.c (Peter Zijlstra             2016-05-11 16:10:34 +0200 2255) 
b5179ac70de85 kernel/sched/core.c (Peter Zijlstra             2016-05-11 16:10:34 +0200 2256) 	if (wake_flags & WF_MIGRATED)
59efa0bac9cf8 kernel/sched/core.c (Peter Zijlstra             2016-05-10 18:24:37 +0200 2257) 		en_flags |= ENQUEUE_MIGRATED;
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2258) #endif
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2259) 
1b174a2cb67a3 kernel/sched/core.c (Peter Zijlstra             2019-04-09 09:53:13 +0200 2260) 	activate_task(rq, p, en_flags);
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 2261) 	ttwu_do_wakeup(rq, p, wake_flags, rf);
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2262) }
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2263) 
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2264) /*
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2265)  * Called in case the task @p isn't fully descheduled from its runqueue,
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2266)  * in this case we must do a remote wakeup. Its a 'light' wakeup though,
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2267)  * since all we need to do is flip p->state to TASK_RUNNING, since
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2268)  * the task is still ->on_rq.
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2269)  */
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2270) static int ttwu_remote(struct task_struct *p, int wake_flags)
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2271) {
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 2272) 	struct rq_flags rf;
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2273) 	struct rq *rq;
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2274) 	int ret = 0;
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2275) 
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 2276) 	rq = __task_rq_lock(p, &rf);
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 2277) 	if (task_on_rq_queued(p)) {
1ad4ec0dc740c kernel/sched/core.c (Frederic Weisbecker        2013-04-12 01:51:00 +0200 2278) 		/* check_preempt_curr() may use rq clock */
1ad4ec0dc740c kernel/sched/core.c (Frederic Weisbecker        2013-04-12 01:51:00 +0200 2279) 		update_rq_clock(rq);
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 2280) 		ttwu_do_wakeup(rq, p, wake_flags, &rf);
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2281) 		ret = 1;
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2282) 	}
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 2283) 	__task_rq_unlock(rq, &rf);
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2284) 
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2285) 	return ret;
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2286) }
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2287) 
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2288) #ifdef CONFIG_SMP
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700 2289) void sched_ttwu_pending(void)
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2290) {
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2291) 	struct rq *rq = this_rq();
fa14ff4accfb2 kernel/sched.c      (Peter Zijlstra             2011-09-12 13:06:17 +0200 2292) 	struct llist_node *llist = llist_del_all(&rq->wake_list);
73215849dfbf6 kernel/sched/core.c (Byungchul Park             2017-05-12 09:39:44 +0900 2293) 	struct task_struct *p, *t;
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 2294) 	struct rq_flags rf;
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2295) 
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700 2296) 	if (!llist)
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700 2297) 		return;
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700 2298) 
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 2299) 	rq_lock_irqsave(rq, &rf);
77558e4d01ac0 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:36:23 +0100 2300) 	update_rq_clock(rq);
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2301) 
73215849dfbf6 kernel/sched/core.c (Byungchul Park             2017-05-12 09:39:44 +0900 2302) 	llist_for_each_entry_safe(p, t, llist, wake_entry)
73215849dfbf6 kernel/sched/core.c (Byungchul Park             2017-05-12 09:39:44 +0900 2303) 		ttwu_do_activate(rq, p, p->sched_remote_wakeup ? WF_MIGRATED : 0, &rf);
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2304) 
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 2305) 	rq_unlock_irqrestore(rq, &rf);
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2306) }
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2307) 
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2308) void scheduler_ipi(void)
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2309) {
f27dde8deef33 kernel/sched/core.c (Peter Zijlstra             2013-08-14 14:55:31 +0200 2310) 	/*
f27dde8deef33 kernel/sched/core.c (Peter Zijlstra             2013-08-14 14:55:31 +0200 2311) 	 * Fold TIF_NEED_RESCHED into the preempt_count; anybody setting
f27dde8deef33 kernel/sched/core.c (Peter Zijlstra             2013-08-14 14:55:31 +0200 2312) 	 * TIF_NEED_RESCHED remotely (for the first time) will also send
f27dde8deef33 kernel/sched/core.c (Peter Zijlstra             2013-08-14 14:55:31 +0200 2313) 	 * this IPI.
f27dde8deef33 kernel/sched/core.c (Peter Zijlstra             2013-08-14 14:55:31 +0200 2314) 	 */
8cb75e0c4ec97 kernel/sched/core.c (Peter Zijlstra             2013-11-20 12:22:37 +0100 2315) 	preempt_fold_need_resched();
f27dde8deef33 kernel/sched/core.c (Peter Zijlstra             2013-08-14 14:55:31 +0200 2316) 
fd2ac4f4a65a7 kernel/sched/core.c (Frederic Weisbecker        2014-03-18 21:12:53 +0100 2317) 	if (llist_empty(&this_rq()->wake_list) && !got_nohz_idle_kick())
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2318) 		return;
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2319) 
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2320) 	/*
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2321) 	 * Not all reschedule IPI handlers call irq_enter/irq_exit, since
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2322) 	 * traditionally all their work was done from the interrupt return
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2323) 	 * path. Now that we actually do some work, we need to make sure
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2324) 	 * we do call them.
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2325) 	 *
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2326) 	 * Some archs already do call them, luckily irq_enter/exit nest
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2327) 	 * properly.
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2328) 	 *
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2329) 	 * Arguably we should visit all archs and update all handlers,
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2330) 	 * however a fair share of IPIs are still resched only so this would
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2331) 	 * somewhat pessimize the simple resched case.
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2332) 	 */
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2333) 	irq_enter();
fa14ff4accfb2 kernel/sched.c      (Peter Zijlstra             2011-09-12 13:06:17 +0200 2334) 	sched_ttwu_pending();
ca38062e57e97 kernel/sched.c      (Suresh Siddha              2011-10-03 15:09:00 -0700 2335) 
ca38062e57e97 kernel/sched.c      (Suresh Siddha              2011-10-03 15:09:00 -0700 2336) 	/*
ca38062e57e97 kernel/sched.c      (Suresh Siddha              2011-10-03 15:09:00 -0700 2337) 	 * Check if someone kicked us for doing the nohz idle load balance.
ca38062e57e97 kernel/sched.c      (Suresh Siddha              2011-10-03 15:09:00 -0700 2338) 	 */
873b4c65b519f kernel/sched/core.c (Vincent Guittot            2013-06-05 10:13:11 +0200 2339) 	if (unlikely(got_nohz_idle_kick())) {
6eb57e0d65ebd kernel/sched.c      (Suresh Siddha              2011-10-03 15:09:01 -0700 2340) 		this_rq()->idle_balance = 1;
ca38062e57e97 kernel/sched.c      (Suresh Siddha              2011-10-03 15:09:00 -0700 2341) 		raise_softirq_irqoff(SCHED_SOFTIRQ);
6eb57e0d65ebd kernel/sched.c      (Suresh Siddha              2011-10-03 15:09:01 -0700 2342) 	}
c5d753a55ac92 kernel/sched.c      (Peter Zijlstra             2011-07-19 15:07:25 -0700 2343) 	irq_exit();
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2344) }
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2345) 
b7e7ade34e618 kernel/sched/core.c (Peter Zijlstra             2016-05-23 11:19:07 +0200 2346) static void ttwu_queue_remote(struct task_struct *p, int cpu, int wake_flags)
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2347) {
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700 2348) 	struct rq *rq = cpu_rq(cpu);
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700 2349) 
b7e7ade34e618 kernel/sched/core.c (Peter Zijlstra             2016-05-23 11:19:07 +0200 2350) 	p->sched_remote_wakeup = !!(wake_flags & WF_MIGRATED);
b7e7ade34e618 kernel/sched/core.c (Peter Zijlstra             2016-05-23 11:19:07 +0200 2351) 
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700 2352) 	if (llist_add(&p->wake_entry, &cpu_rq(cpu)->wake_list)) {
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700 2353) 		if (!set_nr_if_polling(rq->idle))
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700 2354) 			smp_send_reschedule(cpu);
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700 2355) 		else
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700 2356) 			trace_sched_wake_idle_without_ipi(cpu);
e3baac47f0e82 kernel/sched/core.c (Peter Zijlstra             2014-06-04 10:31:18 -0700 2357) 	}
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2358) }
d6aa8f85f1637 kernel/sched.c      (Peter Zijlstra             2011-05-26 14:21:33 +0200 2359) 
f6be8af1c95de kernel/sched/core.c (Chuansheng Liu             2014-09-04 15:17:53 +0800 2360) void wake_up_if_idle(int cpu)
f6be8af1c95de kernel/sched/core.c (Chuansheng Liu             2014-09-04 15:17:53 +0800 2361) {
f6be8af1c95de kernel/sched/core.c (Chuansheng Liu             2014-09-04 15:17:53 +0800 2362) 	struct rq *rq = cpu_rq(cpu);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 2363) 	struct rq_flags rf;
f6be8af1c95de kernel/sched/core.c (Chuansheng Liu             2014-09-04 15:17:53 +0800 2364) 
fd7de1e8d5b2b kernel/sched/core.c (Andy Lutomirski            2014-11-29 08:13:51 -0800 2365) 	rcu_read_lock();
fd7de1e8d5b2b kernel/sched/core.c (Andy Lutomirski            2014-11-29 08:13:51 -0800 2366) 
fd7de1e8d5b2b kernel/sched/core.c (Andy Lutomirski            2014-11-29 08:13:51 -0800 2367) 	if (!is_idle_task(rcu_dereference(rq->curr)))
fd7de1e8d5b2b kernel/sched/core.c (Andy Lutomirski            2014-11-29 08:13:51 -0800 2368) 		goto out;
f6be8af1c95de kernel/sched/core.c (Chuansheng Liu             2014-09-04 15:17:53 +0800 2369) 
f6be8af1c95de kernel/sched/core.c (Chuansheng Liu             2014-09-04 15:17:53 +0800 2370) 	if (set_nr_if_polling(rq->idle)) {
f6be8af1c95de kernel/sched/core.c (Chuansheng Liu             2014-09-04 15:17:53 +0800 2371) 		trace_sched_wake_idle_without_ipi(cpu);
f6be8af1c95de kernel/sched/core.c (Chuansheng Liu             2014-09-04 15:17:53 +0800 2372) 	} else {
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 2373) 		rq_lock_irqsave(rq, &rf);
f6be8af1c95de kernel/sched/core.c (Chuansheng Liu             2014-09-04 15:17:53 +0800 2374) 		if (is_idle_task(rq->curr))
f6be8af1c95de kernel/sched/core.c (Chuansheng Liu             2014-09-04 15:17:53 +0800 2375) 			smp_send_reschedule(cpu);
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2376) 		/* Else CPU is not idle, do nothing here: */
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 2377) 		rq_unlock_irqrestore(rq, &rf);
f6be8af1c95de kernel/sched/core.c (Chuansheng Liu             2014-09-04 15:17:53 +0800 2378) 	}
fd7de1e8d5b2b kernel/sched/core.c (Andy Lutomirski            2014-11-29 08:13:51 -0800 2379) 
fd7de1e8d5b2b kernel/sched/core.c (Andy Lutomirski            2014-11-29 08:13:51 -0800 2380) out:
fd7de1e8d5b2b kernel/sched/core.c (Andy Lutomirski            2014-11-29 08:13:51 -0800 2381) 	rcu_read_unlock();
f6be8af1c95de kernel/sched/core.c (Chuansheng Liu             2014-09-04 15:17:53 +0800 2382) }
f6be8af1c95de kernel/sched/core.c (Chuansheng Liu             2014-09-04 15:17:53 +0800 2383) 
39be350127ec6 kernel/sched/core.c (Peter Zijlstra             2012-01-26 12:44:34 +0100 2384) bool cpus_share_cache(int this_cpu, int that_cpu)
518cd62341786 kernel/sched/core.c (Peter Zijlstra             2011-12-07 15:07:31 +0100 2385) {
518cd62341786 kernel/sched/core.c (Peter Zijlstra             2011-12-07 15:07:31 +0100 2386) 	return per_cpu(sd_llc_id, this_cpu) == per_cpu(sd_llc_id, that_cpu);
518cd62341786 kernel/sched/core.c (Peter Zijlstra             2011-12-07 15:07:31 +0100 2387) }
d6aa8f85f1637 kernel/sched.c      (Peter Zijlstra             2011-05-26 14:21:33 +0200 2388) #endif /* CONFIG_SMP */
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2389) 
b5179ac70de85 kernel/sched/core.c (Peter Zijlstra             2016-05-11 16:10:34 +0200 2390) static void ttwu_queue(struct task_struct *p, int cpu, int wake_flags)
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2391) {
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2392) 	struct rq *rq = cpu_rq(cpu);
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 2393) 	struct rq_flags rf;
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2394) 
17d9f311eca13 kernel/sched.c      (Daniel Hellstrom           2011-05-20 04:01:10 +0000 2395) #if defined(CONFIG_SMP)
39be350127ec6 kernel/sched/core.c (Peter Zijlstra             2012-01-26 12:44:34 +0100 2396) 	if (sched_feat(TTWU_QUEUE) && !cpus_share_cache(smp_processor_id(), cpu)) {
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2397) 		sched_clock_cpu(cpu); /* Sync clocks across CPUs */
b7e7ade34e618 kernel/sched/core.c (Peter Zijlstra             2016-05-23 11:19:07 +0200 2398) 		ttwu_queue_remote(p, cpu, wake_flags);
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2399) 		return;
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2400) 	}
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2401) #endif
317f394160e9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:58 +0200 2402) 
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 2403) 	rq_lock(rq, &rf);
77558e4d01ac0 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:36:23 +0100 2404) 	update_rq_clock(rq);
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 2405) 	ttwu_do_activate(rq, p, wake_flags, &rf);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 2406) 	rq_unlock(rq, &rf);
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2407) }
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2408) 
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2409) /*
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2410)  * Notes on Program-Order guarantees on SMP systems.
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2411)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2412)  *  MIGRATION
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2413)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2414)  * The basic program-order guarantee on SMP systems is that when a task [t]
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2415)  * migrates, all its activity on its old CPU [c0] happens-before any subsequent
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2416)  * execution on its new CPU [c1].
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2417)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2418)  * For migration (of runnable tasks) this is provided by the following means:
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2419)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2420)  *  A) UNLOCK of the rq(c0)->lock scheduling out task t
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2421)  *  B) migration for t is required to synchronize *both* rq(c0)->lock and
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2422)  *     rq(c1)->lock (if not at the same time, then in that order).
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2423)  *  C) LOCK of the rq(c1)->lock scheduling in task
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2424)  *
7696f9910a9a4 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:03 -0700 2425)  * Release/acquire chaining guarantees that B happens after A and C after B.
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2426)  * Note: the CPU doing B need not be c0 or c1
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2427)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2428)  * Example:
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2429)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2430)  *   CPU0            CPU1            CPU2
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2431)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2432)  *   LOCK rq(0)->lock
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2433)  *   sched-out X
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2434)  *   sched-in Y
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2435)  *   UNLOCK rq(0)->lock
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2436)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2437)  *                                   LOCK rq(0)->lock // orders against CPU0
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2438)  *                                   dequeue X
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2439)  *                                   UNLOCK rq(0)->lock
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2440)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2441)  *                                   LOCK rq(1)->lock
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2442)  *                                   enqueue X
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2443)  *                                   UNLOCK rq(1)->lock
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2444)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2445)  *                   LOCK rq(1)->lock // orders against CPU2
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2446)  *                   sched-out Z
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2447)  *                   sched-in X
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2448)  *                   UNLOCK rq(1)->lock
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2449)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2450)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2451)  *  BLOCKING -- aka. SLEEP + WAKEUP
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2452)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2453)  * For blocking we (obviously) need to provide the same guarantee as for
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2454)  * migration. However the means are completely different as there is no lock
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2455)  * chain to provide order. Instead we do:
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2456)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2457)  *   1) smp_store_release(X->on_cpu, 0)
1f03e8d291927 kernel/sched/core.c (Peter Zijlstra             2016-04-04 10:57:12 +0200 2458)  *   2) smp_cond_load_acquire(!X->on_cpu)
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2459)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2460)  * Example:
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2461)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2462)  *   CPU0 (schedule)  CPU1 (try_to_wake_up) CPU2 (schedule)
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2463)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2464)  *   LOCK rq(0)->lock LOCK X->pi_lock
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2465)  *   dequeue X
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2466)  *   sched-out X
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2467)  *   smp_store_release(X->on_cpu, 0);
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2468)  *
1f03e8d291927 kernel/sched/core.c (Peter Zijlstra             2016-04-04 10:57:12 +0200 2469)  *                    smp_cond_load_acquire(&X->on_cpu, !VAL);
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2470)  *                    X->state = WAKING
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2471)  *                    set_task_cpu(X,2)
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2472)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2473)  *                    LOCK rq(2)->lock
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2474)  *                    enqueue X
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2475)  *                    X->state = RUNNING
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2476)  *                    UNLOCK rq(2)->lock
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2477)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2478)  *                                          LOCK rq(2)->lock // orders against CPU1
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2479)  *                                          sched-out Z
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2480)  *                                          sched-in X
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2481)  *                                          UNLOCK rq(2)->lock
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2482)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2483)  *                    UNLOCK X->pi_lock
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2484)  *   UNLOCK rq(0)->lock
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2485)  *
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2486)  *
7696f9910a9a4 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:03 -0700 2487)  * However, for wakeups there is a second guarantee we must provide, namely we
7696f9910a9a4 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:03 -0700 2488)  * must ensure that CONDITION=1 done by the caller can not be reordered with
7696f9910a9a4 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:03 -0700 2489)  * accesses to the task state; see try_to_wake_up() and set_current_state().
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2490)  */
8643cda549ca4 kernel/sched/core.c (Peter Zijlstra             2015-11-17 19:01:11 +0100 2491) 
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2492) /**
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2493)  * try_to_wake_up - wake up a thread
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2494)  * @p: the thread to be awakened
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2495)  * @state: the mask of task states that can be woken
9ed3811a6c0d6 kernel/sched.c      (Tejun Heo                  2009-12-03 15:08:03 +0900 2496)  * @wake_flags: wake modifier flags (WF_*)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2497)  *
a225023828038 kernel/sched/core.c (Peter Zijlstra             2016-10-19 15:45:27 +0200 2498)  * If (@state & @p->state) @p->state = TASK_RUNNING.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2499)  *
a225023828038 kernel/sched/core.c (Peter Zijlstra             2016-10-19 15:45:27 +0200 2500)  * If the task was not queued/runnable, also place it back on a runqueue.
a225023828038 kernel/sched/core.c (Peter Zijlstra             2016-10-19 15:45:27 +0200 2501)  *
a225023828038 kernel/sched/core.c (Peter Zijlstra             2016-10-19 15:45:27 +0200 2502)  * Atomic against schedule() which would dequeue a task, also see
a225023828038 kernel/sched/core.c (Peter Zijlstra             2016-10-19 15:45:27 +0200 2503)  * set_current_state().
a225023828038 kernel/sched/core.c (Peter Zijlstra             2016-10-19 15:45:27 +0200 2504)  *
7696f9910a9a4 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:03 -0700 2505)  * This function executes a full memory barrier before accessing the task
7696f9910a9a4 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:03 -0700 2506)  * state; see set_current_state().
7696f9910a9a4 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:03 -0700 2507)  *
a225023828038 kernel/sched/core.c (Peter Zijlstra             2016-10-19 15:45:27 +0200 2508)  * Return: %true if @p->state changes (an actual wakeup was done),
a225023828038 kernel/sched/core.c (Peter Zijlstra             2016-10-19 15:45:27 +0200 2509)  *	   %false otherwise.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2510)  */
e4a52bcb9a181 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:54 +0200 2511) static int
e4a52bcb9a181 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:54 +0200 2512) try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2513) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2514) 	unsigned long flags;
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2515) 	int cpu, success = 0;
2398f2c6d34b4 kernel/sched.c      (Peter Zijlstra             2008-06-27 13:41:35 +0200 2516) 
e3d85487fba42 kernel/sched/core.c (Peter Zijlstra             2019-07-10 12:57:36 +0200 2517) 	preempt_disable();
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2518) 	if (p == current) {
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2519) 		/*
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2520) 		 * We're waking current, this means 'p->on_rq' and 'task_cpu(p)
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2521) 		 * == smp_processor_id()'. Together this means we can special
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2522) 		 * case the whole 'p->on_rq && ttwu_remote()' case below
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2523) 		 * without taking any locks.
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2524) 		 *
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2525) 		 * In particular:
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2526) 		 *  - we rely on Program-Order guarantees for all the ordering,
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2527) 		 *  - we're serialized against set_special_state() by virtue of
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2528) 		 *    it disabling IRQs (this allows not taking ->pi_lock).
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2529) 		 */
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2530) 		if (!(p->state & state))
e3d85487fba42 kernel/sched/core.c (Peter Zijlstra             2019-07-10 12:57:36 +0200 2531) 			goto out;
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2532) 
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2533) 		success = 1;
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2534) 		cpu = task_cpu(p);
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2535) 		trace_sched_waking(p);
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2536) 		p->state = TASK_RUNNING;
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2537) 		trace_sched_wakeup(p);
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2538) 		goto out;
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2539) 	}
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2540) 
e0acd0a68ec7d kernel/sched/core.c (Oleg Nesterov              2013-08-12 18:14:00 +0200 2541) 	/*
e0acd0a68ec7d kernel/sched/core.c (Oleg Nesterov              2013-08-12 18:14:00 +0200 2542) 	 * If we are going to wake up a thread waiting for CONDITION we
e0acd0a68ec7d kernel/sched/core.c (Oleg Nesterov              2013-08-12 18:14:00 +0200 2543) 	 * need to ensure that CONDITION=1 done by the caller can not be
e0acd0a68ec7d kernel/sched/core.c (Oleg Nesterov              2013-08-12 18:14:00 +0200 2544) 	 * reordered with p->state check below. This pairs with mb() in
e0acd0a68ec7d kernel/sched/core.c (Oleg Nesterov              2013-08-12 18:14:00 +0200 2545) 	 * set_current_state() the waiting thread does.
e0acd0a68ec7d kernel/sched/core.c (Oleg Nesterov              2013-08-12 18:14:00 +0200 2546) 	 */
013fdb8086aca kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:45 +0200 2547) 	raw_spin_lock_irqsave(&p->pi_lock, flags);
d89e588ca4081 kernel/sched/core.c (Peter Zijlstra             2016-09-05 11:37:53 +0200 2548) 	smp_mb__after_spinlock();
e9c8431185d6c kernel/sched.c      (Peter Zijlstra             2009-09-15 14:43:03 +0200 2549) 	if (!(p->state & state))
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2550) 		goto unlock;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2551) 
fbd705a0c6184 kernel/sched/core.c (Peter Zijlstra             2015-06-09 11:13:36 +0200 2552) 	trace_sched_waking(p);
fbd705a0c6184 kernel/sched/core.c (Peter Zijlstra             2015-06-09 11:13:36 +0200 2553) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2554) 	/* We're going to change ->state: */
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2555) 	success = 1;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2556) 	cpu = task_cpu(p);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2557) 
135e8c9250dd5 kernel/sched/core.c (Balbir Singh               2016-09-05 13:16:40 +1000 2558) 	/*
135e8c9250dd5 kernel/sched/core.c (Balbir Singh               2016-09-05 13:16:40 +1000 2559) 	 * Ensure we load p->on_rq _after_ p->state, otherwise it would
135e8c9250dd5 kernel/sched/core.c (Balbir Singh               2016-09-05 13:16:40 +1000 2560) 	 * be possible to, falsely, observe p->on_rq == 0 and get stuck
135e8c9250dd5 kernel/sched/core.c (Balbir Singh               2016-09-05 13:16:40 +1000 2561) 	 * in smp_cond_load_acquire() below.
135e8c9250dd5 kernel/sched/core.c (Balbir Singh               2016-09-05 13:16:40 +1000 2562) 	 *
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2563) 	 * sched_ttwu_pending()			try_to_wake_up()
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2564) 	 *   STORE p->on_rq = 1			  LOAD p->state
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2565) 	 *   UNLOCK rq->lock
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2566) 	 *
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2567) 	 * __schedule() (switch to task 'p')
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2568) 	 *   LOCK rq->lock			  smp_rmb();
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2569) 	 *   smp_mb__after_spinlock();
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2570) 	 *   UNLOCK rq->lock
135e8c9250dd5 kernel/sched/core.c (Balbir Singh               2016-09-05 13:16:40 +1000 2571) 	 *
135e8c9250dd5 kernel/sched/core.c (Balbir Singh               2016-09-05 13:16:40 +1000 2572) 	 * [task p]
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2573) 	 *   STORE p->state = UNINTERRUPTIBLE	  LOAD p->on_rq
135e8c9250dd5 kernel/sched/core.c (Balbir Singh               2016-09-05 13:16:40 +1000 2574) 	 *
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2575) 	 * Pairs with the LOCK+smp_mb__after_spinlock() on rq->lock in
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2576) 	 * __schedule().  See the comment for smp_mb__after_spinlock().
135e8c9250dd5 kernel/sched/core.c (Balbir Singh               2016-09-05 13:16:40 +1000 2577) 	 */
135e8c9250dd5 kernel/sched/core.c (Balbir Singh               2016-09-05 13:16:40 +1000 2578) 	smp_rmb();
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2579) 	if (p->on_rq && ttwu_remote(p, wake_flags))
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2580) 		goto unlock;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2581) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2582) #ifdef CONFIG_SMP
ecf7d01c229d1 kernel/sched/core.c (Peter Zijlstra             2015-10-07 14:14:13 +0200 2583) 	/*
ecf7d01c229d1 kernel/sched/core.c (Peter Zijlstra             2015-10-07 14:14:13 +0200 2584) 	 * Ensure we load p->on_cpu _after_ p->on_rq, otherwise it would be
ecf7d01c229d1 kernel/sched/core.c (Peter Zijlstra             2015-10-07 14:14:13 +0200 2585) 	 * possible to, falsely, observe p->on_cpu == 0.
ecf7d01c229d1 kernel/sched/core.c (Peter Zijlstra             2015-10-07 14:14:13 +0200 2586) 	 *
ecf7d01c229d1 kernel/sched/core.c (Peter Zijlstra             2015-10-07 14:14:13 +0200 2587) 	 * One must be running (->on_cpu == 1) in order to remove oneself
ecf7d01c229d1 kernel/sched/core.c (Peter Zijlstra             2015-10-07 14:14:13 +0200 2588) 	 * from the runqueue.
ecf7d01c229d1 kernel/sched/core.c (Peter Zijlstra             2015-10-07 14:14:13 +0200 2589) 	 *
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2590) 	 * __schedule() (switch to task 'p')	try_to_wake_up()
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2591) 	 *   STORE p->on_cpu = 1		  LOAD p->on_rq
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2592) 	 *   UNLOCK rq->lock
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2593) 	 *
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2594) 	 * __schedule() (put 'p' to sleep)
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2595) 	 *   LOCK rq->lock			  smp_rmb();
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2596) 	 *   smp_mb__after_spinlock();
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2597) 	 *   STORE p->on_rq = 0			  LOAD p->on_cpu
ecf7d01c229d1 kernel/sched/core.c (Peter Zijlstra             2015-10-07 14:14:13 +0200 2598) 	 *
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2599) 	 * Pairs with the LOCK+smp_mb__after_spinlock() on rq->lock in
3d85b27037836 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:02 -0700 2600) 	 * __schedule().  See the comment for smp_mb__after_spinlock().
ecf7d01c229d1 kernel/sched/core.c (Peter Zijlstra             2015-10-07 14:14:13 +0200 2601) 	 */
ecf7d01c229d1 kernel/sched/core.c (Peter Zijlstra             2015-10-07 14:14:13 +0200 2602) 	smp_rmb();
ecf7d01c229d1 kernel/sched/core.c (Peter Zijlstra             2015-10-07 14:14:13 +0200 2603) 
e9c8431185d6c kernel/sched.c      (Peter Zijlstra             2009-09-15 14:43:03 +0200 2604) 	/*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2605) 	 * If the owning (remote) CPU is still in the middle of schedule() with
c05fbafba1c54 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:57 +0200 2606) 	 * this task as prev, wait until its done referencing the task.
b75a22531588e kernel/sched/core.c (Peter Zijlstra             2015-10-06 14:36:17 +0200 2607) 	 *
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 2608) 	 * Pairs with the smp_store_release() in finish_task().
b75a22531588e kernel/sched/core.c (Peter Zijlstra             2015-10-06 14:36:17 +0200 2609) 	 *
b75a22531588e kernel/sched/core.c (Peter Zijlstra             2015-10-06 14:36:17 +0200 2610) 	 * This ensures that tasks getting woken will be fully ordered against
b75a22531588e kernel/sched/core.c (Peter Zijlstra             2015-10-06 14:36:17 +0200 2611) 	 * their previous state and preserve Program Order.
0970d2992dfd7 kernel/sched.c      (Peter Zijlstra             2010-02-15 14:45:54 +0100 2612) 	 */
1f03e8d291927 kernel/sched/core.c (Peter Zijlstra             2016-04-04 10:57:12 +0200 2613) 	smp_cond_load_acquire(&p->on_cpu, !VAL);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2614) 
a8e4f2eaecc9b kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:49 +0200 2615) 	p->sched_contributes_to_load = !!task_contributes_to_load(p);
e9c8431185d6c kernel/sched.c      (Peter Zijlstra             2009-09-15 14:43:03 +0200 2616) 	p->state = TASK_WAKING;
e7693a362ec84 kernel/sched.c      (Gregory Haskins            2008-01-25 21:08:09 +0100 2617) 
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 2618) 	if (p->in_iowait) {
c96f5471ce7d2 kernel/sched/core.c (Josh Snyder                2017-12-18 16:15:10 +0000 2619) 		delayacct_blkio_end(p);
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 2620) 		atomic_dec(&task_rq(p)->nr_iowait);
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 2621) 	}
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 2622) 
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 2623) 	cpu = select_task_rq(p, p->wake_cpu, SD_BALANCE_WAKE, wake_flags);
f339b9dc1f035 kernel/sched.c      (Peter Zijlstra             2011-05-31 10:49:20 +0200 2624) 	if (task_cpu(p) != cpu) {
f339b9dc1f035 kernel/sched.c      (Peter Zijlstra             2011-05-31 10:49:20 +0200 2625) 		wake_flags |= WF_MIGRATED;
eb414681d5a07 kernel/sched/core.c (Johannes Weiner            2018-10-26 15:06:27 -0700 2626) 		psi_ttwu_dequeue(p);
e4a52bcb9a181 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:54 +0200 2627) 		set_task_cpu(p, cpu);
f339b9dc1f035 kernel/sched.c      (Peter Zijlstra             2011-05-31 10:49:20 +0200 2628) 	}
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 2629) 
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 2630) #else /* CONFIG_SMP */
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 2631) 
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 2632) 	if (p->in_iowait) {
c96f5471ce7d2 kernel/sched/core.c (Josh Snyder                2017-12-18 16:15:10 +0000 2633) 		delayacct_blkio_end(p);
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 2634) 		atomic_dec(&task_rq(p)->nr_iowait);
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 2635) 	}
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 2636) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2637) #endif /* CONFIG_SMP */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2638) 
b5179ac70de85 kernel/sched/core.c (Peter Zijlstra             2016-05-11 16:10:34 +0200 2639) 	ttwu_queue(p, cpu, wake_flags);
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2640) unlock:
013fdb8086aca kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:45 +0200 2641) 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2642) out:
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2643) 	if (success)
aacedf26fb760 kernel/sched/core.c (Peter Zijlstra             2019-06-07 15:39:49 +0200 2644) 		ttwu_stat(p, cpu, wake_flags);
e3d85487fba42 kernel/sched/core.c (Peter Zijlstra             2019-07-10 12:57:36 +0200 2645) 	preempt_enable();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2646) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2647) 	return success;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2648) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2649) 
50fa610a3b6ba kernel/sched.c      (David Howells              2009-04-28 15:01:38 +0100 2650) /**
50fa610a3b6ba kernel/sched.c      (David Howells              2009-04-28 15:01:38 +0100 2651)  * wake_up_process - Wake up a specific process
50fa610a3b6ba kernel/sched.c      (David Howells              2009-04-28 15:01:38 +0100 2652)  * @p: The process to be woken up.
50fa610a3b6ba kernel/sched.c      (David Howells              2009-04-28 15:01:38 +0100 2653)  *
50fa610a3b6ba kernel/sched.c      (David Howells              2009-04-28 15:01:38 +0100 2654)  * Attempt to wake up the nominated process and move it to the set of runnable
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 2655)  * processes.
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 2656)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 2657)  * Return: 1 if the process was woken up, 0 if it was already running.
50fa610a3b6ba kernel/sched.c      (David Howells              2009-04-28 15:01:38 +0100 2658)  *
7696f9910a9a4 kernel/sched/core.c (Andrea Parri               2018-07-16 11:06:03 -0700 2659)  * This function executes a full memory barrier before accessing the task state.
50fa610a3b6ba kernel/sched.c      (David Howells              2009-04-28 15:01:38 +0100 2660)  */
7ad5b3a505e68 kernel/sched.c      (Harvey Harrison            2008-02-08 04:19:53 -0800 2661) int wake_up_process(struct task_struct *p)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2662) {
9067ac85d5336 kernel/sched/core.c (Oleg Nesterov              2013-01-21 20:48:17 +0100 2663) 	return try_to_wake_up(p, TASK_NORMAL, 0);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2664) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2665) EXPORT_SYMBOL(wake_up_process);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2666) 
7ad5b3a505e68 kernel/sched.c      (Harvey Harrison            2008-02-08 04:19:53 -0800 2667) int wake_up_state(struct task_struct *p, unsigned int state)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2668) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2669) 	return try_to_wake_up(p, state, 0);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2670) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2671) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2672) /*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2673)  * Perform scheduler related setup for a newly forked process p.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2674)  * p is forked by current.
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2675)  *
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2676)  * __sched_fork() is basic setup used by init_idle() too:
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2677)  */
5e1576ed0e54d kernel/sched/core.c (Rik van Riel               2013-10-07 11:29:26 +0100 2678) static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2679) {
fd2f4419b4cbe kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:44 +0200 2680) 	p->on_rq			= 0;
fd2f4419b4cbe kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:44 +0200 2681) 
fd2f4419b4cbe kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:44 +0200 2682) 	p->se.on_rq			= 0;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2683) 	p->se.exec_start		= 0;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2684) 	p->se.sum_exec_runtime		= 0;
f6cf891c4d712 kernel/sched.c      (Ingo Molnar                2007-08-28 12:53:24 +0200 2685) 	p->se.prev_sum_exec_runtime	= 0;
6c594c21fcb02 kernel/sched.c      (Ingo Molnar                2008-12-14 12:34:15 +0100 2686) 	p->se.nr_migrations		= 0;
da7a735e51f96 kernel/sched.c      (Peter Zijlstra             2011-01-17 17:03:27 +0100 2687) 	p->se.vruntime			= 0;
fd2f4419b4cbe kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:44 +0200 2688) 	INIT_LIST_HEAD(&p->se.group_node);
6cfb0d5d06bea kernel/sched.c      (Ingo Molnar                2007-08-02 17:41:40 +0200 2689) 
ad936d8658fd3 kernel/sched/core.c (Byungchul Park             2015-10-24 01:16:19 +0900 2690) #ifdef CONFIG_FAIR_GROUP_SCHED
ad936d8658fd3 kernel/sched/core.c (Byungchul Park             2015-10-24 01:16:19 +0900 2691) 	p->se.cfs_rq			= NULL;
ad936d8658fd3 kernel/sched/core.c (Byungchul Park             2015-10-24 01:16:19 +0900 2692) #endif
ad936d8658fd3 kernel/sched/core.c (Byungchul Park             2015-10-24 01:16:19 +0900 2693) 
6cfb0d5d06bea kernel/sched.c      (Ingo Molnar                2007-08-02 17:41:40 +0200 2694) #ifdef CONFIG_SCHEDSTATS
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2695) 	/* Even if schedstat is disabled, there should not be garbage */
41acab8851a04 kernel/sched.c      (Lucas De Marchi            2010-03-10 23:37:45 -0300 2696) 	memset(&p->se.statistics, 0, sizeof(p->se.statistics));
6cfb0d5d06bea kernel/sched.c      (Ingo Molnar                2007-08-02 17:41:40 +0200 2697) #endif
476d139c218e4 kernel/sched.c      (Nick Piggin                2005-06-25 14:57:29 -0700 2698) 
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 2699) 	RB_CLEAR_NODE(&p->dl.rb_node);
40767b0dc7680 kernel/sched/core.c (Peter Zijlstra             2015-01-28 15:08:03 +0100 2700) 	init_dl_task_timer(&p->dl);
209a0cbda7a01 kernel/sched/core.c (Luca Abeni                 2017-05-18 22:13:29 +0200 2701) 	init_dl_inactive_task_timer(&p->dl);
a5e7be3b28a23 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:39 +0100 2702) 	__dl_clear_params(p);
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 2703) 
fa717060f1ab7 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:27 +0100 2704) 	INIT_LIST_HEAD(&p->rt.run_list);
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 2705) 	p->rt.timeout		= 0;
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 2706) 	p->rt.time_slice	= sched_rr_timeslice;
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 2707) 	p->rt.on_rq		= 0;
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 2708) 	p->rt.on_list		= 0;
476d139c218e4 kernel/sched.c      (Nick Piggin                2005-06-25 14:57:29 -0700 2709) 
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 2710) #ifdef CONFIG_PREEMPT_NOTIFIERS
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 2711) 	INIT_HLIST_HEAD(&p->preempt_notifiers);
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 2712) #endif
cbee9f88ec1b8 kernel/sched/core.c (Peter Zijlstra             2012-10-25 14:16:43 +0200 2713) 
5e1f0f098b464 kernel/sched/core.c (Mel Gorman                 2019-03-05 15:45:41 -0800 2714) #ifdef CONFIG_COMPACTION
5e1f0f098b464 kernel/sched/core.c (Mel Gorman                 2019-03-05 15:45:41 -0800 2715) 	p->capture_control = NULL;
5e1f0f098b464 kernel/sched/core.c (Mel Gorman                 2019-03-05 15:45:41 -0800 2716) #endif
1378447598432 kernel/sched/core.c (Mel Gorman                 2018-05-04 16:41:09 +0100 2717) 	init_numa_balancing(clone_flags, p);
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2718) }
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2719) 
2a595721a1fa6 kernel/sched/core.c (Srikar Dronamraju          2015-08-11 21:54:21 +0530 2720) DEFINE_STATIC_KEY_FALSE(sched_numa_balancing);
2a595721a1fa6 kernel/sched/core.c (Srikar Dronamraju          2015-08-11 21:54:21 +0530 2721) 
1a687c2e9a993 kernel/sched/core.c (Mel Gorman                 2012-11-22 11:16:36 +0000 2722) #ifdef CONFIG_NUMA_BALANCING
c3b9bc5bbfc37 kernel/sched/core.c (Srikar Dronamraju          2015-08-11 16:30:12 +0530 2723) 
1a687c2e9a993 kernel/sched/core.c (Mel Gorman                 2012-11-22 11:16:36 +0000 2724) void set_numabalancing_state(bool enabled)
1a687c2e9a993 kernel/sched/core.c (Mel Gorman                 2012-11-22 11:16:36 +0000 2725) {
1a687c2e9a993 kernel/sched/core.c (Mel Gorman                 2012-11-22 11:16:36 +0000 2726) 	if (enabled)
2a595721a1fa6 kernel/sched/core.c (Srikar Dronamraju          2015-08-11 21:54:21 +0530 2727) 		static_branch_enable(&sched_numa_balancing);
1a687c2e9a993 kernel/sched/core.c (Mel Gorman                 2012-11-22 11:16:36 +0000 2728) 	else
2a595721a1fa6 kernel/sched/core.c (Srikar Dronamraju          2015-08-11 21:54:21 +0530 2729) 		static_branch_disable(&sched_numa_balancing);
1a687c2e9a993 kernel/sched/core.c (Mel Gorman                 2012-11-22 11:16:36 +0000 2730) }
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2731) 
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2732) #ifdef CONFIG_PROC_SYSCTL
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2733) int sysctl_numa_balancing(struct ctl_table *table, int write,
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2734) 			 void __user *buffer, size_t *lenp, loff_t *ppos)
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2735) {
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2736) 	struct ctl_table t;
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2737) 	int err;
2a595721a1fa6 kernel/sched/core.c (Srikar Dronamraju          2015-08-11 21:54:21 +0530 2738) 	int state = static_branch_likely(&sched_numa_balancing);
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2739) 
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2740) 	if (write && !capable(CAP_SYS_ADMIN))
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2741) 		return -EPERM;
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2742) 
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2743) 	t = *table;
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2744) 	t.data = &state;
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2745) 	err = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2746) 	if (err < 0)
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2747) 		return err;
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2748) 	if (write)
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2749) 		set_numabalancing_state(state);
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2750) 	return err;
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2751) }
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2752) #endif
54a43d54988a3 kernel/sched/core.c (Andi Kleen                 2014-01-23 15:53:13 -0800 2753) #endif
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2754) 
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2755) #ifdef CONFIG_SCHEDSTATS
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2756) 
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2757) DEFINE_STATIC_KEY_FALSE(sched_schedstats);
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2758) static bool __initdata __sched_schedstats = false;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2759) 
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2760) static void set_schedstats(bool enabled)
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2761) {
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2762) 	if (enabled)
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2763) 		static_branch_enable(&sched_schedstats);
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2764) 	else
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2765) 		static_branch_disable(&sched_schedstats);
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2766) }
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2767) 
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2768) void force_schedstat_enabled(void)
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2769) {
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2770) 	if (!schedstat_enabled()) {
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2771) 		pr_info("kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\n");
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2772) 		static_branch_enable(&sched_schedstats);
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2773) 	}
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2774) }
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2775) 
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2776) static int __init setup_schedstats(char *str)
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2777) {
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2778) 	int ret = 0;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2779) 	if (!str)
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2780) 		goto out;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2781) 
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2782) 	/*
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2783) 	 * This code is called before jump labels have been set up, so we can't
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2784) 	 * change the static branch directly just yet.  Instead set a temporary
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2785) 	 * variable so init_schedstats() can do it later.
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2786) 	 */
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2787) 	if (!strcmp(str, "enable")) {
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2788) 		__sched_schedstats = true;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2789) 		ret = 1;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2790) 	} else if (!strcmp(str, "disable")) {
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2791) 		__sched_schedstats = false;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2792) 		ret = 1;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2793) 	}
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2794) out:
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2795) 	if (!ret)
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2796) 		pr_warn("Unable to parse schedstats=\n");
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2797) 
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2798) 	return ret;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2799) }
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2800) __setup("schedstats=", setup_schedstats);
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2801) 
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2802) static void __init init_schedstats(void)
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2803) {
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2804) 	set_schedstats(__sched_schedstats);
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2805) }
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2806) 
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2807) #ifdef CONFIG_PROC_SYSCTL
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2808) int sysctl_schedstats(struct ctl_table *table, int write,
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2809) 			 void __user *buffer, size_t *lenp, loff_t *ppos)
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2810) {
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2811) 	struct ctl_table t;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2812) 	int err;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2813) 	int state = static_branch_likely(&sched_schedstats);
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2814) 
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2815) 	if (write && !capable(CAP_SYS_ADMIN))
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2816) 		return -EPERM;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2817) 
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2818) 	t = *table;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2819) 	t.data = &state;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2820) 	err = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2821) 	if (err < 0)
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2822) 		return err;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2823) 	if (write)
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2824) 		set_schedstats(state);
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2825) 	return err;
cb2517653fcca kernel/sched/core.c (Mel Gorman                 2016-02-05 09:08:36 +0000 2826) }
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2827) #endif /* CONFIG_PROC_SYSCTL */
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2828) #else  /* !CONFIG_SCHEDSTATS */
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2829) static inline void init_schedstats(void) {}
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 2830) #endif /* CONFIG_SCHEDSTATS */
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2831) 
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2832) /*
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2833)  * fork()/clone()-time setup:
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2834)  */
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 2835) int sched_fork(unsigned long clone_flags, struct task_struct *p)
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2836) {
0122ec5b02f76 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:51 +0200 2837) 	unsigned long flags;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2838) 
5e1576ed0e54d kernel/sched/core.c (Rik van Riel               2013-10-07 11:29:26 +0100 2839) 	__sched_fork(clone_flags, p);
06b83b5fbea27 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:35 +0100 2840) 	/*
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 2841) 	 * We mark the process as NEW here. This guarantees that
06b83b5fbea27 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:35 +0100 2842) 	 * nobody will actually run it, and a signal or other external
06b83b5fbea27 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:35 +0100 2843) 	 * event cannot wake it up and insert it on the runqueue either.
06b83b5fbea27 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:35 +0100 2844) 	 */
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 2845) 	p->state = TASK_NEW;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2846) 
c350a04efd1c8 kernel/sched.c      (Mike Galbraith             2011-07-27 17:14:55 +0200 2847) 	/*
c350a04efd1c8 kernel/sched.c      (Mike Galbraith             2011-07-27 17:14:55 +0200 2848) 	 * Make sure we do not leak PI boosting priority to the child.
c350a04efd1c8 kernel/sched.c      (Mike Galbraith             2011-07-27 17:14:55 +0200 2849) 	 */
c350a04efd1c8 kernel/sched.c      (Mike Galbraith             2011-07-27 17:14:55 +0200 2850) 	p->prio = current->normal_prio;
c350a04efd1c8 kernel/sched.c      (Mike Galbraith             2011-07-27 17:14:55 +0200 2851) 
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 2852) 	uclamp_fork(p);
e8f14172c6b11 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:05 +0100 2853) 
b9dc29e72fd3d kernel/sched.c      (Mike Galbraith             2009-06-17 10:46:01 +0200 2854) 	/*
b9dc29e72fd3d kernel/sched.c      (Mike Galbraith             2009-06-17 10:46:01 +0200 2855) 	 * Revert to default priority/policy on fork if requested.
b9dc29e72fd3d kernel/sched.c      (Mike Galbraith             2009-06-17 10:46:01 +0200 2856) 	 */
b9dc29e72fd3d kernel/sched.c      (Mike Galbraith             2009-06-17 10:46:01 +0200 2857) 	if (unlikely(p->sched_reset_on_fork)) {
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 2858) 		if (task_has_dl_policy(p) || task_has_rt_policy(p)) {
b9dc29e72fd3d kernel/sched.c      (Mike Galbraith             2009-06-17 10:46:01 +0200 2859) 			p->policy = SCHED_NORMAL;
6c697bdf08a09 kernel/sched.c      (Mike Galbraith             2009-06-17 10:48:02 +0200 2860) 			p->static_prio = NICE_TO_PRIO(0);
c350a04efd1c8 kernel/sched.c      (Mike Galbraith             2011-07-27 17:14:55 +0200 2861) 			p->rt_priority = 0;
c350a04efd1c8 kernel/sched.c      (Mike Galbraith             2011-07-27 17:14:55 +0200 2862) 		} else if (PRIO_TO_NICE(p->static_prio) < 0)
c350a04efd1c8 kernel/sched.c      (Mike Galbraith             2011-07-27 17:14:55 +0200 2863) 			p->static_prio = NICE_TO_PRIO(0);
c350a04efd1c8 kernel/sched.c      (Mike Galbraith             2011-07-27 17:14:55 +0200 2864) 
c350a04efd1c8 kernel/sched.c      (Mike Galbraith             2011-07-27 17:14:55 +0200 2865) 		p->prio = p->normal_prio = __normal_prio(p);
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200 2866) 		set_load_weight(p, false);
6c697bdf08a09 kernel/sched.c      (Mike Galbraith             2009-06-17 10:48:02 +0200 2867) 
b9dc29e72fd3d kernel/sched.c      (Mike Galbraith             2009-06-17 10:46:01 +0200 2868) 		/*
b9dc29e72fd3d kernel/sched.c      (Mike Galbraith             2009-06-17 10:46:01 +0200 2869) 		 * We don't need the reset flag anymore after the fork. It has
b9dc29e72fd3d kernel/sched.c      (Mike Galbraith             2009-06-17 10:46:01 +0200 2870) 		 * fulfilled its duty:
b9dc29e72fd3d kernel/sched.c      (Mike Galbraith             2009-06-17 10:46:01 +0200 2871) 		 */
b9dc29e72fd3d kernel/sched.c      (Mike Galbraith             2009-06-17 10:46:01 +0200 2872) 		p->sched_reset_on_fork = 0;
b9dc29e72fd3d kernel/sched.c      (Mike Galbraith             2009-06-17 10:46:01 +0200 2873) 	}
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 2874) 
af0fffd9300b9 kernel/sched/core.c (Sebastian Andrzej Siewior  2018-07-06 15:06:15 +0200 2875) 	if (dl_prio(p->prio))
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 2876) 		return -EAGAIN;
af0fffd9300b9 kernel/sched/core.c (Sebastian Andrzej Siewior  2018-07-06 15:06:15 +0200 2877) 	else if (rt_prio(p->prio))
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 2878) 		p->sched_class = &rt_sched_class;
af0fffd9300b9 kernel/sched/core.c (Sebastian Andrzej Siewior  2018-07-06 15:06:15 +0200 2879) 	else
2ddbf952508fb kernel/sched.c      (Hiroshi Shimamoto          2007-10-15 17:00:11 +0200 2880) 		p->sched_class = &fair_sched_class;
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 2881) 
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 2882) 	init_entity_runnable_average(&p->se);
cd29fe6f2637c kernel/sched.c      (Peter Zijlstra             2009-11-27 17:32:46 +0100 2883) 
8695159967957 kernel/sched.c      (Peter Zijlstra             2010-06-22 11:44:53 +0200 2884) 	/*
8695159967957 kernel/sched.c      (Peter Zijlstra             2010-06-22 11:44:53 +0200 2885) 	 * The child is not yet in the pid-hash so no cgroup attach races,
8695159967957 kernel/sched.c      (Peter Zijlstra             2010-06-22 11:44:53 +0200 2886) 	 * and the cgroup is pinned to this child due to cgroup_fork()
8695159967957 kernel/sched.c      (Peter Zijlstra             2010-06-22 11:44:53 +0200 2887) 	 * is ran before sched_fork().
8695159967957 kernel/sched.c      (Peter Zijlstra             2010-06-22 11:44:53 +0200 2888) 	 *
8695159967957 kernel/sched.c      (Peter Zijlstra             2010-06-22 11:44:53 +0200 2889) 	 * Silence PROVE_RCU.
8695159967957 kernel/sched.c      (Peter Zijlstra             2010-06-22 11:44:53 +0200 2890) 	 */
0122ec5b02f76 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:51 +0200 2891) 	raw_spin_lock_irqsave(&p->pi_lock, flags);
e210bffd39d01 kernel/sched/core.c (Peter Zijlstra             2016-06-16 18:51:48 +0200 2892) 	/*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2893) 	 * We're setting the CPU for the first time, we don't migrate,
e210bffd39d01 kernel/sched/core.c (Peter Zijlstra             2016-06-16 18:51:48 +0200 2894) 	 * so use __set_task_cpu().
e210bffd39d01 kernel/sched/core.c (Peter Zijlstra             2016-06-16 18:51:48 +0200 2895) 	 */
af0fffd9300b9 kernel/sched/core.c (Sebastian Andrzej Siewior  2018-07-06 15:06:15 +0200 2896) 	__set_task_cpu(p, smp_processor_id());
e210bffd39d01 kernel/sched/core.c (Peter Zijlstra             2016-06-16 18:51:48 +0200 2897) 	if (p->sched_class->task_fork)
e210bffd39d01 kernel/sched/core.c (Peter Zijlstra             2016-06-16 18:51:48 +0200 2898) 		p->sched_class->task_fork(p);
0122ec5b02f76 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:51 +0200 2899) 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
5f3edc1b1ead6 kernel/sched.c      (Peter Zijlstra             2009-09-10 13:42:00 +0200 2900) 
f6db834799325 kernel/sched/core.c (Naveen N. Rao              2015-06-25 23:53:37 +0530 2901) #ifdef CONFIG_SCHED_INFO
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2902) 	if (likely(sched_info_on()))
52f17b6c2bd44 kernel/sched.c      (Chandra Seetharaman        2006-07-14 00:24:38 -0700 2903) 		memset(&p->sched_info, 0, sizeof(p->sched_info));
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2904) #endif
3ca7a440da394 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:40 +0200 2905) #if defined(CONFIG_SMP)
3ca7a440da394 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:40 +0200 2906) 	p->on_cpu = 0;
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 2907) #endif
01028747559ac kernel/sched/core.c (Peter Zijlstra             2013-08-14 14:55:46 +0200 2908) 	init_task_preempt_count(p);
806c09a7db457 kernel/sched.c      (Dario Faggioli             2010-11-30 19:51:33 +0100 2909) #ifdef CONFIG_SMP
917b627d4d981 kernel/sched.c      (Gregory Haskins            2008-12-29 09:39:53 -0500 2910) 	plist_node_init(&p->pushable_tasks, MAX_PRIO);
1baca4ce16b8c kernel/sched/core.c (Juri Lelli                 2013-11-07 14:43:38 +0100 2911) 	RB_CLEAR_NODE(&p->pushable_dl_tasks);
806c09a7db457 kernel/sched.c      (Dario Faggioli             2010-11-30 19:51:33 +0100 2912) #endif
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 2913) 	return 0;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2914) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2915) 
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 2916) unsigned long to_ratio(u64 period, u64 runtime)
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 2917) {
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 2918) 	if (runtime == RUNTIME_INF)
c52f14d384628 kernel/sched/core.c (Luca Abeni                 2017-05-18 22:13:31 +0200 2919) 		return BW_UNIT;
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 2920) 
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 2921) 	/*
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 2922) 	 * Doing this here saves a lot of checks in all
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 2923) 	 * the calling paths, and returning zero seems
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 2924) 	 * safe for them anyway.
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 2925) 	 */
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 2926) 	if (period == 0)
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 2927) 		return 0;
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 2928) 
c52f14d384628 kernel/sched/core.c (Luca Abeni                 2017-05-18 22:13:31 +0200 2929) 	return div64_u64(runtime << BW_SHIFT, period);
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 2930) }
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 2931) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2932) /*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2933)  * wake_up_new_task - wake up a newly created task for the first time.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2934)  *
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2935)  * This function will do some initial scheduler statistics housekeeping
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2936)  * that must be done for every newly created context, then puts the task
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2937)  * on the runqueue and wakes it.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2938)  */
3e51e3edfd81b kernel/sched.c      (Samir Bellabes             2011-05-11 18:18:05 +0200 2939) void wake_up_new_task(struct task_struct *p)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2940) {
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 2941) 	struct rq_flags rf;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 2942) 	struct rq *rq;
fabf318e5e4bd kernel/sched.c      (Peter Zijlstra             2010-01-21 21:04:57 +0100 2943) 
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 2944) 	raw_spin_lock_irqsave(&p->pi_lock, rf.flags);
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 2945) 	p->state = TASK_RUNNING;
fabf318e5e4bd kernel/sched.c      (Peter Zijlstra             2010-01-21 21:04:57 +0100 2946) #ifdef CONFIG_SMP
fabf318e5e4bd kernel/sched.c      (Peter Zijlstra             2010-01-21 21:04:57 +0100 2947) 	/*
fabf318e5e4bd kernel/sched.c      (Peter Zijlstra             2010-01-21 21:04:57 +0100 2948) 	 * Fork balancing, do it here and not earlier because:
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 2949) 	 *  - cpus_ptr can change in the fork path
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 2950) 	 *  - any previously selected CPU might disappear through hotplug
e210bffd39d01 kernel/sched/core.c (Peter Zijlstra             2016-06-16 18:51:48 +0200 2951) 	 *
e210bffd39d01 kernel/sched/core.c (Peter Zijlstra             2016-06-16 18:51:48 +0200 2952) 	 * Use __set_task_cpu() to avoid calling sched_class::migrate_task_rq,
e210bffd39d01 kernel/sched/core.c (Peter Zijlstra             2016-06-16 18:51:48 +0200 2953) 	 * as we're not fully set-up yet.
fabf318e5e4bd kernel/sched.c      (Peter Zijlstra             2010-01-21 21:04:57 +0100 2954) 	 */
32e839dda3ba5 kernel/sched/core.c (Mel Gorman                 2018-01-30 10:45:55 +0000 2955) 	p->recent_used_cpu = task_cpu(p);
e210bffd39d01 kernel/sched/core.c (Peter Zijlstra             2016-06-16 18:51:48 +0200 2956) 	__set_task_cpu(p, select_task_rq(p, task_cpu(p), SD_BALANCE_FORK, 0));
0017d73509284 kernel/sched.c      (Peter Zijlstra             2010-03-24 18:34:10 +0100 2957) #endif
b7fa30c9cc48c kernel/sched/core.c (Peter Zijlstra             2016-06-09 15:07:50 +0200 2958) 	rq = __task_rq_lock(p, &rf);
4126bad671733 kernel/sched/core.c (Peter Zijlstra             2016-10-03 16:20:59 +0200 2959) 	update_rq_clock(rq);
d0fe0b9c45c14 kernel/sched/core.c (Dietmar Eggemann           2019-01-22 16:25:01 +0000 2960) 	post_init_entity_util_avg(p);
0017d73509284 kernel/sched.c      (Peter Zijlstra             2010-03-24 18:34:10 +0100 2961) 
7a57f32a4d5c8 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:47:02 +0100 2962) 	activate_task(rq, p, ENQUEUE_NOCLOCK);
fbd705a0c6184 kernel/sched/core.c (Peter Zijlstra             2015-06-09 11:13:36 +0200 2963) 	trace_sched_wakeup_new(p);
a7558e01056f5 kernel/sched.c      (Peter Zijlstra             2009-09-14 20:02:34 +0200 2964) 	check_preempt_curr(rq, p, WF_FORK);
9a897c5a6701b kernel/sched.c      (Steven Rostedt             2008-01-25 21:08:22 +0100 2965) #ifdef CONFIG_SMP
0aaafaabfcba8 kernel/sched/core.c (Peter Zijlstra             2015-10-23 11:50:08 +0200 2966) 	if (p->sched_class->task_woken) {
0aaafaabfcba8 kernel/sched/core.c (Peter Zijlstra             2015-10-23 11:50:08 +0200 2967) 		/*
0aaafaabfcba8 kernel/sched/core.c (Peter Zijlstra             2015-10-23 11:50:08 +0200 2968) 		 * Nothing relies on rq->lock after this, so its fine to
0aaafaabfcba8 kernel/sched/core.c (Peter Zijlstra             2015-10-23 11:50:08 +0200 2969) 		 * drop it.
0aaafaabfcba8 kernel/sched/core.c (Peter Zijlstra             2015-10-23 11:50:08 +0200 2970) 		 */
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 2971) 		rq_unpin_lock(rq, &rf);
efbbd05a59534 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:40 +0100 2972) 		p->sched_class->task_woken(rq, p);
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 2973) 		rq_repin_lock(rq, &rf);
0aaafaabfcba8 kernel/sched/core.c (Peter Zijlstra             2015-10-23 11:50:08 +0200 2974) 	}
9a897c5a6701b kernel/sched.c      (Steven Rostedt             2008-01-25 21:08:22 +0100 2975) #endif
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 2976) 	task_rq_unlock(rq, p, &rf);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2977) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 2978) 
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 2979) #ifdef CONFIG_PREEMPT_NOTIFIERS
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 2980) 
b720342849fe6 kernel/sched/core.c (Davidlohr Bueso            2018-03-26 14:09:26 -0700 2981) static DEFINE_STATIC_KEY_FALSE(preempt_notifier_key);
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 2982) 
2ecd9d29abb17 kernel/sched/core.c (Peter Zijlstra             2015-07-03 18:53:58 +0200 2983) void preempt_notifier_inc(void)
2ecd9d29abb17 kernel/sched/core.c (Peter Zijlstra             2015-07-03 18:53:58 +0200 2984) {
b720342849fe6 kernel/sched/core.c (Davidlohr Bueso            2018-03-26 14:09:26 -0700 2985) 	static_branch_inc(&preempt_notifier_key);
2ecd9d29abb17 kernel/sched/core.c (Peter Zijlstra             2015-07-03 18:53:58 +0200 2986) }
2ecd9d29abb17 kernel/sched/core.c (Peter Zijlstra             2015-07-03 18:53:58 +0200 2987) EXPORT_SYMBOL_GPL(preempt_notifier_inc);
2ecd9d29abb17 kernel/sched/core.c (Peter Zijlstra             2015-07-03 18:53:58 +0200 2988) 
2ecd9d29abb17 kernel/sched/core.c (Peter Zijlstra             2015-07-03 18:53:58 +0200 2989) void preempt_notifier_dec(void)
2ecd9d29abb17 kernel/sched/core.c (Peter Zijlstra             2015-07-03 18:53:58 +0200 2990) {
b720342849fe6 kernel/sched/core.c (Davidlohr Bueso            2018-03-26 14:09:26 -0700 2991) 	static_branch_dec(&preempt_notifier_key);
2ecd9d29abb17 kernel/sched/core.c (Peter Zijlstra             2015-07-03 18:53:58 +0200 2992) }
2ecd9d29abb17 kernel/sched/core.c (Peter Zijlstra             2015-07-03 18:53:58 +0200 2993) EXPORT_SYMBOL_GPL(preempt_notifier_dec);
2ecd9d29abb17 kernel/sched/core.c (Peter Zijlstra             2015-07-03 18:53:58 +0200 2994) 
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 2995) /**
80dd99b368cf6 kernel/sched.c      (Luis Henriques             2009-03-16 19:58:09 +0000 2996)  * preempt_notifier_register - tell me when current is being preempted & rescheduled
421cee2935870 kernel/sched.c      (Randy Dunlap               2007-07-31 00:37:50 -0700 2997)  * @notifier: notifier struct to register
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 2998)  */
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 2999) void preempt_notifier_register(struct preempt_notifier *notifier)
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3000) {
b720342849fe6 kernel/sched/core.c (Davidlohr Bueso            2018-03-26 14:09:26 -0700 3001) 	if (!static_branch_unlikely(&preempt_notifier_key))
2ecd9d29abb17 kernel/sched/core.c (Peter Zijlstra             2015-07-03 18:53:58 +0200 3002) 		WARN(1, "registering preempt_notifier while notifiers disabled\n");
2ecd9d29abb17 kernel/sched/core.c (Peter Zijlstra             2015-07-03 18:53:58 +0200 3003) 
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3004) 	hlist_add_head(&notifier->link, &current->preempt_notifiers);
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3005) }
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3006) EXPORT_SYMBOL_GPL(preempt_notifier_register);
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3007) 
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3008) /**
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3009)  * preempt_notifier_unregister - no longer interested in preemption notifications
421cee2935870 kernel/sched.c      (Randy Dunlap               2007-07-31 00:37:50 -0700 3010)  * @notifier: notifier struct to unregister
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3011)  *
d84525a845cc2 kernel/sched/core.c (Mathieu Desnoyers          2015-05-17 12:53:10 -0400 3012)  * This is *not* safe to call from within a preemption notifier.
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3013)  */
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3014) void preempt_notifier_unregister(struct preempt_notifier *notifier)
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3015) {
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3016) 	hlist_del(&notifier->link);
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3017) }
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3018) EXPORT_SYMBOL_GPL(preempt_notifier_unregister);
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3019) 
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3020) static void __fire_sched_in_preempt_notifiers(struct task_struct *curr)
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3021) {
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3022) 	struct preempt_notifier *notifier;
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3023) 
b67bfe0d42cac kernel/sched/core.c (Sasha Levin                2013-02-27 17:06:00 -0800 3024) 	hlist_for_each_entry(notifier, &curr->preempt_notifiers, link)
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3025) 		notifier->ops->sched_in(notifier, raw_smp_processor_id());
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3026) }
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3027) 
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3028) static __always_inline void fire_sched_in_preempt_notifiers(struct task_struct *curr)
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3029) {
b720342849fe6 kernel/sched/core.c (Davidlohr Bueso            2018-03-26 14:09:26 -0700 3030) 	if (static_branch_unlikely(&preempt_notifier_key))
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3031) 		__fire_sched_in_preempt_notifiers(curr);
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3032) }
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3033) 
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3034) static void
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3035) __fire_sched_out_preempt_notifiers(struct task_struct *curr,
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3036) 				   struct task_struct *next)
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3037) {
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3038) 	struct preempt_notifier *notifier;
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3039) 
b67bfe0d42cac kernel/sched/core.c (Sasha Levin                2013-02-27 17:06:00 -0800 3040) 	hlist_for_each_entry(notifier, &curr->preempt_notifiers, link)
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3041) 		notifier->ops->sched_out(notifier, next);
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3042) }
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3043) 
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3044) static __always_inline void
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3045) fire_sched_out_preempt_notifiers(struct task_struct *curr,
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3046) 				 struct task_struct *next)
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3047) {
b720342849fe6 kernel/sched/core.c (Davidlohr Bueso            2018-03-26 14:09:26 -0700 3048) 	if (static_branch_unlikely(&preempt_notifier_key))
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3049) 		__fire_sched_out_preempt_notifiers(curr, next);
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3050) }
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3051) 
6d6bc0ad867c4 kernel/sched.c      (Dhaval Giani               2008-05-30 14:23:45 +0200 3052) #else /* !CONFIG_PREEMPT_NOTIFIERS */
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3053) 
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3054) static inline void fire_sched_in_preempt_notifiers(struct task_struct *curr)
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3055) {
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3056) }
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3057) 
1cde2930e1547 kernel/sched/core.c (Peter Zijlstra             2015-06-08 16:00:30 +0200 3058) static inline void
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3059) fire_sched_out_preempt_notifiers(struct task_struct *curr,
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3060) 				 struct task_struct *next)
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3061) {
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3062) }
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3063) 
6d6bc0ad867c4 kernel/sched.c      (Dhaval Giani               2008-05-30 14:23:45 +0200 3064) #endif /* CONFIG_PREEMPT_NOTIFIERS */
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3065) 
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3066) static inline void prepare_task(struct task_struct *next)
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3067) {
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3068) #ifdef CONFIG_SMP
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3069) 	/*
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3070) 	 * Claim the task as running, we do this before switching to it
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3071) 	 * such that any running task will have this set.
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3072) 	 */
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3073) 	next->on_cpu = 1;
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3074) #endif
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3075) }
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3076) 
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3077) static inline void finish_task(struct task_struct *prev)
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3078) {
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3079) #ifdef CONFIG_SMP
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3080) 	/*
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3081) 	 * After ->on_cpu is cleared, the task can be moved to a different CPU.
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3082) 	 * We must ensure this doesn't happen until the switch is completely
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3083) 	 * finished.
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3084) 	 *
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3085) 	 * In particular, the load of prev->state in finish_task_switch() must
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3086) 	 * happen before this.
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3087) 	 *
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3088) 	 * Pairs with the smp_cond_load_acquire() in try_to_wake_up().
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3089) 	 */
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3090) 	smp_store_release(&prev->on_cpu, 0);
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3091) #endif
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3092) }
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3093) 
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3094) static inline void
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3095) prepare_lock_switch(struct rq *rq, struct task_struct *next, struct rq_flags *rf)
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3096) {
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3097) 	/*
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3098) 	 * Since the runqueue lock will be released by the next
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3099) 	 * task (which is an invalid locking op but in the case
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3100) 	 * of the scheduler it's an obvious special-case), so we
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3101) 	 * do an early lockdep release here:
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3102) 	 */
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3103) 	rq_unpin_lock(rq, rf);
5facae4f3549b kernel/sched/core.c (Qian Cai                   2019-09-19 12:09:40 -0400 3104) 	spin_release(&rq->lock.dep_map, _THIS_IP_);
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3105) #ifdef CONFIG_DEBUG_SPINLOCK
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3106) 	/* this is a valid case when another task releases the spinlock */
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3107) 	rq->lock.owner = next;
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3108) #endif
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3109) }
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3110) 
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3111) static inline void finish_lock_switch(struct rq *rq)
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3112) {
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3113) 	/*
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3114) 	 * If we are tracking spinlock dependencies then we have to
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3115) 	 * fix up the runqueue lock - which gets 'carried over' from
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3116) 	 * prev into current:
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3117) 	 */
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3118) 	spin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3119) 	raw_spin_unlock_irq(&rq->lock);
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3120) }
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3121) 
325ea10c08094 kernel/sched/core.c (Ingo Molnar                2018-03-03 12:20:47 +0100 3122) /*
325ea10c08094 kernel/sched/core.c (Ingo Molnar                2018-03-03 12:20:47 +0100 3123)  * NOP if the arch has not defined these:
325ea10c08094 kernel/sched/core.c (Ingo Molnar                2018-03-03 12:20:47 +0100 3124)  */
325ea10c08094 kernel/sched/core.c (Ingo Molnar                2018-03-03 12:20:47 +0100 3125) 
325ea10c08094 kernel/sched/core.c (Ingo Molnar                2018-03-03 12:20:47 +0100 3126) #ifndef prepare_arch_switch
325ea10c08094 kernel/sched/core.c (Ingo Molnar                2018-03-03 12:20:47 +0100 3127) # define prepare_arch_switch(next)	do { } while (0)
325ea10c08094 kernel/sched/core.c (Ingo Molnar                2018-03-03 12:20:47 +0100 3128) #endif
325ea10c08094 kernel/sched/core.c (Ingo Molnar                2018-03-03 12:20:47 +0100 3129) 
325ea10c08094 kernel/sched/core.c (Ingo Molnar                2018-03-03 12:20:47 +0100 3130) #ifndef finish_arch_post_lock_switch
325ea10c08094 kernel/sched/core.c (Ingo Molnar                2018-03-03 12:20:47 +0100 3131) # define finish_arch_post_lock_switch()	do { } while (0)
325ea10c08094 kernel/sched/core.c (Ingo Molnar                2018-03-03 12:20:47 +0100 3132) #endif
325ea10c08094 kernel/sched/core.c (Ingo Molnar                2018-03-03 12:20:47 +0100 3133) 
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3134) /**
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3135)  * prepare_task_switch - prepare to switch tasks
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3136)  * @rq: the runqueue preparing to switch
421cee2935870 kernel/sched.c      (Randy Dunlap               2007-07-31 00:37:50 -0700 3137)  * @prev: the current task that is being switched out
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3138)  * @next: the task we are going to switch to.
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3139)  *
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3140)  * This is called with the rq lock held and interrupts off. It must
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3141)  * be paired with a subsequent finish_task_switch after the context
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3142)  * switch.
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3143)  *
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3144)  * prepare_task_switch sets up locking and calls architecture specific
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3145)  * hooks.
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3146)  */
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3147) static inline void
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3148) prepare_task_switch(struct rq *rq, struct task_struct *prev,
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3149) 		    struct task_struct *next)
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3150) {
0ed557aa81392 kernel/sched/core.c (Mark Rutland               2018-06-14 15:27:41 -0700 3151) 	kcov_prepare_switch(prev);
4314895165623 kernel/sched/core.c (Michael S. Tsirkin         2013-09-22 17:20:54 +0300 3152) 	sched_info_switch(rq, prev, next);
fe4b04fa31a6d kernel/sched.c      (Peter Zijlstra             2011-02-02 13:19:09 +0100 3153) 	perf_event_task_sched_out(prev, next);
d7822b1e24f2d kernel/sched/core.c (Mathieu Desnoyers          2018-06-02 08:43:54 -0400 3154) 	rseq_preempt(prev);
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3155) 	fire_sched_out_preempt_notifiers(prev, next);
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3156) 	prepare_task(next);
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3157) 	prepare_arch_switch(next);
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3158) }
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3159) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3160) /**
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3161)  * finish_task_switch - clean up after a task-switch
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3162)  * @prev: the thread we just switched away from.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3163)  *
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3164)  * finish_task_switch must be called after the context switch, paired
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3165)  * with a prepare_task_switch call before the context switch.
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3166)  * finish_task_switch will reconcile locking set up by prepare_task_switch,
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 3167)  * and do any other architecture-specific cleanup actions.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3168)  *
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3169)  * Note that we may have delayed dropping an mm in context_switch(). If
41a2d6cfa3f77 kernel/sched.c      (Ingo Molnar                2007-12-05 15:46:09 +0100 3170)  * so, we finish that here outside of the runqueue lock. (Doing it
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3171)  * with the lock held can cause deadlocks; see schedule() for
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3172)  * details.)
dfa50b605c2a9 kernel/sched/core.c (Oleg Nesterov              2014-10-09 21:32:32 +0200 3173)  *
dfa50b605c2a9 kernel/sched/core.c (Oleg Nesterov              2014-10-09 21:32:32 +0200 3174)  * The context switch have flipped the stack from under us and restored the
dfa50b605c2a9 kernel/sched/core.c (Oleg Nesterov              2014-10-09 21:32:32 +0200 3175)  * local variables which were saved when this task called schedule() in the
dfa50b605c2a9 kernel/sched/core.c (Oleg Nesterov              2014-10-09 21:32:32 +0200 3176)  * past. prev == current is still correct but we need to recalculate this_rq
dfa50b605c2a9 kernel/sched/core.c (Oleg Nesterov              2014-10-09 21:32:32 +0200 3177)  * because prev may have moved to another CPU.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3178)  */
dfa50b605c2a9 kernel/sched/core.c (Oleg Nesterov              2014-10-09 21:32:32 +0200 3179) static struct rq *finish_task_switch(struct task_struct *prev)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3180) 	__releases(rq->lock)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3181) {
dfa50b605c2a9 kernel/sched/core.c (Oleg Nesterov              2014-10-09 21:32:32 +0200 3182) 	struct rq *rq = this_rq();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3183) 	struct mm_struct *mm = rq->prev_mm;
55a101f8f71a3 kernel/sched.c      (Oleg Nesterov              2006-09-29 02:01:10 -0700 3184) 	long prev_state;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3185) 
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3186) 	/*
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3187) 	 * The previous task will have left us with a preempt_count of 2
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3188) 	 * because it left us after:
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3189) 	 *
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3190) 	 *	schedule()
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3191) 	 *	  preempt_disable();			// 1
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3192) 	 *	  __schedule()
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3193) 	 *	    raw_spin_lock_irq(&rq->lock)	// 2
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3194) 	 *
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3195) 	 * Also, see FORK_PREEMPT_COUNT.
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3196) 	 */
e2bf1c4b17aff kernel/sched/core.c (Peter Zijlstra             2015-09-29 12:18:46 +0200 3197) 	if (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET,
e2bf1c4b17aff kernel/sched/core.c (Peter Zijlstra             2015-09-29 12:18:46 +0200 3198) 		      "corrupted preempt_count: %s/%d/0x%x\n",
e2bf1c4b17aff kernel/sched/core.c (Peter Zijlstra             2015-09-29 12:18:46 +0200 3199) 		      current->comm, current->pid, preempt_count()))
e2bf1c4b17aff kernel/sched/core.c (Peter Zijlstra             2015-09-29 12:18:46 +0200 3200) 		preempt_count_set(FORK_PREEMPT_COUNT);
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3201) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3202) 	rq->prev_mm = NULL;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3203) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3204) 	/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3205) 	 * A task struct has one reference for the use as "current".
c394cc9fbb367 kernel/sched.c      (Oleg Nesterov              2006-09-29 02:01:11 -0700 3206) 	 * If a task dies, then it sets TASK_DEAD in tsk->state and calls
55a101f8f71a3 kernel/sched.c      (Oleg Nesterov              2006-09-29 02:01:10 -0700 3207) 	 * schedule one last time. The schedule call will never return, and
55a101f8f71a3 kernel/sched.c      (Oleg Nesterov              2006-09-29 02:01:10 -0700 3208) 	 * the scheduled task must drop that reference.
95913d97914f4 kernel/sched/core.c (Peter Zijlstra             2015-09-29 14:45:09 +0200 3209) 	 *
95913d97914f4 kernel/sched/core.c (Peter Zijlstra             2015-09-29 14:45:09 +0200 3210) 	 * We must observe prev->state before clearing prev->on_cpu (in
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3211) 	 * finish_task), otherwise a concurrent wakeup can get prev
95913d97914f4 kernel/sched/core.c (Peter Zijlstra             2015-09-29 14:45:09 +0200 3212) 	 * running on another CPU and we could rave with its RUNNING -> DEAD
95913d97914f4 kernel/sched/core.c (Peter Zijlstra             2015-09-29 14:45:09 +0200 3213) 	 * transition, resulting in a double drop.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3214) 	 */
55a101f8f71a3 kernel/sched.c      (Oleg Nesterov              2006-09-29 02:01:10 -0700 3215) 	prev_state = prev->state;
bf9fae9f5e4ca kernel/sched/core.c (Frederic Weisbecker        2012-09-08 15:23:11 +0200 3216) 	vtime_task_switch(prev);
a8d757ef076f0 kernel/sched.c      (Stephane Eranian           2011-08-25 15:58:03 +0200 3217) 	perf_event_task_sched_in(prev, current);
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3218) 	finish_task(prev);
31cb1bc0dc948 kernel/sched/core.c (rodrigosiqueira            2017-12-15 12:06:03 -0200 3219) 	finish_lock_switch(rq);
01f23e1630d94 kernel/sched/core.c (Catalin Marinas            2011-11-27 21:43:10 +0000 3220) 	finish_arch_post_lock_switch();
0ed557aa81392 kernel/sched/core.c (Mark Rutland               2018-06-14 15:27:41 -0700 3221) 	kcov_finish_switch(current);
e8fa136262e11 kernel/sched.c      (Steven Rostedt             2008-01-25 21:08:05 +0100 3222) 
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3223) 	fire_sched_in_preempt_notifiers(current);
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 3224) 	/*
70216e18e519a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:17 -0500 3225) 	 * When switching through a kernel thread, the loop in
70216e18e519a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:17 -0500 3226) 	 * membarrier_{private,global}_expedited() may have observed that
70216e18e519a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:17 -0500 3227) 	 * kernel thread and not issued an IPI. It is therefore possible to
70216e18e519a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:17 -0500 3228) 	 * schedule between user->kernel->user threads without passing though
70216e18e519a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:17 -0500 3229) 	 * switch_mm(). Membarrier requires a barrier after storing to
70216e18e519a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:17 -0500 3230) 	 * rq->curr, before returning to userspace, so provide them here:
70216e18e519a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:17 -0500 3231) 	 *
70216e18e519a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:17 -0500 3232) 	 * - a full memory barrier for {PRIVATE,GLOBAL}_EXPEDITED, implicitly
70216e18e519a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:17 -0500 3233) 	 *   provided by mmdrop(),
70216e18e519a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:17 -0500 3234) 	 * - a sync_core for SYNC_CORE.
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 3235) 	 */
70216e18e519a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:17 -0500 3236) 	if (mm) {
70216e18e519a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:17 -0500 3237) 		membarrier_mm_sync_core_before_usermode(mm);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3238) 		mmdrop(mm);
70216e18e519a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:17 -0500 3239) 	}
1cef1150ef40e kernel/sched/core.c (Peter Zijlstra             2018-06-07 11:45:49 +0200 3240) 	if (unlikely(prev_state == TASK_DEAD)) {
1cef1150ef40e kernel/sched/core.c (Peter Zijlstra             2018-06-07 11:45:49 +0200 3241) 		if (prev->sched_class->task_dead)
1cef1150ef40e kernel/sched/core.c (Peter Zijlstra             2018-06-07 11:45:49 +0200 3242) 			prev->sched_class->task_dead(prev);
68f24b08ee892 kernel/sched/core.c (Andy Lutomirski            2016-09-15 22:45:48 -0700 3243) 
1cef1150ef40e kernel/sched/core.c (Peter Zijlstra             2018-06-07 11:45:49 +0200 3244) 		/*
1cef1150ef40e kernel/sched/core.c (Peter Zijlstra             2018-06-07 11:45:49 +0200 3245) 		 * Remove function-return probe instances associated with this
1cef1150ef40e kernel/sched/core.c (Peter Zijlstra             2018-06-07 11:45:49 +0200 3246) 		 * task and put them back on the free list.
1cef1150ef40e kernel/sched/core.c (Peter Zijlstra             2018-06-07 11:45:49 +0200 3247) 		 */
1cef1150ef40e kernel/sched/core.c (Peter Zijlstra             2018-06-07 11:45:49 +0200 3248) 		kprobe_flush_task(prev);
1cef1150ef40e kernel/sched/core.c (Peter Zijlstra             2018-06-07 11:45:49 +0200 3249) 
1cef1150ef40e kernel/sched/core.c (Peter Zijlstra             2018-06-07 11:45:49 +0200 3250) 		/* Task is done with its stack. */
1cef1150ef40e kernel/sched/core.c (Peter Zijlstra             2018-06-07 11:45:49 +0200 3251) 		put_task_stack(prev);
1cef1150ef40e kernel/sched/core.c (Peter Zijlstra             2018-06-07 11:45:49 +0200 3252) 
0ff7b2cfbae36 kernel/sched/core.c (Eric W. Biederman          2019-09-14 07:33:58 -0500 3253) 		put_task_struct_rcu_user(prev);
c6fd91f0bdcd2 kernel/sched.c      (bibo mao                   2006-03-26 01:38:20 -0800 3254) 	}
99e5ada9407cc kernel/sched/core.c (Frederic Weisbecker        2013-04-20 17:11:50 +0200 3255) 
de734f89b67c2 kernel/sched/core.c (Frederic Weisbecker        2015-06-11 18:07:12 +0200 3256) 	tick_nohz_task_switch();
dfa50b605c2a9 kernel/sched/core.c (Oleg Nesterov              2014-10-09 21:32:32 +0200 3257) 	return rq;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3258) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3259) 
3f029d3c6d620 kernel/sched.c      (Gregory Haskins            2009-07-29 11:08:47 -0400 3260) #ifdef CONFIG_SMP
3f029d3c6d620 kernel/sched.c      (Gregory Haskins            2009-07-29 11:08:47 -0400 3261) 
3f029d3c6d620 kernel/sched.c      (Gregory Haskins            2009-07-29 11:08:47 -0400 3262) /* rq->lock is NOT held, but preemption is disabled */
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3263) static void __balance_callback(struct rq *rq)
3f029d3c6d620 kernel/sched.c      (Gregory Haskins            2009-07-29 11:08:47 -0400 3264) {
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3265) 	struct callback_head *head, *next;
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3266) 	void (*func)(struct rq *rq);
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3267) 	unsigned long flags;
3f029d3c6d620 kernel/sched.c      (Gregory Haskins            2009-07-29 11:08:47 -0400 3268) 
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3269) 	raw_spin_lock_irqsave(&rq->lock, flags);
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3270) 	head = rq->balance_callback;
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3271) 	rq->balance_callback = NULL;
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3272) 	while (head) {
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3273) 		func = (void (*)(struct rq *))head->func;
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3274) 		next = head->next;
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3275) 		head->next = NULL;
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3276) 		head = next;
3f029d3c6d620 kernel/sched.c      (Gregory Haskins            2009-07-29 11:08:47 -0400 3277) 
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3278) 		func(rq);
3f029d3c6d620 kernel/sched.c      (Gregory Haskins            2009-07-29 11:08:47 -0400 3279) 	}
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3280) 	raw_spin_unlock_irqrestore(&rq->lock, flags);
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3281) }
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3282) 
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3283) static inline void balance_callback(struct rq *rq)
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3284) {
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3285) 	if (unlikely(rq->balance_callback))
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3286) 		__balance_callback(rq);
3f029d3c6d620 kernel/sched.c      (Gregory Haskins            2009-07-29 11:08:47 -0400 3287) }
3f029d3c6d620 kernel/sched.c      (Gregory Haskins            2009-07-29 11:08:47 -0400 3288) 
3f029d3c6d620 kernel/sched.c      (Gregory Haskins            2009-07-29 11:08:47 -0400 3289) #else
da19ab510343c kernel/sched.c      (Steven Rostedt             2009-07-29 00:21:22 -0400 3290) 
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3291) static inline void balance_callback(struct rq *rq)
3f029d3c6d620 kernel/sched.c      (Gregory Haskins            2009-07-29 11:08:47 -0400 3292) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3293) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3294) 
3f029d3c6d620 kernel/sched.c      (Gregory Haskins            2009-07-29 11:08:47 -0400 3295) #endif
3f029d3c6d620 kernel/sched.c      (Gregory Haskins            2009-07-29 11:08:47 -0400 3296) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3297) /**
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3298)  * schedule_tail - first thing a freshly forked thread must call.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3299)  * @prev: the thread we just switched away from.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3300)  */
722a9f9299ca7 kernel/sched/core.c (Andi Kleen                 2014-05-02 00:44:38 +0200 3301) asmlinkage __visible void schedule_tail(struct task_struct *prev)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3302) 	__releases(rq->lock)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3303) {
1a43a14a5bd9c kernel/sched/core.c (Oleg Nesterov              2014-10-08 21:36:44 +0200 3304) 	struct rq *rq;
da19ab510343c kernel/sched.c      (Steven Rostedt             2009-07-29 00:21:22 -0400 3305) 
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3306) 	/*
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3307) 	 * New tasks start with FORK_PREEMPT_COUNT, see there and
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3308) 	 * finish_task_switch() for details.
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3309) 	 *
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3310) 	 * finish_task_switch() will drop rq->lock() and lower preempt_count
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3311) 	 * and the preempt_enable() will end up enabling preemption (on
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3312) 	 * PREEMPT_COUNT kernels).
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3313) 	 */
609ca066386b2 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:52:18 +0200 3314) 
dfa50b605c2a9 kernel/sched/core.c (Oleg Nesterov              2014-10-09 21:32:32 +0200 3315) 	rq = finish_task_switch(prev);
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 3316) 	balance_callback(rq);
1a43a14a5bd9c kernel/sched/core.c (Oleg Nesterov              2014-10-08 21:36:44 +0200 3317) 	preempt_enable();
70b97a7f0b19c kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:42 -0700 3318) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3319) 	if (current->set_child_tid)
b488893a390ed kernel/sched.c      (Pavel Emelyanov            2007-10-18 23:40:14 -0700 3320) 		put_user(task_pid_vnr(current), current->set_child_tid);
088fe47ce9525 kernel/sched/core.c (Eric W. Biederman          2018-07-23 17:26:49 -0500 3321) 
088fe47ce9525 kernel/sched/core.c (Eric W. Biederman          2018-07-23 17:26:49 -0500 3322) 	calculate_sigpending();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3323) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3324) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3325) /*
dfa50b605c2a9 kernel/sched/core.c (Oleg Nesterov              2014-10-09 21:32:32 +0200 3326)  * context_switch - switch to the new MM and the new thread's register state.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3327)  */
049369487e206 kernel/sched/core.c (Josh Poimboeuf             2016-02-28 22:22:39 -0600 3328) static __always_inline struct rq *
70b97a7f0b19c kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:42 -0700 3329) context_switch(struct rq *rq, struct task_struct *prev,
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 3330) 	       struct task_struct *next, struct rq_flags *rf)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3331) {
e107be36efb2a kernel/sched.c      (Avi Kivity                 2007-07-26 13:40:43 +0200 3332) 	prepare_task_switch(rq, prev, next);
fe4b04fa31a6d kernel/sched.c      (Peter Zijlstra             2011-02-02 13:19:09 +0100 3333) 
9226d125d94c7 kernel/sched.c      (Zachary Amsden             2007-02-13 13:26:21 +0100 3334) 	/*
9226d125d94c7 kernel/sched.c      (Zachary Amsden             2007-02-13 13:26:21 +0100 3335) 	 * For paravirt, this is coupled with an exit in switch_to to
9226d125d94c7 kernel/sched.c      (Zachary Amsden             2007-02-13 13:26:21 +0100 3336) 	 * combine the page table reload and the switch backend into
9226d125d94c7 kernel/sched.c      (Zachary Amsden             2007-02-13 13:26:21 +0100 3337) 	 * one hypercall.
9226d125d94c7 kernel/sched.c      (Zachary Amsden             2007-02-13 13:26:21 +0100 3338) 	 */
224101ed69d3f kernel/sched.c      (Jeremy Fitzhardinge        2009-02-18 11:18:57 -0800 3339) 	arch_start_context_switch(prev);
9226d125d94c7 kernel/sched.c      (Zachary Amsden             2007-02-13 13:26:21 +0100 3340) 
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 3341) 	/*
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3342) 	 * kernel -> kernel   lazy + transfer active
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3343) 	 *   user -> kernel   lazy + mmgrab() active
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3344) 	 *
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3345) 	 * kernel ->   user   switch + mmdrop() active
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3346) 	 *   user ->   user   switch
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 3347) 	 */
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3348) 	if (!next->mm) {                                // to kernel
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3349) 		enter_lazy_tlb(prev->active_mm, next);
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3350) 
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3351) 		next->active_mm = prev->active_mm;
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3352) 		if (prev->mm)                           // from user
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3353) 			mmgrab(prev->active_mm);
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3354) 		else
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3355) 			prev->active_mm = NULL;
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3356) 	} else {                                        // to user
227a4aadc75ba kernel/sched/core.c (Mathieu Desnoyers          2019-09-19 13:37:02 -0400 3357) 		membarrier_switch_mm(rq, prev->active_mm, next->mm);
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3358) 		/*
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3359) 		 * sys_membarrier() requires an smp_mb() between setting
227a4aadc75ba kernel/sched/core.c (Mathieu Desnoyers          2019-09-19 13:37:02 -0400 3360) 		 * rq->curr / membarrier_switch_mm() and returning to userspace.
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3361) 		 *
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3362) 		 * The below provides this either through switch_mm(), or in
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3363) 		 * case 'prev->active_mm == next->mm' through
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3364) 		 * finish_task_switch()'s mmdrop().
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3365) 		 */
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3366) 		switch_mm_irqs_off(prev->active_mm, next->mm, next);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3367) 
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3368) 		if (!prev->mm) {                        // from kernel
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3369) 			/* will mmdrop() in finish_task_switch(). */
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3370) 			rq->prev_mm = prev->active_mm;
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3371) 			prev->active_mm = NULL;
139d025cda1da kernel/sched/core.c (Peter Zijlstra             2019-07-29 16:05:15 +0200 3372) 		}
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3373) 	}
92509b732baf1 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:11 +0100 3374) 
cb42c9a3ebbbb kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:13 +0100 3375) 	rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
92509b732baf1 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:11 +0100 3376) 
269d599271fa6 kernel/sched/core.c (Peter Zijlstra             2018-02-06 17:52:13 +0100 3377) 	prepare_lock_switch(rq, next, rf);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3378) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3379) 	/* Here we just switch the register state and the stack. */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3380) 	switch_to(prev, next, prev);
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3381) 	barrier();
dfa50b605c2a9 kernel/sched/core.c (Oleg Nesterov              2014-10-09 21:32:32 +0200 3382) 
dfa50b605c2a9 kernel/sched/core.c (Oleg Nesterov              2014-10-09 21:32:32 +0200 3383) 	return finish_task_switch(prev);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3384) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3385) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3386) /*
1c3e826482ab6 kernel/sched/core.c (Sha Zhengju                2013-02-20 17:14:38 +0800 3387)  * nr_running and nr_context_switches:
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3388)  *
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3389)  * externally visible scheduler statistics: current number of runnable
1c3e826482ab6 kernel/sched/core.c (Sha Zhengju                2013-02-20 17:14:38 +0800 3390)  * threads, total number of context switches performed since bootup.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3391)  */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3392) unsigned long nr_running(void)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3393) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3394) 	unsigned long i, sum = 0;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3395) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3396) 	for_each_online_cpu(i)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3397) 		sum += cpu_rq(i)->nr_running;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3398) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3399) 	return sum;
f711f6090a81c kernel/sched.c      (Gautham R Shenoy           2009-04-14 10:25:30 +0530 3400) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3401) 
2ee507c472939 kernel/sched/core.c (Tim Chen                   2014-07-31 10:29:48 -0700 3402) /*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 3403)  * Check if only the current task is running on the CPU.
00cc1633816de kernel/sched/core.c (Dominik Dingel             2015-09-18 11:27:45 +0200 3404)  *
00cc1633816de kernel/sched/core.c (Dominik Dingel             2015-09-18 11:27:45 +0200 3405)  * Caution: this function does not check that the caller has disabled
00cc1633816de kernel/sched/core.c (Dominik Dingel             2015-09-18 11:27:45 +0200 3406)  * preemption, thus the result might have a time-of-check-to-time-of-use
00cc1633816de kernel/sched/core.c (Dominik Dingel             2015-09-18 11:27:45 +0200 3407)  * race.  The caller is responsible to use it correctly, for example:
00cc1633816de kernel/sched/core.c (Dominik Dingel             2015-09-18 11:27:45 +0200 3408)  *
dfcb245e28481 kernel/sched/core.c (Ingo Molnar                2018-12-03 10:05:56 +0100 3409)  * - from a non-preemptible section (of course)
00cc1633816de kernel/sched/core.c (Dominik Dingel             2015-09-18 11:27:45 +0200 3410)  *
00cc1633816de kernel/sched/core.c (Dominik Dingel             2015-09-18 11:27:45 +0200 3411)  * - from a thread that is bound to a single CPU
00cc1633816de kernel/sched/core.c (Dominik Dingel             2015-09-18 11:27:45 +0200 3412)  *
00cc1633816de kernel/sched/core.c (Dominik Dingel             2015-09-18 11:27:45 +0200 3413)  * - in a loop with very short iterations (e.g. a polling loop)
2ee507c472939 kernel/sched/core.c (Tim Chen                   2014-07-31 10:29:48 -0700 3414)  */
2ee507c472939 kernel/sched/core.c (Tim Chen                   2014-07-31 10:29:48 -0700 3415) bool single_task_running(void)
2ee507c472939 kernel/sched/core.c (Tim Chen                   2014-07-31 10:29:48 -0700 3416) {
00cc1633816de kernel/sched/core.c (Dominik Dingel             2015-09-18 11:27:45 +0200 3417) 	return raw_rq()->nr_running == 1;
2ee507c472939 kernel/sched/core.c (Tim Chen                   2014-07-31 10:29:48 -0700 3418) }
2ee507c472939 kernel/sched/core.c (Tim Chen                   2014-07-31 10:29:48 -0700 3419) EXPORT_SYMBOL(single_task_running);
2ee507c472939 kernel/sched/core.c (Tim Chen                   2014-07-31 10:29:48 -0700 3420) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3421) unsigned long long nr_context_switches(void)
46cb4b7c88fa5 kernel/sched.c      (Siddha, Suresh B           2007-05-08 00:32:51 -0700 3422) {
cc94abfcbc9fe kernel/sched.c      (Steven Rostedt             2006-06-27 02:54:31 -0700 3423) 	int i;
cc94abfcbc9fe kernel/sched.c      (Steven Rostedt             2006-06-27 02:54:31 -0700 3424) 	unsigned long long sum = 0;
46cb4b7c88fa5 kernel/sched.c      (Siddha, Suresh B           2007-05-08 00:32:51 -0700 3425) 
0a945022778f1 kernel/sched.c      (KAMEZAWA Hiroyuki          2006-03-28 01:56:37 -0800 3426) 	for_each_possible_cpu(i)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3427) 		sum += cpu_rq(i)->nr_switches;
46cb4b7c88fa5 kernel/sched.c      (Siddha, Suresh B           2007-05-08 00:32:51 -0700 3428) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3429) 	return sum;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3430) }
483b4ee60edbe kernel/sched.c      (Suresh Siddha              2009-02-04 11:59:44 -0800 3431) 
145d952a29320 kernel/sched/core.c (Daniel Lezcano             2018-10-04 14:04:02 +0200 3432) /*
145d952a29320 kernel/sched/core.c (Daniel Lezcano             2018-10-04 14:04:02 +0200 3433)  * Consumers of these two interfaces, like for example the cpuidle menu
145d952a29320 kernel/sched/core.c (Daniel Lezcano             2018-10-04 14:04:02 +0200 3434)  * governor, are using nonsensical data. Preferring shallow idle state selection
145d952a29320 kernel/sched/core.c (Daniel Lezcano             2018-10-04 14:04:02 +0200 3435)  * for a CPU that has IO-wait which might not even end up running the task when
145d952a29320 kernel/sched/core.c (Daniel Lezcano             2018-10-04 14:04:02 +0200 3436)  * it does become runnable.
145d952a29320 kernel/sched/core.c (Daniel Lezcano             2018-10-04 14:04:02 +0200 3437)  */
145d952a29320 kernel/sched/core.c (Daniel Lezcano             2018-10-04 14:04:02 +0200 3438) 
145d952a29320 kernel/sched/core.c (Daniel Lezcano             2018-10-04 14:04:02 +0200 3439) unsigned long nr_iowait_cpu(int cpu)
145d952a29320 kernel/sched/core.c (Daniel Lezcano             2018-10-04 14:04:02 +0200 3440) {
145d952a29320 kernel/sched/core.c (Daniel Lezcano             2018-10-04 14:04:02 +0200 3441) 	return atomic_read(&cpu_rq(cpu)->nr_iowait);
145d952a29320 kernel/sched/core.c (Daniel Lezcano             2018-10-04 14:04:02 +0200 3442) }
145d952a29320 kernel/sched/core.c (Daniel Lezcano             2018-10-04 14:04:02 +0200 3443) 
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3444) /*
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3445)  * IO-wait accounting, and how its mostly bollocks (on SMP).
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3446)  *
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3447)  * The idea behind IO-wait account is to account the idle time that we could
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3448)  * have spend running if it were not for IO. That is, if we were to improve the
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3449)  * storage performance, we'd have a proportional reduction in IO-wait time.
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3450)  *
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3451)  * This all works nicely on UP, where, when a task blocks on IO, we account
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3452)  * idle time as IO-wait, because if the storage were faster, it could've been
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3453)  * running and we'd not be idle.
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3454)  *
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3455)  * This has been extended to SMP, by doing the same for each CPU. This however
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3456)  * is broken.
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3457)  *
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3458)  * Imagine for instance the case where two tasks block on one CPU, only the one
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3459)  * CPU will have IO-wait accounted, while the other has regular idle. Even
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3460)  * though, if the storage were faster, both could've ran at the same time,
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3461)  * utilising both CPUs.
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3462)  *
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3463)  * This means, that when looking globally, the current IO-wait accounting on
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3464)  * SMP is a lower bound, by reason of under accounting.
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3465)  *
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3466)  * Worse, since the numbers are provided per CPU, they are sometimes
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3467)  * interpreted per CPU, and that is nonsensical. A blocked task isn't strictly
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3468)  * associated with any one particular CPU, it can wake to another CPU than it
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3469)  * blocked on. This means the per CPU IO-wait number is meaningless.
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3470)  *
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3471)  * Task CPU affinities can make all that even more 'interesting'.
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3472)  */
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 3473) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3474) unsigned long nr_iowait(void)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3475) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3476) 	unsigned long i, sum = 0;
483b4ee60edbe kernel/sched.c      (Suresh Siddha              2009-02-04 11:59:44 -0800 3477) 
0a945022778f1 kernel/sched.c      (KAMEZAWA Hiroyuki          2006-03-28 01:56:37 -0800 3478) 	for_each_possible_cpu(i)
145d952a29320 kernel/sched/core.c (Daniel Lezcano             2018-10-04 14:04:02 +0200 3479) 		sum += nr_iowait_cpu(i);
46cb4b7c88fa5 kernel/sched.c      (Siddha, Suresh B           2007-05-08 00:32:51 -0700 3480) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3481) 	return sum;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3482) }
483b4ee60edbe kernel/sched.c      (Suresh Siddha              2009-02-04 11:59:44 -0800 3483) 
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3484) #ifdef CONFIG_SMP
8a0be9ef82256 kernel/sched.c      (Frederic Weisbecker        2009-03-05 01:27:02 +0100 3485) 
46cb4b7c88fa5 kernel/sched.c      (Siddha, Suresh B           2007-05-08 00:32:51 -0700 3486) /*
3802290628348 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:37 +0100 3487)  * sched_exec - execve() is a valuable balancing opportunity, because at
3802290628348 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:37 +0100 3488)  * this point the task has the smallest effective memory and cache footprint.
46cb4b7c88fa5 kernel/sched.c      (Siddha, Suresh B           2007-05-08 00:32:51 -0700 3489)  */
3802290628348 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:37 +0100 3490) void sched_exec(void)
46cb4b7c88fa5 kernel/sched.c      (Siddha, Suresh B           2007-05-08 00:32:51 -0700 3491) {
3802290628348 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:37 +0100 3492) 	struct task_struct *p = current;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3493) 	unsigned long flags;
0017d73509284 kernel/sched.c      (Peter Zijlstra             2010-03-24 18:34:10 +0100 3494) 	int dest_cpu;
46cb4b7c88fa5 kernel/sched.c      (Siddha, Suresh B           2007-05-08 00:32:51 -0700 3495) 
8f42ced974df7 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:53 +0200 3496) 	raw_spin_lock_irqsave(&p->pi_lock, flags);
ac66f5477239e kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:16 +0100 3497) 	dest_cpu = p->sched_class->select_task_rq(p, task_cpu(p), SD_BALANCE_EXEC, 0);
0017d73509284 kernel/sched.c      (Peter Zijlstra             2010-03-24 18:34:10 +0100 3498) 	if (dest_cpu == smp_processor_id())
0017d73509284 kernel/sched.c      (Peter Zijlstra             2010-03-24 18:34:10 +0100 3499) 		goto unlock;
3802290628348 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:37 +0100 3500) 
8f42ced974df7 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:53 +0200 3501) 	if (likely(cpu_active(dest_cpu))) {
969c79215a35b kernel/sched.c      (Tejun Heo                  2010-05-06 18:49:21 +0200 3502) 		struct migration_arg arg = { p, dest_cpu };
46cb4b7c88fa5 kernel/sched.c      (Siddha, Suresh B           2007-05-08 00:32:51 -0700 3503) 
8f42ced974df7 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:53 +0200 3504) 		raw_spin_unlock_irqrestore(&p->pi_lock, flags);
8f42ced974df7 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:53 +0200 3505) 		stop_one_cpu(task_cpu(p), migration_cpu_stop, &arg);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3506) 		return;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3507) 	}
0017d73509284 kernel/sched.c      (Peter Zijlstra             2010-03-24 18:34:10 +0100 3508) unlock:
8f42ced974df7 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:53 +0200 3509) 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3510) }
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3511) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3512) #endif
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3513) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3514) DEFINE_PER_CPU(struct kernel_stat, kstat);
3292beb340c76 kernel/sched/core.c (Glauber Costa              2011-11-28 14:45:17 -0200 3515) DEFINE_PER_CPU(struct kernel_cpustat, kernel_cpustat);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3516) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3517) EXPORT_PER_CPU_SYMBOL(kstat);
3292beb340c76 kernel/sched/core.c (Glauber Costa              2011-11-28 14:45:17 -0200 3518) EXPORT_PER_CPU_SYMBOL(kernel_cpustat);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3519) 
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3520) /*
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3521)  * The function fair_sched_class.update_curr accesses the struct curr
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3522)  * and its field curr->exec_start; when called from task_sched_runtime(),
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3523)  * we observe a high rate of cache misses in practice.
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3524)  * Prefetching this data results in improved performance.
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3525)  */
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3526) static inline void prefetch_curr_exec_start(struct task_struct *p)
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3527) {
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3528) #ifdef CONFIG_FAIR_GROUP_SCHED
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3529) 	struct sched_entity *curr = (&p->se)->cfs_rq->curr;
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3530) #else
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3531) 	struct sched_entity *curr = (&task_rq(p)->cfs)->curr;
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3532) #endif
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3533) 	prefetch(curr);
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3534) 	prefetch(&curr->exec_start);
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3535) }
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3536) 
c5f8d99585d7b kernel/sched.c      (Hidetoshi Seto             2009-03-31 16:56:03 +0900 3537) /*
c5f8d99585d7b kernel/sched.c      (Hidetoshi Seto             2009-03-31 16:56:03 +0900 3538)  * Return accounted runtime for the task.
c5f8d99585d7b kernel/sched.c      (Hidetoshi Seto             2009-03-31 16:56:03 +0900 3539)  * In case the task is currently running, return the runtime plus current's
c5f8d99585d7b kernel/sched.c      (Hidetoshi Seto             2009-03-31 16:56:03 +0900 3540)  * pending runtime that have not been accounted yet.
c5f8d99585d7b kernel/sched.c      (Hidetoshi Seto             2009-03-31 16:56:03 +0900 3541)  */
c5f8d99585d7b kernel/sched.c      (Hidetoshi Seto             2009-03-31 16:56:03 +0900 3542) unsigned long long task_sched_runtime(struct task_struct *p)
c5f8d99585d7b kernel/sched.c      (Hidetoshi Seto             2009-03-31 16:56:03 +0900 3543) {
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 3544) 	struct rq_flags rf;
c5f8d99585d7b kernel/sched.c      (Hidetoshi Seto             2009-03-31 16:56:03 +0900 3545) 	struct rq *rq;
6e998916dfe32 kernel/sched/core.c (Stanislaw Gruszka          2014-11-12 16:58:44 +0100 3546) 	u64 ns;
c5f8d99585d7b kernel/sched.c      (Hidetoshi Seto             2009-03-31 16:56:03 +0900 3547) 
911b2898b3c9f kernel/sched/core.c (Peter Zijlstra             2013-11-11 18:21:56 +0100 3548) #if defined(CONFIG_64BIT) && defined(CONFIG_SMP)
911b2898b3c9f kernel/sched/core.c (Peter Zijlstra             2013-11-11 18:21:56 +0100 3549) 	/*
97fb7a0a8944b kernel/sched/core.c (Ingo Molnar                2018-03-03 14:01:12 +0100 3550) 	 * 64-bit doesn't need locks to atomically read a 64-bit value.
911b2898b3c9f kernel/sched/core.c (Peter Zijlstra             2013-11-11 18:21:56 +0100 3551) 	 * So we have a optimization chance when the task's delta_exec is 0.
911b2898b3c9f kernel/sched/core.c (Peter Zijlstra             2013-11-11 18:21:56 +0100 3552) 	 * Reading ->on_cpu is racy, but this is ok.
911b2898b3c9f kernel/sched/core.c (Peter Zijlstra             2013-11-11 18:21:56 +0100 3553) 	 *
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 3554) 	 * If we race with it leaving CPU, we'll take a lock. So we're correct.
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 3555) 	 * If we race with it entering CPU, unaccounted time is 0. This is
911b2898b3c9f kernel/sched/core.c (Peter Zijlstra             2013-11-11 18:21:56 +0100 3556) 	 * indistinguishable from the read occurring a few cycles earlier.
4036ac1567834 kernel/sched/core.c (Mike Galbraith             2014-06-24 07:49:40 +0200 3557) 	 * If we see ->on_cpu without ->on_rq, the task is leaving, and has
4036ac1567834 kernel/sched/core.c (Mike Galbraith             2014-06-24 07:49:40 +0200 3558) 	 * been accounted, so we're correct here as well.
911b2898b3c9f kernel/sched/core.c (Peter Zijlstra             2013-11-11 18:21:56 +0100 3559) 	 */
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 3560) 	if (!p->on_cpu || !task_on_rq_queued(p))
911b2898b3c9f kernel/sched/core.c (Peter Zijlstra             2013-11-11 18:21:56 +0100 3561) 		return p->se.sum_exec_runtime;
911b2898b3c9f kernel/sched/core.c (Peter Zijlstra             2013-11-11 18:21:56 +0100 3562) #endif
911b2898b3c9f kernel/sched/core.c (Peter Zijlstra             2013-11-11 18:21:56 +0100 3563) 
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 3564) 	rq = task_rq_lock(p, &rf);
6e998916dfe32 kernel/sched/core.c (Stanislaw Gruszka          2014-11-12 16:58:44 +0100 3565) 	/*
6e998916dfe32 kernel/sched/core.c (Stanislaw Gruszka          2014-11-12 16:58:44 +0100 3566) 	 * Must be ->curr _and_ ->on_rq.  If dequeued, we would
6e998916dfe32 kernel/sched/core.c (Stanislaw Gruszka          2014-11-12 16:58:44 +0100 3567) 	 * project cycles that may never be accounted to this
6e998916dfe32 kernel/sched/core.c (Stanislaw Gruszka          2014-11-12 16:58:44 +0100 3568) 	 * thread, breaking clock_gettime().
6e998916dfe32 kernel/sched/core.c (Stanislaw Gruszka          2014-11-12 16:58:44 +0100 3569) 	 */
6e998916dfe32 kernel/sched/core.c (Stanislaw Gruszka          2014-11-12 16:58:44 +0100 3570) 	if (task_current(rq, p) && task_on_rq_queued(p)) {
6075620b0590e kernel/sched/core.c (Giovanni Gherdovich        2016-08-05 10:21:56 +0200 3571) 		prefetch_curr_exec_start(p);
6e998916dfe32 kernel/sched/core.c (Stanislaw Gruszka          2014-11-12 16:58:44 +0100 3572) 		update_rq_clock(rq);
6e998916dfe32 kernel/sched/core.c (Stanislaw Gruszka          2014-11-12 16:58:44 +0100 3573) 		p->sched_class->update_curr(rq);
6e998916dfe32 kernel/sched/core.c (Stanislaw Gruszka          2014-11-12 16:58:44 +0100 3574) 	}
6e998916dfe32 kernel/sched/core.c (Stanislaw Gruszka          2014-11-12 16:58:44 +0100 3575) 	ns = p->se.sum_exec_runtime;
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 3576) 	task_rq_unlock(rq, p, &rf);
c5f8d99585d7b kernel/sched.c      (Hidetoshi Seto             2009-03-31 16:56:03 +0900 3577) 
c5f8d99585d7b kernel/sched.c      (Hidetoshi Seto             2009-03-31 16:56:03 +0900 3578) 	return ns;
c5f8d99585d7b kernel/sched.c      (Hidetoshi Seto             2009-03-31 16:56:03 +0900 3579) }
48f24c4da1ee7 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:40 -0700 3580) 
7835b98bc6de2 kernel/sched.c      (Christoph Lameter          2006-12-10 02:20:22 -0800 3581) /*
7835b98bc6de2 kernel/sched.c      (Christoph Lameter          2006-12-10 02:20:22 -0800 3582)  * This function gets called by the timer code, with HZ frequency.
7835b98bc6de2 kernel/sched.c      (Christoph Lameter          2006-12-10 02:20:22 -0800 3583)  * We call it with interrupts disabled.
7835b98bc6de2 kernel/sched.c      (Christoph Lameter          2006-12-10 02:20:22 -0800 3584)  */
7835b98bc6de2 kernel/sched.c      (Christoph Lameter          2006-12-10 02:20:22 -0800 3585) void scheduler_tick(void)
7835b98bc6de2 kernel/sched.c      (Christoph Lameter          2006-12-10 02:20:22 -0800 3586) {
7835b98bc6de2 kernel/sched.c      (Christoph Lameter          2006-12-10 02:20:22 -0800 3587) 	int cpu = smp_processor_id();
7835b98bc6de2 kernel/sched.c      (Christoph Lameter          2006-12-10 02:20:22 -0800 3588) 	struct rq *rq = cpu_rq(cpu);
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3589) 	struct task_struct *curr = rq->curr;
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 3590) 	struct rq_flags rf;
3e51f33fcc7f5 kernel/sched.c      (Peter Zijlstra             2008-05-03 18:29:28 +0200 3591) 
3e51f33fcc7f5 kernel/sched.c      (Peter Zijlstra             2008-05-03 18:29:28 +0200 3592) 	sched_clock_tick();
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3593) 
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 3594) 	rq_lock(rq, &rf);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 3595) 
3e51f33fcc7f5 kernel/sched.c      (Peter Zijlstra             2008-05-03 18:29:28 +0200 3596) 	update_rq_clock(rq);
fa85ae2418e68 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100 3597) 	curr->sched_class->task_tick(rq, curr, 0);
3289bdb429884 kernel/sched/core.c (Peter Zijlstra             2015-04-14 13:19:42 +0200 3598) 	calc_global_load_tick(rq);
eb414681d5a07 kernel/sched/core.c (Johannes Weiner            2018-10-26 15:06:27 -0700 3599) 	psi_task_tick(rq);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 3600) 
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 3601) 	rq_unlock(rq, &rf);
7835b98bc6de2 kernel/sched.c      (Christoph Lameter          2006-12-10 02:20:22 -0800 3602) 
e9d2b064149ff kernel/sched.c      (Peter Zijlstra             2010-09-17 11:28:50 +0200 3603) 	perf_event_task_tick();
e220d2dcb944c kernel/sched.c      (Peter Zijlstra             2009-05-23 18:28:55 +0200 3604) 
e418e1c2bf1a2 kernel/sched.c      (Christoph Lameter          2006-12-10 02:20:23 -0800 3605) #ifdef CONFIG_SMP
6eb57e0d65ebd kernel/sched.c      (Suresh Siddha              2011-10-03 15:09:01 -0700 3606) 	rq->idle_balance = idle_cpu(cpu);
7caff66f361c4 kernel/sched/core.c (Daniel Lezcano             2014-01-06 12:34:38 +0100 3607) 	trigger_load_balance(rq);
e418e1c2bf1a2 kernel/sched.c      (Christoph Lameter          2006-12-10 02:20:23 -0800 3608) #endif
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3609) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3610) 
265f22a975c1e kernel/sched/core.c (Frederic Weisbecker        2013-05-03 03:39:05 +0200 3611) #ifdef CONFIG_NO_HZ_FULL
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3612) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3613) struct tick_work {
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3614) 	int			cpu;
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3615) 	atomic_t		state;
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3616) 	struct delayed_work	work;
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3617) };
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3618) /* Values for ->state, see diagram below. */
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3619) #define TICK_SCHED_REMOTE_OFFLINE	0
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3620) #define TICK_SCHED_REMOTE_OFFLINING	1
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3621) #define TICK_SCHED_REMOTE_RUNNING	2
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3622) 
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3623) /*
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3624)  * State diagram for ->state:
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3625)  *
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3626)  *
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3627)  *          TICK_SCHED_REMOTE_OFFLINE
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3628)  *                    |   ^
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3629)  *                    |   |
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3630)  *                    |   | sched_tick_remote()
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3631)  *                    |   |
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3632)  *                    |   |
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3633)  *                    +--TICK_SCHED_REMOTE_OFFLINING
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3634)  *                    |   ^
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3635)  *                    |   |
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3636)  * sched_tick_start() |   | sched_tick_stop()
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3637)  *                    |   |
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3638)  *                    V   |
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3639)  *          TICK_SCHED_REMOTE_RUNNING
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3640)  *
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3641)  *
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3642)  * Other transitions get WARN_ON_ONCE(), except that sched_tick_remote()
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3643)  * and sched_tick_start() are happy to leave the state in RUNNING.
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3644)  */
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3645) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3646) static struct tick_work __percpu *tick_work_cpu;
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3647) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3648) static void sched_tick_remote(struct work_struct *work)
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3649) {
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3650) 	struct delayed_work *dwork = to_delayed_work(work);
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3651) 	struct tick_work *twork = container_of(dwork, struct tick_work, work);
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3652) 	int cpu = twork->cpu;
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3653) 	struct rq *rq = cpu_rq(cpu);
d9c0ffcabd6aa kernel/sched/core.c (Frederic Weisbecker        2018-06-28 18:29:41 +0200 3654) 	struct task_struct *curr;
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3655) 	struct rq_flags rf;
d9c0ffcabd6aa kernel/sched/core.c (Frederic Weisbecker        2018-06-28 18:29:41 +0200 3656) 	u64 delta;
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3657) 	int os;
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3658) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3659) 	/*
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3660) 	 * Handle the tick only if it appears the remote CPU is running in full
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3661) 	 * dynticks mode. The check is racy by nature, but missing a tick or
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3662) 	 * having one too much is no big deal because the scheduler tick updates
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3663) 	 * statistics and checks timeslices in a time-independent way, regardless
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3664) 	 * of when exactly it is running.
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3665) 	 */
488603b815a75 kernel/sched/core.c (Scott Wood                 2020-01-11 04:53:38 -0500 3666) 	if (!tick_nohz_tick_stopped_cpu(cpu))
d9c0ffcabd6aa kernel/sched/core.c (Frederic Weisbecker        2018-06-28 18:29:41 +0200 3667) 		goto out_requeue;
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3668) 
d9c0ffcabd6aa kernel/sched/core.c (Frederic Weisbecker        2018-06-28 18:29:41 +0200 3669) 	rq_lock_irq(rq, &rf);
d9c0ffcabd6aa kernel/sched/core.c (Frederic Weisbecker        2018-06-28 18:29:41 +0200 3670) 	curr = rq->curr;
488603b815a75 kernel/sched/core.c (Scott Wood                 2020-01-11 04:53:38 -0500 3671) 	if (cpu_is_offline(cpu))
d9c0ffcabd6aa kernel/sched/core.c (Frederic Weisbecker        2018-06-28 18:29:41 +0200 3672) 		goto out_unlock;
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3673) 
ebc0f83c78a2d kernel/sched/core.c (Peter Zijlstra (Intel)     2020-01-11 04:53:39 -0500 3674) 	curr = rq->curr;
d9c0ffcabd6aa kernel/sched/core.c (Frederic Weisbecker        2018-06-28 18:29:41 +0200 3675) 	update_rq_clock(rq);
d9c0ffcabd6aa kernel/sched/core.c (Frederic Weisbecker        2018-06-28 18:29:41 +0200 3676) 
488603b815a75 kernel/sched/core.c (Scott Wood                 2020-01-11 04:53:38 -0500 3677) 	if (!is_idle_task(curr)) {
488603b815a75 kernel/sched/core.c (Scott Wood                 2020-01-11 04:53:38 -0500 3678) 		/*
488603b815a75 kernel/sched/core.c (Scott Wood                 2020-01-11 04:53:38 -0500 3679) 		 * Make sure the next tick runs within a reasonable
488603b815a75 kernel/sched/core.c (Scott Wood                 2020-01-11 04:53:38 -0500 3680) 		 * amount of time.
488603b815a75 kernel/sched/core.c (Scott Wood                 2020-01-11 04:53:38 -0500 3681) 		 */
488603b815a75 kernel/sched/core.c (Scott Wood                 2020-01-11 04:53:38 -0500 3682) 		delta = rq_clock_task(rq) - curr->se.exec_start;
488603b815a75 kernel/sched/core.c (Scott Wood                 2020-01-11 04:53:38 -0500 3683) 		WARN_ON_ONCE(delta > (u64)NSEC_PER_SEC * 3);
488603b815a75 kernel/sched/core.c (Scott Wood                 2020-01-11 04:53:38 -0500 3684) 	}
d9c0ffcabd6aa kernel/sched/core.c (Frederic Weisbecker        2018-06-28 18:29:41 +0200 3685) 	curr->sched_class->task_tick(rq, curr, 0);
d9c0ffcabd6aa kernel/sched/core.c (Frederic Weisbecker        2018-06-28 18:29:41 +0200 3686) 
ebc0f83c78a2d kernel/sched/core.c (Peter Zijlstra (Intel)     2020-01-11 04:53:39 -0500 3687) 	calc_load_nohz_remote(rq);
d9c0ffcabd6aa kernel/sched/core.c (Frederic Weisbecker        2018-06-28 18:29:41 +0200 3688) out_unlock:
d9c0ffcabd6aa kernel/sched/core.c (Frederic Weisbecker        2018-06-28 18:29:41 +0200 3689) 	rq_unlock_irq(rq, &rf);
d9c0ffcabd6aa kernel/sched/core.c (Frederic Weisbecker        2018-06-28 18:29:41 +0200 3690) out_requeue:
ebc0f83c78a2d kernel/sched/core.c (Peter Zijlstra (Intel)     2020-01-11 04:53:39 -0500 3691) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3692) 	/*
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3693) 	 * Run the remote tick once per second (1Hz). This arbitrary
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3694) 	 * frequency is large enough to avoid overload but short enough
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3695) 	 * to keep scheduler internal stats reasonably up to date.  But
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3696) 	 * first update state to reflect hotplug activity if required.
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3697) 	 */
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3698) 	os = atomic_fetch_add_unless(&twork->state, -1, TICK_SCHED_REMOTE_RUNNING);
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3699) 	WARN_ON_ONCE(os == TICK_SCHED_REMOTE_OFFLINE);
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3700) 	if (os == TICK_SCHED_REMOTE_RUNNING)
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3701) 		queue_delayed_work(system_unbound_wq, dwork, HZ);
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3702) }
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3703) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3704) static void sched_tick_start(int cpu)
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3705) {
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3706) 	int os;
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3707) 	struct tick_work *twork;
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3708) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3709) 	if (housekeeping_cpu(cpu, HK_FLAG_TICK))
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3710) 		return;
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3711) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3712) 	WARN_ON_ONCE(!tick_work_cpu);
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3713) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3714) 	twork = per_cpu_ptr(tick_work_cpu, cpu);
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3715) 	os = atomic_xchg(&twork->state, TICK_SCHED_REMOTE_RUNNING);
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3716) 	WARN_ON_ONCE(os == TICK_SCHED_REMOTE_RUNNING);
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3717) 	if (os == TICK_SCHED_REMOTE_OFFLINE) {
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3718) 		twork->cpu = cpu;
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3719) 		INIT_DELAYED_WORK(&twork->work, sched_tick_remote);
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3720) 		queue_delayed_work(system_unbound_wq, &twork->work, HZ);
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3721) 	}
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3722) }
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3723) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3724) #ifdef CONFIG_HOTPLUG_CPU
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3725) static void sched_tick_stop(int cpu)
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3726) {
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3727) 	struct tick_work *twork;
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3728) 	int os;
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3729) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3730) 	if (housekeeping_cpu(cpu, HK_FLAG_TICK))
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3731) 		return;
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3732) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3733) 	WARN_ON_ONCE(!tick_work_cpu);
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3734) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3735) 	twork = per_cpu_ptr(tick_work_cpu, cpu);
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3736) 	/* There cannot be competing actions, but don't rely on stop-machine. */
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3737) 	os = atomic_xchg(&twork->state, TICK_SCHED_REMOTE_OFFLINING);
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3738) 	WARN_ON_ONCE(os != TICK_SCHED_REMOTE_RUNNING);
b55bd585551ed kernel/sched/core.c (Paul E. McKenney           2019-05-30 05:39:25 -0700 3739) 	/* Don't cancel, as this would mess up the state machine. */
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3740) }
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3741) #endif /* CONFIG_HOTPLUG_CPU */
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3742) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3743) int __init sched_tick_offload_init(void)
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3744) {
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3745) 	tick_work_cpu = alloc_percpu(struct tick_work);
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3746) 	BUG_ON(!tick_work_cpu);
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3747) 	return 0;
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3748) }
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3749) 
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3750) #else /* !CONFIG_NO_HZ_FULL */
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3751) static inline void sched_tick_start(int cpu) { }
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 3752) static inline void sched_tick_stop(int cpu) { }
265f22a975c1e kernel/sched/core.c (Frederic Weisbecker        2013-05-03 03:39:05 +0200 3753) #endif
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3754) 
c1a280b68d4e6 kernel/sched/core.c (Thomas Gleixner            2019-07-26 23:19:37 +0200 3755) #if defined(CONFIG_PREEMPTION) && (defined(CONFIG_DEBUG_PREEMPT) || \
c3bc8fd637a96 kernel/sched/core.c (Joel Fernandes (Google)    2018-07-30 15:24:23 -0700 3756) 				defined(CONFIG_TRACE_PREEMPT_TOGGLE))
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3757) /*
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3758)  * If the value passed in is equal to the current preempt count
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3759)  * then we just disabled preemption. Start timing the latency.
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3760)  */
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3761) static inline void preempt_latency_start(int val)
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3762) {
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3763) 	if (preempt_count() == val) {
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3764) 		unsigned long ip = get_lock_parent_ip();
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3765) #ifdef CONFIG_DEBUG_PREEMPT
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3766) 		current->preempt_disable_ip = ip;
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3767) #endif
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3768) 		trace_preempt_off(CALLER_ADDR0, ip);
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3769) 	}
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3770) }
7e49fcce1bdad kernel/sched.c      (Steven Rostedt             2009-01-22 19:01:40 -0500 3771) 
edafe3a56dbd4 kernel/sched/core.c (Masami Hiramatsu           2014-04-17 17:18:42 +0900 3772) void preempt_count_add(int val)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3773) {
6cd8a4bb2f975 kernel/sched.c      (Steven Rostedt             2008-05-12 21:20:42 +0200 3774) #ifdef CONFIG_DEBUG_PREEMPT
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3775) 	/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3776) 	 * Underflow?
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3777) 	 */
9a11b49a80566 kernel/sched.c      (Ingo Molnar                2006-07-03 00:24:33 -0700 3778) 	if (DEBUG_LOCKS_WARN_ON((preempt_count() < 0)))
9a11b49a80566 kernel/sched.c      (Ingo Molnar                2006-07-03 00:24:33 -0700 3779) 		return;
6cd8a4bb2f975 kernel/sched.c      (Steven Rostedt             2008-05-12 21:20:42 +0200 3780) #endif
bdb4380658909 kernel/sched/core.c (Peter Zijlstra             2013-09-10 12:15:23 +0200 3781) 	__preempt_count_add(val);
6cd8a4bb2f975 kernel/sched.c      (Steven Rostedt             2008-05-12 21:20:42 +0200 3782) #ifdef CONFIG_DEBUG_PREEMPT
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3783) 	/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3784) 	 * Spinlock count overflowing soon?
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3785) 	 */
33859f7f9788d kernel/sched.c      (Miguel Ojeda Sandonis      2006-12-10 02:20:38 -0800 3786) 	DEBUG_LOCKS_WARN_ON((preempt_count() & PREEMPT_MASK) >=
33859f7f9788d kernel/sched.c      (Miguel Ojeda Sandonis      2006-12-10 02:20:38 -0800 3787) 				PREEMPT_MASK - 10);
6cd8a4bb2f975 kernel/sched.c      (Steven Rostedt             2008-05-12 21:20:42 +0200 3788) #endif
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3789) 	preempt_latency_start(val);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3790) }
bdb4380658909 kernel/sched/core.c (Peter Zijlstra             2013-09-10 12:15:23 +0200 3791) EXPORT_SYMBOL(preempt_count_add);
edafe3a56dbd4 kernel/sched/core.c (Masami Hiramatsu           2014-04-17 17:18:42 +0900 3792) NOKPROBE_SYMBOL(preempt_count_add);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3793) 
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3794) /*
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3795)  * If the value passed in equals to the current preempt count
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3796)  * then we just enabled preemption. Stop timing the latency.
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3797)  */
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3798) static inline void preempt_latency_stop(int val)
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3799) {
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3800) 	if (preempt_count() == val)
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3801) 		trace_preempt_on(CALLER_ADDR0, get_lock_parent_ip());
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3802) }
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3803) 
edafe3a56dbd4 kernel/sched/core.c (Masami Hiramatsu           2014-04-17 17:18:42 +0900 3804) void preempt_count_sub(int val)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3805) {
6cd8a4bb2f975 kernel/sched.c      (Steven Rostedt             2008-05-12 21:20:42 +0200 3806) #ifdef CONFIG_DEBUG_PREEMPT
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3807) 	/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3808) 	 * Underflow?
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3809) 	 */
01e3eb82278bf kernel/sched.c      (Ingo Molnar                2009-01-12 13:00:50 +0100 3810) 	if (DEBUG_LOCKS_WARN_ON(val > preempt_count()))
9a11b49a80566 kernel/sched.c      (Ingo Molnar                2006-07-03 00:24:33 -0700 3811) 		return;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3812) 	/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3813) 	 * Is the spinlock portion underflowing?
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3814) 	 */
9a11b49a80566 kernel/sched.c      (Ingo Molnar                2006-07-03 00:24:33 -0700 3815) 	if (DEBUG_LOCKS_WARN_ON((val < PREEMPT_MASK) &&
9a11b49a80566 kernel/sched.c      (Ingo Molnar                2006-07-03 00:24:33 -0700 3816) 			!(preempt_count() & PREEMPT_MASK)))
9a11b49a80566 kernel/sched.c      (Ingo Molnar                2006-07-03 00:24:33 -0700 3817) 		return;
6cd8a4bb2f975 kernel/sched.c      (Steven Rostedt             2008-05-12 21:20:42 +0200 3818) #endif
9a11b49a80566 kernel/sched.c      (Ingo Molnar                2006-07-03 00:24:33 -0700 3819) 
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3820) 	preempt_latency_stop(val);
bdb4380658909 kernel/sched/core.c (Peter Zijlstra             2013-09-10 12:15:23 +0200 3821) 	__preempt_count_sub(val);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3822) }
bdb4380658909 kernel/sched/core.c (Peter Zijlstra             2013-09-10 12:15:23 +0200 3823) EXPORT_SYMBOL(preempt_count_sub);
edafe3a56dbd4 kernel/sched/core.c (Masami Hiramatsu           2014-04-17 17:18:42 +0900 3824) NOKPROBE_SYMBOL(preempt_count_sub);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3825) 
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3826) #else
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3827) static inline void preempt_latency_start(int val) { }
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 3828) static inline void preempt_latency_stop(int val) { }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3829) #endif
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3830) 
59ddbcb2f45b9 kernel/sched/core.c (Ingo Molnar                2017-02-03 23:37:48 +0100 3831) static inline unsigned long get_preempt_disable_ip(struct task_struct *p)
59ddbcb2f45b9 kernel/sched/core.c (Ingo Molnar                2017-02-03 23:37:48 +0100 3832) {
59ddbcb2f45b9 kernel/sched/core.c (Ingo Molnar                2017-02-03 23:37:48 +0100 3833) #ifdef CONFIG_DEBUG_PREEMPT
59ddbcb2f45b9 kernel/sched/core.c (Ingo Molnar                2017-02-03 23:37:48 +0100 3834) 	return p->preempt_disable_ip;
59ddbcb2f45b9 kernel/sched/core.c (Ingo Molnar                2017-02-03 23:37:48 +0100 3835) #else
59ddbcb2f45b9 kernel/sched/core.c (Ingo Molnar                2017-02-03 23:37:48 +0100 3836) 	return 0;
59ddbcb2f45b9 kernel/sched/core.c (Ingo Molnar                2017-02-03 23:37:48 +0100 3837) #endif
59ddbcb2f45b9 kernel/sched/core.c (Ingo Molnar                2017-02-03 23:37:48 +0100 3838) }
59ddbcb2f45b9 kernel/sched/core.c (Ingo Molnar                2017-02-03 23:37:48 +0100 3839) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3840) /*
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3841)  * Print scheduling while atomic bug:
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3842)  */
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3843) static noinline void __schedule_bug(struct task_struct *prev)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3844) {
d1c6d149cf04d kernel/sched/core.c (Vegard Nossum              2016-07-23 09:46:39 +0200 3845) 	/* Save this before calling printk(), since that will clobber it */
d1c6d149cf04d kernel/sched/core.c (Vegard Nossum              2016-07-23 09:46:39 +0200 3846) 	unsigned long preempt_disable_ip = get_preempt_disable_ip(current);
d1c6d149cf04d kernel/sched/core.c (Vegard Nossum              2016-07-23 09:46:39 +0200 3847) 
664dfa65e8442 kernel/sched/core.c (Dave Jones                 2011-12-22 16:39:30 -0500 3848) 	if (oops_in_progress)
664dfa65e8442 kernel/sched/core.c (Dave Jones                 2011-12-22 16:39:30 -0500 3849) 		return;
664dfa65e8442 kernel/sched/core.c (Dave Jones                 2011-12-22 16:39:30 -0500 3850) 
3df0fc5b2e9d8 kernel/sched.c      (Peter Zijlstra             2009-12-20 14:23:57 +0100 3851) 	printk(KERN_ERR "BUG: scheduling while atomic: %s/%d/0x%08x\n",
3df0fc5b2e9d8 kernel/sched.c      (Peter Zijlstra             2009-12-20 14:23:57 +0100 3852) 		prev->comm, prev->pid, preempt_count());
838225b48edc9 kernel/sched.c      (Satyam Sharma              2007-10-24 18:23:50 +0200 3853) 
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3854) 	debug_show_held_locks(prev);
e21f5b153b9b4 kernel/sched.c      (Arjan van de Ven           2008-05-23 09:05:58 -0700 3855) 	print_modules();
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3856) 	if (irqs_disabled())
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3857) 		print_irqtrace_events(prev);
d1c6d149cf04d kernel/sched/core.c (Vegard Nossum              2016-07-23 09:46:39 +0200 3858) 	if (IS_ENABLED(CONFIG_DEBUG_PREEMPT)
d1c6d149cf04d kernel/sched/core.c (Vegard Nossum              2016-07-23 09:46:39 +0200 3859) 	    && in_atomic_preempt_off()) {
8f47b1871b8aa kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:39 +0100 3860) 		pr_err("Preemption disabled at:");
d1c6d149cf04d kernel/sched/core.c (Vegard Nossum              2016-07-23 09:46:39 +0200 3861) 		print_ip_sym(preempt_disable_ip);
8f47b1871b8aa kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:39 +0100 3862) 		pr_cont("\n");
8f47b1871b8aa kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:39 +0100 3863) 	}
748c7201e622d kernel/sched/core.c (Daniel Bristot de Oliveira 2016-06-03 17:10:18 -0300 3864) 	if (panic_on_warn)
748c7201e622d kernel/sched/core.c (Daniel Bristot de Oliveira 2016-06-03 17:10:18 -0300 3865) 		panic("scheduling while atomic\n");
748c7201e622d kernel/sched/core.c (Daniel Bristot de Oliveira 2016-06-03 17:10:18 -0300 3866) 
6135fc1eb4b1c kernel/sched/core.c (Stephen Boyd               2012-03-28 17:10:47 -0700 3867) 	dump_stack();
373d4d099761c kernel/sched/core.c (Rusty Russell              2013-01-21 17:17:39 +1030 3868) 	add_taint(TAINT_WARN, LOCKDEP_STILL_OK);
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3869) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3870) 
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3871) /*
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3872)  * Various schedule()-time debugging checks and statistics:
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3873)  */
312364f3534cc kernel/sched/core.c (Daniel Vetter              2019-08-26 22:14:23 +0200 3874) static inline void schedule_debug(struct task_struct *prev, bool preempt)
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3875) {
0d9e26329b0c9 kernel/sched/core.c (Aaron Tomlin               2014-09-12 14:16:19 +0100 3876) #ifdef CONFIG_SCHED_STACK_END_CHECK
29d6455178a09 kernel/sched/core.c (Jann Horn                  2016-06-01 11:55:07 +0200 3877) 	if (task_stack_end_corrupted(prev))
29d6455178a09 kernel/sched/core.c (Jann Horn                  2016-06-01 11:55:07 +0200 3878) 		panic("corrupted stack end detected inside scheduler\n");
0d9e26329b0c9 kernel/sched/core.c (Aaron Tomlin               2014-09-12 14:16:19 +0100 3879) #endif
b99def8b96144 kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:02:03 +0200 3880) 
312364f3534cc kernel/sched/core.c (Daniel Vetter              2019-08-26 22:14:23 +0200 3881) #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
312364f3534cc kernel/sched/core.c (Daniel Vetter              2019-08-26 22:14:23 +0200 3882) 	if (!preempt && prev->state && prev->non_block_count) {
312364f3534cc kernel/sched/core.c (Daniel Vetter              2019-08-26 22:14:23 +0200 3883) 		printk(KERN_ERR "BUG: scheduling in a non-blocking section: %s/%d/%i\n",
312364f3534cc kernel/sched/core.c (Daniel Vetter              2019-08-26 22:14:23 +0200 3884) 			prev->comm, prev->pid, prev->non_block_count);
312364f3534cc kernel/sched/core.c (Daniel Vetter              2019-08-26 22:14:23 +0200 3885) 		dump_stack();
312364f3534cc kernel/sched/core.c (Daniel Vetter              2019-08-26 22:14:23 +0200 3886) 		add_taint(TAINT_WARN, LOCKDEP_STILL_OK);
312364f3534cc kernel/sched/core.c (Daniel Vetter              2019-08-26 22:14:23 +0200 3887) 	}
312364f3534cc kernel/sched/core.c (Daniel Vetter              2019-08-26 22:14:23 +0200 3888) #endif
312364f3534cc kernel/sched/core.c (Daniel Vetter              2019-08-26 22:14:23 +0200 3889) 
1dc0fffc48af9 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:57:39 +0200 3890) 	if (unlikely(in_atomic_preempt_off())) {
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3891) 		__schedule_bug(prev);
1dc0fffc48af9 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:57:39 +0200 3892) 		preempt_count_set(PREEMPT_DISABLED);
1dc0fffc48af9 kernel/sched/core.c (Peter Zijlstra             2015-09-28 17:57:39 +0200 3893) 	}
b3fbab0571eb0 kernel/sched.c      (Paul E. McKenney           2011-05-24 08:31:09 -0700 3894) 	rcu_sleep_check();
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3895) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3896) 	profile_hit(SCHED_PROFILING, __builtin_return_address(0));
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3897) 
ae92882e5646d kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:24 -0500 3898) 	schedstat_inc(this_rq()->sched_count);
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3899) }
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3900) 
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3901) /*
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3902)  * Pick up the highest-prio task:
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3903)  */
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3904) static inline struct task_struct *
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 3905) pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3906) {
49ee576809d83 kernel/sched/core.c (Peter Zijlstra             2017-01-19 18:44:08 +0100 3907) 	const struct sched_class *class;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3908) 	struct task_struct *p;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3909) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3910) 	/*
0ba87bb27d66b kernel/sched/core.c (Peter Zijlstra             2017-03-01 10:51:47 +0100 3911) 	 * Optimization: we know that if all tasks are in the fair class we can
0ba87bb27d66b kernel/sched/core.c (Peter Zijlstra             2017-03-01 10:51:47 +0100 3912) 	 * call that function directly, but only if the @prev task wasn't of a
0ba87bb27d66b kernel/sched/core.c (Peter Zijlstra             2017-03-01 10:51:47 +0100 3913) 	 * higher scheduling class, because otherwise those loose the
0ba87bb27d66b kernel/sched/core.c (Peter Zijlstra             2017-03-01 10:51:47 +0100 3914) 	 * opportunity to pull in more work from other CPUs.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3915) 	 */
0ba87bb27d66b kernel/sched/core.c (Peter Zijlstra             2017-03-01 10:51:47 +0100 3916) 	if (likely((prev->sched_class == &idle_sched_class ||
0ba87bb27d66b kernel/sched/core.c (Peter Zijlstra             2017-03-01 10:51:47 +0100 3917) 		    prev->sched_class == &fair_sched_class) &&
0ba87bb27d66b kernel/sched/core.c (Peter Zijlstra             2017-03-01 10:51:47 +0100 3918) 		   rq->nr_running == rq->cfs.h_nr_running)) {
0ba87bb27d66b kernel/sched/core.c (Peter Zijlstra             2017-03-01 10:51:47 +0100 3919) 
5d7d605642b28 kernel/sched/core.c (Peter Zijlstra             2019-11-08 14:15:57 +0100 3920) 		p = pick_next_task_fair(rq, prev, rf);
6ccdc84b81a0a kernel/sched/core.c (Peter Zijlstra             2014-04-24 12:00:47 +0200 3921) 		if (unlikely(p == RETRY_TASK))
67692435c411e kernel/sched/core.c (Peter Zijlstra             2019-05-29 20:36:44 +0000 3922) 			goto restart;
6ccdc84b81a0a kernel/sched/core.c (Peter Zijlstra             2014-04-24 12:00:47 +0200 3923) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 3924) 		/* Assumes fair_sched_class->next == idle_sched_class */
5d7d605642b28 kernel/sched/core.c (Peter Zijlstra             2019-11-08 14:15:57 +0100 3925) 		if (!p) {
f488e1057bb97 kernel/sched/core.c (Peter Zijlstra             2019-11-08 14:15:56 +0100 3926) 			put_prev_task(rq, prev);
98c2f700edb41 kernel/sched/core.c (Peter Zijlstra             2019-11-08 14:15:58 +0100 3927) 			p = pick_next_task_idle(rq);
f488e1057bb97 kernel/sched/core.c (Peter Zijlstra             2019-11-08 14:15:56 +0100 3928) 		}
6ccdc84b81a0a kernel/sched/core.c (Peter Zijlstra             2014-04-24 12:00:47 +0200 3929) 
6ccdc84b81a0a kernel/sched/core.c (Peter Zijlstra             2014-04-24 12:00:47 +0200 3930) 		return p;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3931) 	}
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3932) 
67692435c411e kernel/sched/core.c (Peter Zijlstra             2019-05-29 20:36:44 +0000 3933) restart:
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 3934) #ifdef CONFIG_SMP
67692435c411e kernel/sched/core.c (Peter Zijlstra             2019-05-29 20:36:44 +0000 3935) 	/*
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 3936) 	 * We must do the balancing pass before put_next_task(), such
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 3937) 	 * that when we release the rq->lock the task is in the same
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 3938) 	 * state as before we took rq->lock.
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 3939) 	 *
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 3940) 	 * We can terminate the balance pass as soon as we know there is
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 3941) 	 * a runnable task of @class priority or higher.
67692435c411e kernel/sched/core.c (Peter Zijlstra             2019-05-29 20:36:44 +0000 3942) 	 */
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 3943) 	for_class_range(class, prev->sched_class, &idle_sched_class) {
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 3944) 		if (class->balance(rq, prev, rf))
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 3945) 			break;
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 3946) 	}
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 3947) #endif
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 3948) 
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 3949) 	put_prev_task(rq, prev);
67692435c411e kernel/sched/core.c (Peter Zijlstra             2019-05-29 20:36:44 +0000 3950) 
34f971f6f7988 kernel/sched.c      (Peter Zijlstra             2010-09-22 13:53:15 +0200 3951) 	for_each_class(class) {
98c2f700edb41 kernel/sched/core.c (Peter Zijlstra             2019-11-08 14:15:58 +0100 3952) 		p = class->pick_next_task(rq);
67692435c411e kernel/sched/core.c (Peter Zijlstra             2019-05-29 20:36:44 +0000 3953) 		if (p)
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3954) 			return p;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3955) 	}
34f971f6f7988 kernel/sched.c      (Peter Zijlstra             2010-09-22 13:53:15 +0200 3956) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 3957) 	/* The idle class should always have a runnable task: */
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 3958) 	BUG();
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3959) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 3960) 
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3961) /*
c259e01a1ec90 kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:00 +0200 3962)  * __schedule() is the main scheduler function.
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3963)  *
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3964)  * The main means of driving the scheduler and thus entering this function are:
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3965)  *
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3966)  *   1. Explicit blocking: mutex, semaphore, waitqueue, etc.
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3967)  *
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3968)  *   2. TIF_NEED_RESCHED flag is checked on interrupt and userspace return
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3969)  *      paths. For example, see arch/x86/entry_64.S.
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3970)  *
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3971)  *      To drive preemption between tasks, the scheduler sets the flag in timer
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3972)  *      interrupt handler scheduler_tick().
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3973)  *
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3974)  *   3. Wakeups don't really cause entry into schedule(). They add a
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3975)  *      task to the run-queue and that's it.
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3976)  *
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3977)  *      Now, if the new task added to the run-queue preempts the current
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3978)  *      task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3979)  *      called on the nearest possible occasion:
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3980)  *
c1a280b68d4e6 kernel/sched/core.c (Thomas Gleixner            2019-07-26 23:19:37 +0200 3981)  *       - If the kernel is preemptible (CONFIG_PREEMPTION=y):
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3982)  *
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3983)  *         - in syscall or exception context, at the next outmost
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3984)  *           preempt_enable(). (this might be as soon as the wake_up()'s
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3985)  *           spin_unlock()!)
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3986)  *
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3987)  *         - in IRQ context, return from interrupt-handler to
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3988)  *           preemptible context
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3989)  *
c1a280b68d4e6 kernel/sched/core.c (Thomas Gleixner            2019-07-26 23:19:37 +0200 3990)  *       - If the kernel is not preemptible (CONFIG_PREEMPTION is not set)
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3991)  *         then at the next:
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3992)  *
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3993)  *          - cond_resched() call
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3994)  *          - explicit schedule() call
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3995)  *          - return from syscall or exception to user-space
edde96eafc91a kernel/sched/core.c (Pekka Enberg               2012-08-04 11:49:47 +0300 3996)  *          - return from interrupt-handler to user-space
bfd9b2b5f80e7 kernel/sched/core.c (Frederic Weisbecker        2015-01-28 01:24:09 +0100 3997)  *
b30f0e3ffedfa kernel/sched/core.c (Frederic Weisbecker        2015-05-12 16:41:49 +0200 3998)  * WARNING: must be called with preemption disabled!
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 3999)  */
499d79559ffe4 kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:52:36 +0200 4000) static void __sched notrace __schedule(bool preempt)
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4001) {
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4002) 	struct task_struct *prev, *next;
67ca7bde2e9d3 kernel/sched.c      (Harvey Harrison            2008-02-15 09:56:36 -0800 4003) 	unsigned long *switch_count;
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 4004) 	struct rq_flags rf;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4005) 	struct rq *rq;
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200 4006) 	int cpu;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4007) 
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4008) 	cpu = smp_processor_id();
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4009) 	rq = cpu_rq(cpu);
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4010) 	prev = rq->curr;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4011) 
312364f3534cc kernel/sched/core.c (Daniel Vetter              2019-08-26 22:14:23 +0200 4012) 	schedule_debug(prev, preempt);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4013) 
31656519e132f kernel/sched.c      (Peter Zijlstra             2008-07-18 18:01:23 +0200 4014) 	if (sched_feat(HRTICK))
f333fdc9098b7 kernel/sched.c      (Mike Galbraith             2008-05-12 21:20:55 +0200 4015) 		hrtick_clear(rq);
8f4d37ec073c1 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:29 +0100 4016) 
46a5d164db53b kernel/sched/core.c (Paul E. McKenney           2015-10-07 09:10:48 -0700 4017) 	local_irq_disable();
bcbfdd01dce55 kernel/sched/core.c (Paul E. McKenney           2017-04-11 15:50:41 -0700 4018) 	rcu_note_context_switch(preempt);
46a5d164db53b kernel/sched/core.c (Paul E. McKenney           2015-10-07 09:10:48 -0700 4019) 
e0acd0a68ec7d kernel/sched/core.c (Oleg Nesterov              2013-08-12 18:14:00 +0200 4020) 	/*
e0acd0a68ec7d kernel/sched/core.c (Oleg Nesterov              2013-08-12 18:14:00 +0200 4021) 	 * Make sure that signal_pending_state()->signal_pending() below
e0acd0a68ec7d kernel/sched/core.c (Oleg Nesterov              2013-08-12 18:14:00 +0200 4022) 	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)
e0acd0a68ec7d kernel/sched/core.c (Oleg Nesterov              2013-08-12 18:14:00 +0200 4023) 	 * done by the caller to avoid the race with signal_wake_up().
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 4024) 	 *
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 4025) 	 * The membarrier system call requires a full memory barrier
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 4026) 	 * after coming from user-space, before storing to rq->curr.
e0acd0a68ec7d kernel/sched/core.c (Oleg Nesterov              2013-08-12 18:14:00 +0200 4027) 	 */
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 4028) 	rq_lock(rq, &rf);
d89e588ca4081 kernel/sched/core.c (Peter Zijlstra             2016-09-05 11:37:53 +0200 4029) 	smp_mb__after_spinlock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4030) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4031) 	/* Promote REQ to ACT */
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4032) 	rq->clock_update_flags <<= 1;
bce4dc80c66ad kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:40:35 +0100 4033) 	update_rq_clock(rq);
9edfbfed3f544 kernel/sched/core.c (Peter Zijlstra             2015-01-05 11:18:11 +0100 4034) 
246d86b518450 kernel/sched.c      (Oleg Nesterov              2010-05-19 14:57:11 +0200 4035) 	switch_count = &prev->nivcsw;
fc13aebab7d8f kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:05:34 +0200 4036) 	if (!preempt && prev->state) {
34ec35ad8f5f4 kernel/sched/core.c (Davidlohr Bueso            2019-01-03 15:28:48 -0800 4037) 		if (signal_pending_state(prev->state, prev)) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4038) 			prev->state = TASK_RUNNING;
21aa9af03d06c kernel/sched.c      (Tejun Heo                  2010-06-08 21:40:37 +0200 4039) 		} else {
bce4dc80c66ad kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:40:35 +0100 4040) 			deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);
2acca55ed98ad kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:50 +0200 4041) 
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 4042) 			if (prev->in_iowait) {
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 4043) 				atomic_inc(&rq->nr_iowait);
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 4044) 				delayacct_blkio_start();
e33a9bba85a86 kernel/sched/core.c (Tejun Heo                  2016-12-07 15:48:41 -0500 4045) 			}
21aa9af03d06c kernel/sched.c      (Tejun Heo                  2010-06-08 21:40:37 +0200 4046) 		}
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4047) 		switch_count = &prev->nvcsw;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4048) 	}
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4049) 
d8ac897137a23 kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:10 +0100 4050) 	next = pick_next_task(rq, prev, &rf);
f26f9aff6aaf6 kernel/sched.c      (Mike Galbraith             2010-12-08 11:05:42 +0100 4051) 	clear_tsk_need_resched(prev);
f27dde8deef33 kernel/sched/core.c (Peter Zijlstra             2013-08-14 14:55:31 +0200 4052) 	clear_preempt_need_resched();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4053) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4054) 	if (likely(prev != next)) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4055) 		rq->nr_switches++;
5311a98fef7d0 kernel/sched/core.c (Eric W. Biederman          2019-09-14 07:35:02 -0500 4056) 		/*
5311a98fef7d0 kernel/sched/core.c (Eric W. Biederman          2019-09-14 07:35:02 -0500 4057) 		 * RCU users of rcu_dereference(rq->curr) may not see
5311a98fef7d0 kernel/sched/core.c (Eric W. Biederman          2019-09-14 07:35:02 -0500 4058) 		 * changes to task_struct made by pick_next_task().
5311a98fef7d0 kernel/sched/core.c (Eric W. Biederman          2019-09-14 07:35:02 -0500 4059) 		 */
5311a98fef7d0 kernel/sched/core.c (Eric W. Biederman          2019-09-14 07:35:02 -0500 4060) 		RCU_INIT_POINTER(rq->curr, next);
22e4ebb975822 kernel/sched/core.c (Mathieu Desnoyers          2017-07-28 16:40:40 -0400 4061) 		/*
22e4ebb975822 kernel/sched/core.c (Mathieu Desnoyers          2017-07-28 16:40:40 -0400 4062) 		 * The membarrier system call requires each architecture
22e4ebb975822 kernel/sched/core.c (Mathieu Desnoyers          2017-07-28 16:40:40 -0400 4063) 		 * to have a full memory barrier after updating
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 4064) 		 * rq->curr, before returning to user-space.
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 4065) 		 *
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 4066) 		 * Here are the schemes providing that barrier on the
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 4067) 		 * various architectures:
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 4068) 		 * - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC.
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 4069) 		 *   switch_mm() rely on membarrier_arch_switch_mm() on PowerPC.
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 4070) 		 * - finish_lock_switch() for weakly-ordered
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 4071) 		 *   architectures where spin_unlock is a full barrier,
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 4072) 		 * - switch_to() for arm64 (weakly-ordered, spin_unlock
306e060435d7a kernel/sched/core.c (Mathieu Desnoyers          2018-01-29 15:20:12 -0500 4073) 		 *   is a RELEASE barrier),
22e4ebb975822 kernel/sched/core.c (Mathieu Desnoyers          2017-07-28 16:40:40 -0400 4074) 		 */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4075) 		++*switch_count;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4076) 
c73464b1c8434 kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:06:56 +0200 4077) 		trace_sched_switch(preempt, prev, next);
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4078) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4079) 		/* Also unlocks the rq: */
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4080) 		rq = context_switch(rq, prev, next, &rf);
cbce1a6867005 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:54 +0200 4081) 	} else {
cb42c9a3ebbbb kernel/sched/core.c (Matt Fleming               2016-09-21 14:38:13 +0100 4082) 		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 4083) 		rq_unlock_irq(rq, &rf);
cbce1a6867005 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:54 +0200 4084) 	}
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4085) 
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 4086) 	balance_callback(rq);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4087) }
c259e01a1ec90 kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:00 +0200 4088) 
9af6528ee9b68 kernel/sched/core.c (Peter Zijlstra             2016-09-13 18:37:29 +0200 4089) void __noreturn do_task_dead(void)
9af6528ee9b68 kernel/sched/core.c (Peter Zijlstra             2016-09-13 18:37:29 +0200 4090) {
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4091) 	/* Causes final put_task_struct in finish_task_switch(): */
b5bf9a90bbebf kernel/sched/core.c (Peter Zijlstra             2018-04-30 14:51:01 +0200 4092) 	set_special_state(TASK_DEAD);
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4093) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4094) 	/* Tell freezer to ignore us: */
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4095) 	current->flags |= PF_NOFREEZE;
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4096) 
9af6528ee9b68 kernel/sched/core.c (Peter Zijlstra             2016-09-13 18:37:29 +0200 4097) 	__schedule(false);
9af6528ee9b68 kernel/sched/core.c (Peter Zijlstra             2016-09-13 18:37:29 +0200 4098) 	BUG();
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4099) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4100) 	/* Avoid "noreturn function does return" - but don't continue if BUG() is a NOP: */
9af6528ee9b68 kernel/sched/core.c (Peter Zijlstra             2016-09-13 18:37:29 +0200 4101) 	for (;;)
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4102) 		cpu_relax();
9af6528ee9b68 kernel/sched/core.c (Peter Zijlstra             2016-09-13 18:37:29 +0200 4103) }
9af6528ee9b68 kernel/sched/core.c (Peter Zijlstra             2016-09-13 18:37:29 +0200 4104) 
9c40cef2b799f kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:01 +0200 4105) static inline void sched_submit_work(struct task_struct *tsk)
9c40cef2b799f kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:01 +0200 4106) {
b0fdc01354f45 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-08-16 18:06:26 +0200 4107) 	if (!tsk->state)
9c40cef2b799f kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:01 +0200 4108) 		return;
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4109) 
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4110) 	/*
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4111) 	 * If a worker went to sleep, notify and ask workqueue whether
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4112) 	 * it wants to wake up a task to maintain concurrency.
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4113) 	 * As this function is called inside the schedule() context,
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4114) 	 * we disable preemption to avoid it calling schedule() again
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4115) 	 * in the possible wakeup of a kworker.
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4116) 	 */
771b53d033e86 kernel/sched/core.c (Jens Axboe                 2019-10-22 10:25:58 -0600 4117) 	if (tsk->flags & (PF_WQ_WORKER | PF_IO_WORKER)) {
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4118) 		preempt_disable();
771b53d033e86 kernel/sched/core.c (Jens Axboe                 2019-10-22 10:25:58 -0600 4119) 		if (tsk->flags & PF_WQ_WORKER)
771b53d033e86 kernel/sched/core.c (Jens Axboe                 2019-10-22 10:25:58 -0600 4120) 			wq_worker_sleeping(tsk);
771b53d033e86 kernel/sched/core.c (Jens Axboe                 2019-10-22 10:25:58 -0600 4121) 		else
771b53d033e86 kernel/sched/core.c (Jens Axboe                 2019-10-22 10:25:58 -0600 4122) 			io_wq_worker_sleeping(tsk);
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4123) 		preempt_enable_no_resched();
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4124) 	}
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4125) 
b0fdc01354f45 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-08-16 18:06:26 +0200 4126) 	if (tsk_is_pi_blocked(tsk))
b0fdc01354f45 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-08-16 18:06:26 +0200 4127) 		return;
b0fdc01354f45 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-08-16 18:06:26 +0200 4128) 
9c40cef2b799f kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:01 +0200 4129) 	/*
9c40cef2b799f kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:01 +0200 4130) 	 * If we are going to sleep and we have plugged IO queued,
9c40cef2b799f kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:01 +0200 4131) 	 * make sure to submit it to avoid deadlocks.
9c40cef2b799f kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:01 +0200 4132) 	 */
9c40cef2b799f kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:01 +0200 4133) 	if (blk_needs_flush_plug(tsk))
9c40cef2b799f kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:01 +0200 4134) 		blk_schedule_flush_plug(tsk);
9c40cef2b799f kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:01 +0200 4135) }
9c40cef2b799f kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:01 +0200 4136) 
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4137) static void sched_update_worker(struct task_struct *tsk)
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4138) {
771b53d033e86 kernel/sched/core.c (Jens Axboe                 2019-10-22 10:25:58 -0600 4139) 	if (tsk->flags & (PF_WQ_WORKER | PF_IO_WORKER)) {
771b53d033e86 kernel/sched/core.c (Jens Axboe                 2019-10-22 10:25:58 -0600 4140) 		if (tsk->flags & PF_WQ_WORKER)
771b53d033e86 kernel/sched/core.c (Jens Axboe                 2019-10-22 10:25:58 -0600 4141) 			wq_worker_running(tsk);
771b53d033e86 kernel/sched/core.c (Jens Axboe                 2019-10-22 10:25:58 -0600 4142) 		else
771b53d033e86 kernel/sched/core.c (Jens Axboe                 2019-10-22 10:25:58 -0600 4143) 			io_wq_worker_running(tsk);
771b53d033e86 kernel/sched/core.c (Jens Axboe                 2019-10-22 10:25:58 -0600 4144) 	}
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4145) }
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4146) 
722a9f9299ca7 kernel/sched/core.c (Andi Kleen                 2014-05-02 00:44:38 +0200 4147) asmlinkage __visible void __sched schedule(void)
c259e01a1ec90 kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:00 +0200 4148) {
9c40cef2b799f kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:01 +0200 4149) 	struct task_struct *tsk = current;
9c40cef2b799f kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:01 +0200 4150) 
9c40cef2b799f kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:01 +0200 4151) 	sched_submit_work(tsk);
bfd9b2b5f80e7 kernel/sched/core.c (Frederic Weisbecker        2015-01-28 01:24:09 +0100 4152) 	do {
b30f0e3ffedfa kernel/sched/core.c (Frederic Weisbecker        2015-05-12 16:41:49 +0200 4153) 		preempt_disable();
fc13aebab7d8f kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:05:34 +0200 4154) 		__schedule(false);
b30f0e3ffedfa kernel/sched/core.c (Frederic Weisbecker        2015-05-12 16:41:49 +0200 4155) 		sched_preempt_enable_no_resched();
bfd9b2b5f80e7 kernel/sched/core.c (Frederic Weisbecker        2015-01-28 01:24:09 +0100 4156) 	} while (need_resched());
6d25be5782e48 kernel/sched/core.c (Thomas Gleixner            2019-03-13 17:55:48 +0100 4157) 	sched_update_worker(tsk);
c259e01a1ec90 kernel/sched.c      (Thomas Gleixner            2011-06-22 19:47:00 +0200 4158) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4159) EXPORT_SYMBOL(schedule);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4160) 
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4161) /*
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4162)  * synchronize_rcu_tasks() makes sure that no task is stuck in preempted
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4163)  * state (have scheduled out non-voluntarily) by making sure that all
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4164)  * tasks have either left the run queue or have gone into user space.
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4165)  * As idle tasks do not do either, they must not ever be preempted
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4166)  * (schedule out non-voluntarily).
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4167)  *
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4168)  * schedule_idle() is similar to schedule_preempt_disable() except that it
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4169)  * never enables preemption because it does not call sched_submit_work().
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4170)  */
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4171) void __sched schedule_idle(void)
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4172) {
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4173) 	/*
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4174) 	 * As this skips calling sched_submit_work(), which the idle task does
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4175) 	 * regardless because that function is a nop when the task is in a
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4176) 	 * TASK_RUNNING state, make sure this isn't used someplace that the
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4177) 	 * current task can be in any other state. Note, idle is always in the
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4178) 	 * TASK_RUNNING state.
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4179) 	 */
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4180) 	WARN_ON_ONCE(current->state);
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4181) 	do {
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4182) 		__schedule(false);
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4183) 	} while (need_resched());
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4184) }
8663effb24f94 kernel/sched/core.c (Steven Rostedt (VMware)    2017-04-14 08:48:09 -0400 4185) 
91d1aa43d3050 kernel/sched/core.c (Frederic Weisbecker        2012-11-27 19:33:25 +0100 4186) #ifdef CONFIG_CONTEXT_TRACKING
722a9f9299ca7 kernel/sched/core.c (Andi Kleen                 2014-05-02 00:44:38 +0200 4187) asmlinkage __visible void __sched schedule_user(void)
20ab65e33f469 kernel/sched/core.c (Frederic Weisbecker        2012-07-11 20:26:37 +0200 4188) {
20ab65e33f469 kernel/sched/core.c (Frederic Weisbecker        2012-07-11 20:26:37 +0200 4189) 	/*
20ab65e33f469 kernel/sched/core.c (Frederic Weisbecker        2012-07-11 20:26:37 +0200 4190) 	 * If we come here after a random call to set_need_resched(),
20ab65e33f469 kernel/sched/core.c (Frederic Weisbecker        2012-07-11 20:26:37 +0200 4191) 	 * or we have been woken up remotely but the IPI has not yet arrived,
20ab65e33f469 kernel/sched/core.c (Frederic Weisbecker        2012-07-11 20:26:37 +0200 4192) 	 * we haven't yet exited the RCU idle mode. Do it here manually until
20ab65e33f469 kernel/sched/core.c (Frederic Weisbecker        2012-07-11 20:26:37 +0200 4193) 	 * we find a better solution.
7cc78f8fa02c2 kernel/sched/core.c (Andy Lutomirski            2014-12-03 15:37:08 -0800 4194) 	 *
7cc78f8fa02c2 kernel/sched/core.c (Andy Lutomirski            2014-12-03 15:37:08 -0800 4195) 	 * NB: There are buggy callers of this function.  Ideally we
c467ea763fd5d kernel/sched/core.c (Frederic Weisbecker        2015-03-04 18:06:33 +0100 4196) 	 * should warn if prev_state != CONTEXT_USER, but that will trigger
7cc78f8fa02c2 kernel/sched/core.c (Andy Lutomirski            2014-12-03 15:37:08 -0800 4197) 	 * too frequently to make sense yet.
20ab65e33f469 kernel/sched/core.c (Frederic Weisbecker        2012-07-11 20:26:37 +0200 4198) 	 */
7cc78f8fa02c2 kernel/sched/core.c (Andy Lutomirski            2014-12-03 15:37:08 -0800 4199) 	enum ctx_state prev_state = exception_enter();
20ab65e33f469 kernel/sched/core.c (Frederic Weisbecker        2012-07-11 20:26:37 +0200 4200) 	schedule();
7cc78f8fa02c2 kernel/sched/core.c (Andy Lutomirski            2014-12-03 15:37:08 -0800 4201) 	exception_exit(prev_state);
20ab65e33f469 kernel/sched/core.c (Frederic Weisbecker        2012-07-11 20:26:37 +0200 4202) }
20ab65e33f469 kernel/sched/core.c (Frederic Weisbecker        2012-07-11 20:26:37 +0200 4203) #endif
20ab65e33f469 kernel/sched/core.c (Frederic Weisbecker        2012-07-11 20:26:37 +0200 4204) 
c5491ea779793 kernel/sched/core.c (Thomas Gleixner            2011-03-21 12:09:35 +0100 4205) /**
c5491ea779793 kernel/sched/core.c (Thomas Gleixner            2011-03-21 12:09:35 +0100 4206)  * schedule_preempt_disabled - called with preemption disabled
c5491ea779793 kernel/sched/core.c (Thomas Gleixner            2011-03-21 12:09:35 +0100 4207)  *
c5491ea779793 kernel/sched/core.c (Thomas Gleixner            2011-03-21 12:09:35 +0100 4208)  * Returns with preemption disabled. Note: preempt_count must be 1
c5491ea779793 kernel/sched/core.c (Thomas Gleixner            2011-03-21 12:09:35 +0100 4209)  */
c5491ea779793 kernel/sched/core.c (Thomas Gleixner            2011-03-21 12:09:35 +0100 4210) void __sched schedule_preempt_disabled(void)
c5491ea779793 kernel/sched/core.c (Thomas Gleixner            2011-03-21 12:09:35 +0100 4211) {
ba74c1448f127 kernel/sched/core.c (Thomas Gleixner            2011-03-21 13:32:17 +0100 4212) 	sched_preempt_enable_no_resched();
c5491ea779793 kernel/sched/core.c (Thomas Gleixner            2011-03-21 12:09:35 +0100 4213) 	schedule();
c5491ea779793 kernel/sched/core.c (Thomas Gleixner            2011-03-21 12:09:35 +0100 4214) 	preempt_disable();
c5491ea779793 kernel/sched/core.c (Thomas Gleixner            2011-03-21 12:09:35 +0100 4215) }
c5491ea779793 kernel/sched/core.c (Thomas Gleixner            2011-03-21 12:09:35 +0100 4216) 
06b1f8083d6ed kernel/sched/core.c (Frederic Weisbecker        2015-02-16 19:20:07 +0100 4217) static void __sched notrace preempt_schedule_common(void)
a18b5d0181923 kernel/sched/core.c (Frederic Weisbecker        2015-01-22 18:08:04 +0100 4218) {
a18b5d0181923 kernel/sched/core.c (Frederic Weisbecker        2015-01-22 18:08:04 +0100 4219) 	do {
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4220) 		/*
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4221) 		 * Because the function tracer can trace preempt_count_sub()
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4222) 		 * and it also uses preempt_enable/disable_notrace(), if
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4223) 		 * NEED_RESCHED is set, the preempt_enable_notrace() called
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4224) 		 * by the function tracer will call this function again and
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4225) 		 * cause infinite recursion.
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4226) 		 *
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4227) 		 * Preemption must be disabled here before the function
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4228) 		 * tracer can trace. Break up preempt_disable() into two
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4229) 		 * calls. One to disable preemption without fear of being
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4230) 		 * traced. The other to still record the preemption latency,
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4231) 		 * which can also be traced by the function tracer.
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4232) 		 */
499d79559ffe4 kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:52:36 +0200 4233) 		preempt_disable_notrace();
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4234) 		preempt_latency_start(1);
fc13aebab7d8f kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:05:34 +0200 4235) 		__schedule(true);
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4236) 		preempt_latency_stop(1);
499d79559ffe4 kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:52:36 +0200 4237) 		preempt_enable_no_resched_notrace();
a18b5d0181923 kernel/sched/core.c (Frederic Weisbecker        2015-01-22 18:08:04 +0100 4238) 
a18b5d0181923 kernel/sched/core.c (Frederic Weisbecker        2015-01-22 18:08:04 +0100 4239) 		/*
a18b5d0181923 kernel/sched/core.c (Frederic Weisbecker        2015-01-22 18:08:04 +0100 4240) 		 * Check again in case we missed a preemption opportunity
a18b5d0181923 kernel/sched/core.c (Frederic Weisbecker        2015-01-22 18:08:04 +0100 4241) 		 * between schedule and now.
a18b5d0181923 kernel/sched/core.c (Frederic Weisbecker        2015-01-22 18:08:04 +0100 4242) 		 */
a18b5d0181923 kernel/sched/core.c (Frederic Weisbecker        2015-01-22 18:08:04 +0100 4243) 	} while (need_resched());
a18b5d0181923 kernel/sched/core.c (Frederic Weisbecker        2015-01-22 18:08:04 +0100 4244) }
a18b5d0181923 kernel/sched/core.c (Frederic Weisbecker        2015-01-22 18:08:04 +0100 4245) 
c1a280b68d4e6 kernel/sched/core.c (Thomas Gleixner            2019-07-26 23:19:37 +0200 4246) #ifdef CONFIG_PREEMPTION
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4247) /*
a49b4f4012ef2 kernel/sched/core.c (Valentin Schneider         2019-09-23 15:36:12 +0100 4248)  * This is the entry point to schedule() from in-kernel preemption
a49b4f4012ef2 kernel/sched/core.c (Valentin Schneider         2019-09-23 15:36:12 +0100 4249)  * off of preempt_enable.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4250)  */
722a9f9299ca7 kernel/sched/core.c (Andi Kleen                 2014-05-02 00:44:38 +0200 4251) asmlinkage __visible void __sched notrace preempt_schedule(void)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4252) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4253) 	/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4254) 	 * If there is a non-zero preempt_count or interrupts are disabled,
41a2d6cfa3f77 kernel/sched.c      (Ingo Molnar                2007-12-05 15:46:09 +0100 4255) 	 * we do not want to preempt the current task. Just return..
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4256) 	 */
fbb00b568bc93 kernel/sched/core.c (Frederic Weisbecker        2013-06-19 23:56:22 +0200 4257) 	if (likely(!preemptible()))
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4258) 		return;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4259) 
a18b5d0181923 kernel/sched/core.c (Frederic Weisbecker        2015-01-22 18:08:04 +0100 4260) 	preempt_schedule_common();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4261) }
376e242429bf8 kernel/sched/core.c (Masami Hiramatsu           2014-04-17 17:17:05 +0900 4262) NOKPROBE_SYMBOL(preempt_schedule);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4263) EXPORT_SYMBOL(preempt_schedule);
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4264) 
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4265) /**
4eaca0a887eae kernel/sched/core.c (Frederic Weisbecker        2015-06-04 17:39:08 +0200 4266)  * preempt_schedule_notrace - preempt_schedule called by tracing
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4267)  *
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4268)  * The tracing infrastructure uses preempt_enable_notrace to prevent
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4269)  * recursion and tracing preempt enabling caused by the tracing
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4270)  * infrastructure itself. But as tracing can happen in areas coming
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4271)  * from userspace or just about to enter userspace, a preempt enable
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4272)  * can occur before user_exit() is called. This will cause the scheduler
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4273)  * to be called when the system is still in usermode.
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4274)  *
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4275)  * To prevent this, the preempt_enable_notrace will use this function
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4276)  * instead of preempt_schedule() to exit user context if needed before
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4277)  * calling the scheduler.
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4278)  */
4eaca0a887eae kernel/sched/core.c (Frederic Weisbecker        2015-06-04 17:39:08 +0200 4279) asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4280) {
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4281) 	enum ctx_state prev_ctx;
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4282) 
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4283) 	if (likely(!preemptible()))
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4284) 		return;
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4285) 
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4286) 	do {
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4287) 		/*
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4288) 		 * Because the function tracer can trace preempt_count_sub()
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4289) 		 * and it also uses preempt_enable/disable_notrace(), if
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4290) 		 * NEED_RESCHED is set, the preempt_enable_notrace() called
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4291) 		 * by the function tracer will call this function again and
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4292) 		 * cause infinite recursion.
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4293) 		 *
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4294) 		 * Preemption must be disabled here before the function
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4295) 		 * tracer can trace. Break up preempt_disable() into two
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4296) 		 * calls. One to disable preemption without fear of being
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4297) 		 * traced. The other to still record the preemption latency,
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4298) 		 * which can also be traced by the function tracer.
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4299) 		 */
3d8f74dd4ca1d kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:09:19 +0200 4300) 		preempt_disable_notrace();
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4301) 		preempt_latency_start(1);
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4302) 		/*
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4303) 		 * Needs preempt disabled in case user_exit() is traced
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4304) 		 * and the tracer calls preempt_enable_notrace() causing
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4305) 		 * an infinite recursion.
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4306) 		 */
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4307) 		prev_ctx = exception_enter();
fc13aebab7d8f kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:05:34 +0200 4308) 		__schedule(true);
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4309) 		exception_exit(prev_ctx);
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4310) 
47252cfbac036 kernel/sched/core.c (Steven Rostedt             2016-03-21 11:23:39 -0400 4311) 		preempt_latency_stop(1);
3d8f74dd4ca1d kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:09:19 +0200 4312) 		preempt_enable_no_resched_notrace();
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4313) 	} while (need_resched());
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4314) }
4eaca0a887eae kernel/sched/core.c (Frederic Weisbecker        2015-06-04 17:39:08 +0200 4315) EXPORT_SYMBOL_GPL(preempt_schedule_notrace);
009f60e276356 kernel/sched/core.c (Oleg Nesterov              2014-10-05 22:23:22 +0200 4316) 
c1a280b68d4e6 kernel/sched/core.c (Thomas Gleixner            2019-07-26 23:19:37 +0200 4317) #endif /* CONFIG_PREEMPTION */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4318) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4319) /*
a49b4f4012ef2 kernel/sched/core.c (Valentin Schneider         2019-09-23 15:36:12 +0100 4320)  * This is the entry point to schedule() from kernel preemption
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4321)  * off of irq context.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4322)  * Note, that this is called and return with irqs disabled. This will
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4323)  * protect us against recursive calling from irq.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4324)  */
722a9f9299ca7 kernel/sched/core.c (Andi Kleen                 2014-05-02 00:44:38 +0200 4325) asmlinkage __visible void __sched preempt_schedule_irq(void)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4326) {
b22366cd54c6f kernel/sched/core.c (Frederic Weisbecker        2013-02-24 12:59:30 +0100 4327) 	enum ctx_state prev_state;
6478d8800b752 kernel/sched.c      (Ingo Molnar                2008-01-25 21:08:33 +0100 4328) 
2ed6e34f88a0d kernel/sched.c      (Andreas Mohr               2006-07-10 04:43:52 -0700 4329) 	/* Catch callers which need to be fixed */
f27dde8deef33 kernel/sched/core.c (Peter Zijlstra             2013-08-14 14:55:31 +0200 4330) 	BUG_ON(preempt_count() || !irqs_disabled());
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4331) 
b22366cd54c6f kernel/sched/core.c (Frederic Weisbecker        2013-02-24 12:59:30 +0100 4332) 	prev_state = exception_enter();
b22366cd54c6f kernel/sched/core.c (Frederic Weisbecker        2013-02-24 12:59:30 +0100 4333) 
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 4334) 	do {
3d8f74dd4ca1d kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:09:19 +0200 4335) 		preempt_disable();
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 4336) 		local_irq_enable();
fc13aebab7d8f kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:05:34 +0200 4337) 		__schedule(true);
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 4338) 		local_irq_disable();
3d8f74dd4ca1d kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:09:19 +0200 4339) 		sched_preempt_enable_no_resched();
5ed0cec0ac5f1 kernel/sched.c      (Lai Jiangshan              2009-03-06 19:40:20 +0800 4340) 	} while (need_resched());
b22366cd54c6f kernel/sched/core.c (Frederic Weisbecker        2013-02-24 12:59:30 +0100 4341) 
b22366cd54c6f kernel/sched/core.c (Frederic Weisbecker        2013-02-24 12:59:30 +0100 4342) 	exception_exit(prev_state);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4343) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4344) 
ac6424b981bce kernel/sched/core.c (Ingo Molnar                2017-06-20 12:06:13 +0200 4345) int default_wake_function(wait_queue_entry_t *curr, unsigned mode, int wake_flags,
95cdf3b799a48 kernel/sched.c      (Ingo Molnar                2005-09-10 00:26:11 -0700 4346) 			  void *key)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4347) {
63859d4fe4c97 kernel/sched.c      (Peter Zijlstra             2009-09-15 19:14:42 +0200 4348) 	return try_to_wake_up(curr->private, mode, wake_flags);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4349) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4350) EXPORT_SYMBOL(default_wake_function);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4351) 
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4352) #ifdef CONFIG_RT_MUTEXES
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4353) 
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4354) static inline int __rt_effective_prio(struct task_struct *pi_task, int prio)
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4355) {
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4356) 	if (pi_task)
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4357) 		prio = min(prio, pi_task->prio);
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4358) 
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4359) 	return prio;
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4360) }
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4361) 
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4362) static inline int rt_effective_prio(struct task_struct *p, int prio)
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4363) {
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4364) 	struct task_struct *pi_task = rt_mutex_get_top_task(p);
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4365) 
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4366) 	return __rt_effective_prio(pi_task, prio);
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4367) }
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4368) 
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4369) /*
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4370)  * rt_mutex_setprio - set the current priority of a task
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4371)  * @p: task to boost
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4372)  * @pi_task: donor task
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4373)  *
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4374)  * This function changes the 'effective' priority of a task. It does
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4375)  * not touch ->normal_prio like __setscheduler().
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4376)  *
c365c292d0590 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:42 +0100 4377)  * Used by the rt_mutex code to implement priority inheritance
c365c292d0590 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:42 +0100 4378)  * logic. Call site only calls if the priority of the task changed.
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4379)  */
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4380) void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4381) {
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4382) 	int prio, oldprio, queued, running, queue_flag =
7a57f32a4d5c8 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:47:02 +0100 4383) 		DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
83ab0aa0d5623 kernel/sched.c      (Thomas Gleixner            2010-02-17 09:05:48 +0100 4384) 	const struct sched_class *prev_class;
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 4385) 	struct rq_flags rf;
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 4386) 	struct rq *rq;
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4387) 
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4388) 	/* XXX used to be waiter->prio, not waiter->task->prio */
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4389) 	prio = __rt_effective_prio(pi_task, p->normal_prio);
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4390) 
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4391) 	/*
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4392) 	 * If nothing changed; bail early.
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4393) 	 */
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4394) 	if (p->pi_top_task == pi_task && prio == p->prio && !dl_prio(prio))
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4395) 		return;
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4396) 
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 4397) 	rq = __task_rq_lock(p, &rf);
80f5c1b84baa8 kernel/sched/core.c (Peter Zijlstra             2016-10-03 16:28:37 +0200 4398) 	update_rq_clock(rq);
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4399) 	/*
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4400) 	 * Set under pi_lock && rq->lock, such that the value can be used under
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4401) 	 * either lock.
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4402) 	 *
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4403) 	 * Note that there is loads of tricky to make this pointer cache work
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4404) 	 * right. rt_mutex_slowunlock()+rt_mutex_postunlock() work together to
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4405) 	 * ensure a task is de-boosted (pi_task is set to NULL) before the
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4406) 	 * task is allowed to run again (and can exit). This ensures the pointer
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4407) 	 * points to a blocked task -- which guaratees the task is present.
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4408) 	 */
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4409) 	p->pi_top_task = pi_task;
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4410) 
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4411) 	/*
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4412) 	 * For FIFO/RR we only need to set prio, if that matches we're done.
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4413) 	 */
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4414) 	if (prio == p->prio && !dl_prio(prio))
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4415) 		goto out_unlock;
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4416) 
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4417) 	/*
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4418) 	 * Idle task boosting is a nono in general. There is one
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4419) 	 * exception, when PREEMPT_RT and NOHZ is active:
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4420) 	 *
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4421) 	 * The idle task calls get_next_timer_interrupt() and holds
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4422) 	 * the timer wheel base->lock on the CPU and another CPU wants
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4423) 	 * to access the timer (probably to cancel it). We can safely
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4424) 	 * ignore the boosting request, as the idle CPU runs this code
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4425) 	 * with interrupts disabled and will complete the lock
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4426) 	 * protected section without being interrupted. So there is no
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4427) 	 * real need to boost.
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4428) 	 */
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4429) 	if (unlikely(p == rq->idle)) {
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4430) 		WARN_ON(p != rq->curr);
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4431) 		WARN_ON(p->pi_blocked_on);
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4432) 		goto out_unlock;
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4433) 	}
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4434) 
b91473ff6e979 kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:12 +0100 4435) 	trace_sched_pi_setprio(p, pi_task);
d5f9f942c601f kernel/sched.c      (Andrew Morton              2007-05-08 20:27:06 -0700 4436) 	oldprio = p->prio;
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 4437) 
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 4438) 	if (oldprio == prio)
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 4439) 		queue_flag &= ~DEQUEUE_MOVE;
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 4440) 
83ab0aa0d5623 kernel/sched.c      (Thomas Gleixner            2010-02-17 09:05:48 +0100 4441) 	prev_class = p->sched_class;
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 4442) 	queued = task_on_rq_queued(p);
051a1d1afa472 kernel/sched.c      (Dmitry Adamushko           2007-12-18 15:21:13 +0100 4443) 	running = task_current(rq, p);
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 4444) 	if (queued)
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 4445) 		dequeue_task(rq, p, queue_flag);
0e1f34833bd91 kernel/sched.c      (Hiroshi Shimamoto          2008-03-10 11:01:20 -0700 4446) 	if (running)
f3cd1c4ec059c kernel/sched/core.c (Kirill Tkhai               2014-09-12 17:41:40 +0400 4447) 		put_prev_task(rq, p);
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4448) 
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4449) 	/*
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4450) 	 * Boosting condition are:
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4451) 	 * 1. -rt task is running and holds mutex A
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4452) 	 *      --> -dl task blocks on mutex A
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4453) 	 *
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4454) 	 * 2. -dl task is running and holds mutex A
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4455) 	 *      --> -dl task blocks on mutex A and could preempt the
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4456) 	 *          running task
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4457) 	 */
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4458) 	if (dl_prio(prio)) {
466af29bf4270 kernel/sched/core.c (Oleg Nesterov              2014-06-06 18:52:06 +0200 4459) 		if (!dl_prio(p->normal_prio) ||
466af29bf4270 kernel/sched/core.c (Oleg Nesterov              2014-06-06 18:52:06 +0200 4460) 		    (pi_task && dl_entity_preempt(&pi_task->dl, &p->dl))) {
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4461) 			p->dl.dl_boosted = 1;
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 4462) 			queue_flag |= ENQUEUE_REPLENISH;
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4463) 		} else
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4464) 			p->dl.dl_boosted = 0;
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 4465) 		p->sched_class = &dl_sched_class;
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4466) 	} else if (rt_prio(prio)) {
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4467) 		if (dl_prio(oldprio))
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4468) 			p->dl.dl_boosted = 0;
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4469) 		if (oldprio < prio)
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 4470) 			queue_flag |= ENQUEUE_HEAD;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4471) 		p->sched_class = &rt_sched_class;
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4472) 	} else {
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4473) 		if (dl_prio(oldprio))
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4474) 			p->dl.dl_boosted = 0;
746db9443ea57 kernel/sched/core.c (Brian Silverman            2015-02-18 16:23:56 -0800 4475) 		if (rt_prio(oldprio))
746db9443ea57 kernel/sched/core.c (Brian Silverman            2015-02-18 16:23:56 -0800 4476) 			p->rt.timeout = 0;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4477) 		p->sched_class = &fair_sched_class;
2d3d891d33441 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:44 +0100 4478) 	}
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4479) 
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4480) 	p->prio = prio;
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4481) 
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 4482) 	if (queued)
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 4483) 		enqueue_task(rq, p, queue_flag);
a399d233078ed kernel/sched/core.c (Vincent Guittot            2016-09-12 09:47:52 +0200 4484) 	if (running)
03b7fad167efc kernel/sched/core.c (Peter Zijlstra             2019-05-29 20:36:41 +0000 4485) 		set_next_task(rq, p);
cb46984504048 kernel/sched.c      (Steven Rostedt             2008-01-25 21:08:22 +0100 4486) 
da7a735e51f96 kernel/sched.c      (Peter Zijlstra             2011-01-17 17:03:27 +0100 4487) 	check_class_changed(rq, p, prev_class, oldprio);
1c4dd99bed5f6 kernel/sched/core.c (Thomas Gleixner            2011-06-06 20:07:38 +0200 4488) out_unlock:
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4489) 	/* Avoid rq from going away on us: */
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4490) 	preempt_disable();
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 4491) 	__task_rq_unlock(rq, &rf);
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 4492) 
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 4493) 	balance_callback(rq);
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 4494) 	preempt_enable();
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4495) }
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4496) #else
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4497) static inline int rt_effective_prio(struct task_struct *p, int prio)
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4498) {
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4499) 	return prio;
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4500) }
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4501) #endif
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4502) 
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 4503) void set_user_nice(struct task_struct *p, long nice)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4504) {
49bd21efe7fc8 kernel/sched/core.c (Peter Zijlstra             2016-09-20 22:06:01 +0200 4505) 	bool queued, running;
53a23364b6b0c kernel/sched/core.c (Qian Cai                   2019-12-19 09:03:14 -0500 4506) 	int old_prio;
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 4507) 	struct rq_flags rf;
70b97a7f0b19c kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:42 -0700 4508) 	struct rq *rq;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4509) 
75e45d512f257 kernel/sched/core.c (Dongsheng Yang             2014-02-11 15:34:50 +0800 4510) 	if (task_nice(p) == nice || nice < MIN_NICE || nice > MAX_NICE)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4511) 		return;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4512) 	/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4513) 	 * We have to be careful, if called from sys_setpriority(),
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4514) 	 * the task might be in the middle of scheduling on another CPU.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4515) 	 */
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 4516) 	rq = task_rq_lock(p, &rf);
2fb8d36787aff kernel/sched/core.c (Peter Zijlstra             2016-10-03 16:44:25 +0200 4517) 	update_rq_clock(rq);
2fb8d36787aff kernel/sched/core.c (Peter Zijlstra             2016-10-03 16:44:25 +0200 4518) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4519) 	/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4520) 	 * The RT priorities are set via sched_setscheduler(), but we still
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4521) 	 * allow the 'normal' nice value to be set - but as expected
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4522) 	 * it wont have any effect on scheduling until the task is
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 4523) 	 * SCHED_DEADLINE, SCHED_FIFO or SCHED_RR:
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4524) 	 */
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 4525) 	if (task_has_dl_policy(p) || task_has_rt_policy(p)) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4526) 		p->static_prio = NICE_TO_PRIO(nice);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4527) 		goto out_unlock;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4528) 	}
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 4529) 	queued = task_on_rq_queued(p);
49bd21efe7fc8 kernel/sched/core.c (Peter Zijlstra             2016-09-20 22:06:01 +0200 4530) 	running = task_current(rq, p);
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 4531) 	if (queued)
7a57f32a4d5c8 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:47:02 +0100 4532) 		dequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK);
49bd21efe7fc8 kernel/sched/core.c (Peter Zijlstra             2016-09-20 22:06:01 +0200 4533) 	if (running)
49bd21efe7fc8 kernel/sched/core.c (Peter Zijlstra             2016-09-20 22:06:01 +0200 4534) 		put_prev_task(rq, p);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4535) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4536) 	p->static_prio = NICE_TO_PRIO(nice);
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200 4537) 	set_load_weight(p, true);
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4538) 	old_prio = p->prio;
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4539) 	p->prio = effective_prio(p);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4540) 
5443a0be6121d kernel/sched/core.c (Frederic Weisbecker        2019-12-03 17:01:06 +0100 4541) 	if (queued)
7134b3e941613 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:23:38 +0100 4542) 		enqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);
49bd21efe7fc8 kernel/sched/core.c (Peter Zijlstra             2016-09-20 22:06:01 +0200 4543) 	if (running)
03b7fad167efc kernel/sched/core.c (Peter Zijlstra             2019-05-29 20:36:41 +0000 4544) 		set_next_task(rq, p);
5443a0be6121d kernel/sched/core.c (Frederic Weisbecker        2019-12-03 17:01:06 +0100 4545) 
5443a0be6121d kernel/sched/core.c (Frederic Weisbecker        2019-12-03 17:01:06 +0100 4546) 	/*
5443a0be6121d kernel/sched/core.c (Frederic Weisbecker        2019-12-03 17:01:06 +0100 4547) 	 * If the task increased its priority or is running and
5443a0be6121d kernel/sched/core.c (Frederic Weisbecker        2019-12-03 17:01:06 +0100 4548) 	 * lowered its priority, then reschedule its CPU:
5443a0be6121d kernel/sched/core.c (Frederic Weisbecker        2019-12-03 17:01:06 +0100 4549) 	 */
5443a0be6121d kernel/sched/core.c (Frederic Weisbecker        2019-12-03 17:01:06 +0100 4550) 	p->sched_class->prio_changed(rq, p, old_prio);
5443a0be6121d kernel/sched/core.c (Frederic Weisbecker        2019-12-03 17:01:06 +0100 4551) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4552) out_unlock:
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 4553) 	task_rq_unlock(rq, p, &rf);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4554) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4555) EXPORT_SYMBOL(set_user_nice);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4556) 
e43379f10b421 kernel/sched.c      (Matt Mackall               2005-05-01 08:59:00 -0700 4557) /*
e43379f10b421 kernel/sched.c      (Matt Mackall               2005-05-01 08:59:00 -0700 4558)  * can_nice - check if a task can reduce its nice value
e43379f10b421 kernel/sched.c      (Matt Mackall               2005-05-01 08:59:00 -0700 4559)  * @p: task
e43379f10b421 kernel/sched.c      (Matt Mackall               2005-05-01 08:59:00 -0700 4560)  * @nice: nice value
e43379f10b421 kernel/sched.c      (Matt Mackall               2005-05-01 08:59:00 -0700 4561)  */
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 4562) int can_nice(const struct task_struct *p, const int nice)
e43379f10b421 kernel/sched.c      (Matt Mackall               2005-05-01 08:59:00 -0700 4563) {
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4564) 	/* Convert nice value [19,-20] to rlimit style value [1,40]: */
7aa2c016db216 kernel/sched/core.c (Dongsheng Yang             2014-05-08 18:33:49 +0900 4565) 	int nice_rlim = nice_to_rlimit(nice);
48f24c4da1ee7 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:40 -0700 4566) 
78d7d407b62a0 kernel/sched.c      (Jiri Slaby                 2010-03-05 13:42:54 -0800 4567) 	return (nice_rlim <= task_rlimit(p, RLIMIT_NICE) ||
e43379f10b421 kernel/sched.c      (Matt Mackall               2005-05-01 08:59:00 -0700 4568) 		capable(CAP_SYS_NICE));
e43379f10b421 kernel/sched.c      (Matt Mackall               2005-05-01 08:59:00 -0700 4569) }
e43379f10b421 kernel/sched.c      (Matt Mackall               2005-05-01 08:59:00 -0700 4570) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4571) #ifdef __ARCH_WANT_SYS_NICE
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4572) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4573) /*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4574)  * sys_nice - change the priority of the current process.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4575)  * @increment: priority increment
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4576)  *
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4577)  * sys_setpriority is a more generic, but much slower function that
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4578)  * does similar things.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4579)  */
5add95d4f7cf0 kernel/sched.c      (Heiko Carstens             2009-01-14 14:14:08 +0100 4580) SYSCALL_DEFINE1(nice, int, increment)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4581) {
48f24c4da1ee7 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:40 -0700 4582) 	long nice, retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4583) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4584) 	/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4585) 	 * Setpriority might change our priority at the same moment.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4586) 	 * We don't have to worry. Conceptually one call occurs first
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4587) 	 * and we have a single winner.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4588) 	 */
a9467fa3cd2d5 kernel/sched/core.c (Dongsheng Yang             2014-05-08 18:35:15 +0900 4589) 	increment = clamp(increment, -NICE_WIDTH, NICE_WIDTH);
d0ea026808ad8 kernel/sched/core.c (Dongsheng Yang             2014-01-27 22:00:45 -0500 4590) 	nice = task_nice(current) + increment;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4591) 
a9467fa3cd2d5 kernel/sched/core.c (Dongsheng Yang             2014-05-08 18:35:15 +0900 4592) 	nice = clamp_val(nice, MIN_NICE, MAX_NICE);
e43379f10b421 kernel/sched.c      (Matt Mackall               2005-05-01 08:59:00 -0700 4593) 	if (increment < 0 && !can_nice(current, nice))
e43379f10b421 kernel/sched.c      (Matt Mackall               2005-05-01 08:59:00 -0700 4594) 		return -EPERM;
e43379f10b421 kernel/sched.c      (Matt Mackall               2005-05-01 08:59:00 -0700 4595) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4596) 	retval = security_task_setnice(current, nice);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4597) 	if (retval)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4598) 		return retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4599) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4600) 	set_user_nice(current, nice);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4601) 	return 0;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4602) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4603) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4604) #endif
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4605) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4606) /**
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4607)  * task_prio - return the priority value of a given task.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4608)  * @p: the task in question.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4609)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 4610)  * Return: The priority value as seen by users in /proc.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4611)  * RT tasks are offset by -200. Normal tasks are centered
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4612)  * around 0, value goes from -16 to +15.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4613)  */
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 4614) int task_prio(const struct task_struct *p)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4615) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4616) 	return p->prio - MAX_RT_PRIO;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4617) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4618) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4619) /**
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4620)  * idle_cpu - is a given CPU idle currently?
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4621)  * @cpu: the processor in question.
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 4622)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 4623)  * Return: 1 if the CPU is currently idle. 0 otherwise.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4624)  */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4625) int idle_cpu(int cpu)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4626) {
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4627) 	struct rq *rq = cpu_rq(cpu);
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4628) 
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4629) 	if (rq->curr != rq->idle)
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4630) 		return 0;
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4631) 
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4632) 	if (rq->nr_running)
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4633) 		return 0;
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4634) 
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4635) #ifdef CONFIG_SMP
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4636) 	if (!llist_empty(&rq->wake_list))
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4637) 		return 0;
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4638) #endif
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4639) 
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4640) 	return 1;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4641) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4642) 
943d355d7feef kernel/sched/core.c (Rohit Jain                 2018-05-09 09:39:48 -0700 4643) /**
943d355d7feef kernel/sched/core.c (Rohit Jain                 2018-05-09 09:39:48 -0700 4644)  * available_idle_cpu - is a given CPU idle for enqueuing work.
943d355d7feef kernel/sched/core.c (Rohit Jain                 2018-05-09 09:39:48 -0700 4645)  * @cpu: the CPU in question.
943d355d7feef kernel/sched/core.c (Rohit Jain                 2018-05-09 09:39:48 -0700 4646)  *
943d355d7feef kernel/sched/core.c (Rohit Jain                 2018-05-09 09:39:48 -0700 4647)  * Return: 1 if the CPU is currently idle. 0 otherwise.
943d355d7feef kernel/sched/core.c (Rohit Jain                 2018-05-09 09:39:48 -0700 4648)  */
943d355d7feef kernel/sched/core.c (Rohit Jain                 2018-05-09 09:39:48 -0700 4649) int available_idle_cpu(int cpu)
943d355d7feef kernel/sched/core.c (Rohit Jain                 2018-05-09 09:39:48 -0700 4650) {
943d355d7feef kernel/sched/core.c (Rohit Jain                 2018-05-09 09:39:48 -0700 4651) 	if (!idle_cpu(cpu))
943d355d7feef kernel/sched/core.c (Rohit Jain                 2018-05-09 09:39:48 -0700 4652) 		return 0;
943d355d7feef kernel/sched/core.c (Rohit Jain                 2018-05-09 09:39:48 -0700 4653) 
247f2f6f3c706 kernel/sched/core.c (Rohit Jain                 2018-05-02 13:52:10 -0700 4654) 	if (vcpu_is_preempted(cpu))
247f2f6f3c706 kernel/sched/core.c (Rohit Jain                 2018-05-02 13:52:10 -0700 4655) 		return 0;
247f2f6f3c706 kernel/sched/core.c (Rohit Jain                 2018-05-02 13:52:10 -0700 4656) 
908a3283728d9 kernel/sched.c      (Thomas Gleixner            2011-09-15 15:32:06 +0200 4657) 	return 1;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4658) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4659) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4660) /**
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4661)  * idle_task - return the idle task for a given CPU.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4662)  * @cpu: the processor in question.
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 4663)  *
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4664)  * Return: The idle task for the CPU @cpu.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4665)  */
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 4666) struct task_struct *idle_task(int cpu)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4667) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4668) 	return cpu_rq(cpu)->idle;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4669) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4670) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4671) /**
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4672)  * find_process_by_pid - find a process with a matching PID value.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4673)  * @pid: the pid in question.
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 4674)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 4675)  * The task of @pid, if found. %NULL otherwise.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4676)  */
a9957449b08ab kernel/sched.c      (Alexey Dobriyan            2007-10-15 17:00:13 +0200 4677) static struct task_struct *find_process_by_pid(pid_t pid)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4678) {
228ebcbe634a3 kernel/sched.c      (Pavel Emelyanov            2007-10-18 23:40:16 -0700 4679) 	return pid ? find_task_by_vpid(pid) : current;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4680) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4681) 
c13db6b131a43 kernel/sched/core.c (Steven Rostedt             2014-07-23 11:28:26 -0400 4682) /*
c13db6b131a43 kernel/sched/core.c (Steven Rostedt             2014-07-23 11:28:26 -0400 4683)  * sched_setparam() passes in -1 for its policy, to let the functions
c13db6b131a43 kernel/sched/core.c (Steven Rostedt             2014-07-23 11:28:26 -0400 4684)  * it calls know not to change it.
c13db6b131a43 kernel/sched/core.c (Steven Rostedt             2014-07-23 11:28:26 -0400 4685)  */
c13db6b131a43 kernel/sched/core.c (Steven Rostedt             2014-07-23 11:28:26 -0400 4686) #define SETPARAM_POLICY	-1
c13db6b131a43 kernel/sched/core.c (Steven Rostedt             2014-07-23 11:28:26 -0400 4687) 
c365c292d0590 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:42 +0100 4688) static void __setscheduler_params(struct task_struct *p,
c365c292d0590 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:42 +0100 4689) 		const struct sched_attr *attr)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4690) {
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4691) 	int policy = attr->sched_policy;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4692) 
c13db6b131a43 kernel/sched/core.c (Steven Rostedt             2014-07-23 11:28:26 -0400 4693) 	if (policy == SETPARAM_POLICY)
39fd8fd22b322 kernel/sched/core.c (Peter Zijlstra             2014-01-15 16:33:20 +0100 4694) 		policy = p->policy;
39fd8fd22b322 kernel/sched/core.c (Peter Zijlstra             2014-01-15 16:33:20 +0100 4695) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4696) 	p->policy = policy;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4697) 
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 4698) 	if (dl_policy(policy))
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 4699) 		__setparam_dl(p, attr);
39fd8fd22b322 kernel/sched/core.c (Peter Zijlstra             2014-01-15 16:33:20 +0100 4700) 	else if (fair_policy(policy))
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4701) 		p->static_prio = NICE_TO_PRIO(attr->sched_nice);
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4702) 
39fd8fd22b322 kernel/sched/core.c (Peter Zijlstra             2014-01-15 16:33:20 +0100 4703) 	/*
39fd8fd22b322 kernel/sched/core.c (Peter Zijlstra             2014-01-15 16:33:20 +0100 4704) 	 * __sched_setscheduler() ensures attr->sched_priority == 0 when
39fd8fd22b322 kernel/sched/core.c (Peter Zijlstra             2014-01-15 16:33:20 +0100 4705) 	 * !rt_policy. Always setting this ensures that things like
39fd8fd22b322 kernel/sched/core.c (Peter Zijlstra             2014-01-15 16:33:20 +0100 4706) 	 * getparam()/getattr() don't report silly values for !rt tasks.
39fd8fd22b322 kernel/sched/core.c (Peter Zijlstra             2014-01-15 16:33:20 +0100 4707) 	 */
39fd8fd22b322 kernel/sched/core.c (Peter Zijlstra             2014-01-15 16:33:20 +0100 4708) 	p->rt_priority = attr->sched_priority;
383afd0971538 kernel/sched/core.c (Steven Rostedt             2014-03-11 19:24:20 -0400 4709) 	p->normal_prio = normal_prio(p);
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200 4710) 	set_load_weight(p, true);
c365c292d0590 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:42 +0100 4711) }
39fd8fd22b322 kernel/sched/core.c (Peter Zijlstra             2014-01-15 16:33:20 +0100 4712) 
c365c292d0590 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:42 +0100 4713) /* Actually do priority change: must hold pi & rq lock. */
c365c292d0590 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:42 +0100 4714) static void __setscheduler(struct rq *rq, struct task_struct *p,
0782e63bc6fe7 kernel/sched/core.c (Thomas Gleixner            2015-05-05 19:49:49 +0200 4715) 			   const struct sched_attr *attr, bool keep_boost)
c365c292d0590 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:42 +0100 4716) {
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4717) 	/*
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4718) 	 * If params can't change scheduling class changes aren't allowed
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4719) 	 * either.
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4720) 	 */
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4721) 	if (attr->sched_flags & SCHED_FLAG_KEEP_PARAMS)
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4722) 		return;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4723) 
c365c292d0590 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:42 +0100 4724) 	__setscheduler_params(p, attr);
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4725) 
383afd0971538 kernel/sched/core.c (Steven Rostedt             2014-03-11 19:24:20 -0400 4726) 	/*
0782e63bc6fe7 kernel/sched/core.c (Thomas Gleixner            2015-05-05 19:49:49 +0200 4727) 	 * Keep a potential priority boosting if called from
0782e63bc6fe7 kernel/sched/core.c (Thomas Gleixner            2015-05-05 19:49:49 +0200 4728) 	 * sched_setscheduler().
383afd0971538 kernel/sched/core.c (Steven Rostedt             2014-03-11 19:24:20 -0400 4729) 	 */
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4730) 	p->prio = normal_prio(p);
0782e63bc6fe7 kernel/sched/core.c (Thomas Gleixner            2015-05-05 19:49:49 +0200 4731) 	if (keep_boost)
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4732) 		p->prio = rt_effective_prio(p, p->prio);
383afd0971538 kernel/sched/core.c (Steven Rostedt             2014-03-11 19:24:20 -0400 4733) 
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 4734) 	if (dl_prio(p->prio))
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 4735) 		p->sched_class = &dl_sched_class;
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 4736) 	else if (rt_prio(p->prio))
ffd44db5f02af kernel/sched.c      (Peter Zijlstra             2009-11-10 20:12:01 +0100 4737) 		p->sched_class = &rt_sched_class;
ffd44db5f02af kernel/sched.c      (Peter Zijlstra             2009-11-10 20:12:01 +0100 4738) 	else
ffd44db5f02af kernel/sched.c      (Peter Zijlstra             2009-11-10 20:12:01 +0100 4739) 		p->sched_class = &fair_sched_class;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4740) }
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 4741) 
c69e8d9c01db2 kernel/sched.c      (David Howells              2008-11-14 10:39:19 +1100 4742) /*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4743)  * Check the target process has a UID that matches the current process's:
c69e8d9c01db2 kernel/sched.c      (David Howells              2008-11-14 10:39:19 +1100 4744)  */
c69e8d9c01db2 kernel/sched.c      (David Howells              2008-11-14 10:39:19 +1100 4745) static bool check_same_owner(struct task_struct *p)
c69e8d9c01db2 kernel/sched.c      (David Howells              2008-11-14 10:39:19 +1100 4746) {
c69e8d9c01db2 kernel/sched.c      (David Howells              2008-11-14 10:39:19 +1100 4747) 	const struct cred *cred = current_cred(), *pcred;
c69e8d9c01db2 kernel/sched.c      (David Howells              2008-11-14 10:39:19 +1100 4748) 	bool match;
c69e8d9c01db2 kernel/sched.c      (David Howells              2008-11-14 10:39:19 +1100 4749) 
c69e8d9c01db2 kernel/sched.c      (David Howells              2008-11-14 10:39:19 +1100 4750) 	rcu_read_lock();
c69e8d9c01db2 kernel/sched.c      (David Howells              2008-11-14 10:39:19 +1100 4751) 	pcred = __task_cred(p);
9c806aa06f8e1 kernel/sched/core.c (Eric W. Biederman          2012-02-02 18:54:02 -0800 4752) 	match = (uid_eq(cred->euid, pcred->euid) ||
9c806aa06f8e1 kernel/sched/core.c (Eric W. Biederman          2012-02-02 18:54:02 -0800 4753) 		 uid_eq(cred->euid, pcred->uid));
c69e8d9c01db2 kernel/sched.c      (David Howells              2008-11-14 10:39:19 +1100 4754) 	rcu_read_unlock();
c69e8d9c01db2 kernel/sched.c      (David Howells              2008-11-14 10:39:19 +1100 4755) 	return match;
c69e8d9c01db2 kernel/sched.c      (David Howells              2008-11-14 10:39:19 +1100 4756) }
c69e8d9c01db2 kernel/sched.c      (David Howells              2008-11-14 10:39:19 +1100 4757) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4758) static int __sched_setscheduler(struct task_struct *p,
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4759) 				const struct sched_attr *attr,
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 4760) 				bool user, bool pi)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4761) {
383afd0971538 kernel/sched/core.c (Steven Rostedt             2014-03-11 19:24:20 -0400 4762) 	int newprio = dl_policy(attr->sched_policy) ? MAX_DL_PRIO - 1 :
383afd0971538 kernel/sched/core.c (Steven Rostedt             2014-03-11 19:24:20 -0400 4763) 		      MAX_RT_PRIO - 1 - attr->sched_priority;
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 4764) 	int retval, oldprio, oldpolicy = -1, queued, running;
0782e63bc6fe7 kernel/sched/core.c (Thomas Gleixner            2015-05-05 19:49:49 +0200 4765) 	int new_effective_prio, policy = attr->sched_policy;
83ab0aa0d5623 kernel/sched.c      (Thomas Gleixner            2010-02-17 09:05:48 +0100 4766) 	const struct sched_class *prev_class;
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 4767) 	struct rq_flags rf;
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 4768) 	int reset_on_fork;
7a57f32a4d5c8 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:47:02 +0100 4769) 	int queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 4770) 	struct rq *rq;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4771) 
896bbb2522587 kernel/sched/core.c (Steven Rostedt (VMware)    2017-03-09 10:18:42 -0500 4772) 	/* The pi code expects interrupts enabled */
896bbb2522587 kernel/sched/core.c (Steven Rostedt (VMware)    2017-03-09 10:18:42 -0500 4773) 	BUG_ON(pi && in_interrupt());
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4774) recheck:
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4775) 	/* Double check policy once rq lock held: */
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 4776) 	if (policy < 0) {
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 4777) 		reset_on_fork = p->sched_reset_on_fork;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4778) 		policy = oldpolicy = p->policy;
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 4779) 	} else {
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 4780) 		reset_on_fork = !!(attr->sched_flags & SCHED_FLAG_RESET_ON_FORK);
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 4781) 
20f9cd2acb1d7 kernel/sched/core.c (Henrik Austad              2015-09-09 17:00:41 +0200 4782) 		if (!valid_policy(policy))
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 4783) 			return -EINVAL;
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 4784) 	}
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 4785) 
794a56ebd9a57 kernel/sched/core.c (Juri Lelli                 2017-12-04 11:23:20 +0100 4786) 	if (attr->sched_flags & ~(SCHED_FLAG_ALL | SCHED_FLAG_SUGOV))
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 4787) 		return -EINVAL;
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 4788) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4789) 	/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4790) 	 * Valid priorities for SCHED_FIFO and SCHED_RR are
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4791) 	 * 1..MAX_USER_RT_PRIO-1, valid priority for SCHED_NORMAL,
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4792) 	 * SCHED_BATCH and SCHED_IDLE is 0.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4793) 	 */
0bb040a443812 kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:15:13 +0100 4794) 	if ((p->mm && attr->sched_priority > MAX_USER_RT_PRIO-1) ||
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4795) 	    (!p->mm && attr->sched_priority > MAX_RT_PRIO-1))
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4796) 		return -EINVAL;
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 4797) 	if ((dl_policy(policy) && !__checkparam_dl(attr)) ||
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 4798) 	    (rt_policy(policy) != (attr->sched_priority != 0)))
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4799) 		return -EINVAL;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4800) 
37e4ab3f0cba1 kernel/sched.c      (Olivier Croquette          2005-06-25 14:57:32 -0700 4801) 	/*
37e4ab3f0cba1 kernel/sched.c      (Olivier Croquette          2005-06-25 14:57:32 -0700 4802) 	 * Allow unprivileged RT tasks to decrease priority:
37e4ab3f0cba1 kernel/sched.c      (Olivier Croquette          2005-06-25 14:57:32 -0700 4803) 	 */
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 4804) 	if (user && !capable(CAP_SYS_NICE)) {
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4805) 		if (fair_policy(policy)) {
d0ea026808ad8 kernel/sched/core.c (Dongsheng Yang             2014-01-27 22:00:45 -0500 4806) 			if (attr->sched_nice < task_nice(p) &&
eaad45132c564 kernel/sched/core.c (Peter Zijlstra             2014-01-16 17:54:25 +0100 4807) 			    !can_nice(p, attr->sched_nice))
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4808) 				return -EPERM;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4809) 		}
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4810) 
e05606d330152 kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4811) 		if (rt_policy(policy)) {
a44702e8858a0 kernel/sched.c      (Oleg Nesterov              2010-06-11 01:09:44 +0200 4812) 			unsigned long rlim_rtprio =
a44702e8858a0 kernel/sched.c      (Oleg Nesterov              2010-06-11 01:09:44 +0200 4813) 					task_rlimit(p, RLIMIT_RTPRIO);
8dc3e9099e01d kernel/sched.c      (Oleg Nesterov              2006-09-29 02:00:50 -0700 4814) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4815) 			/* Can't set/change the rt policy: */
8dc3e9099e01d kernel/sched.c      (Oleg Nesterov              2006-09-29 02:00:50 -0700 4816) 			if (policy != p->policy && !rlim_rtprio)
8dc3e9099e01d kernel/sched.c      (Oleg Nesterov              2006-09-29 02:00:50 -0700 4817) 				return -EPERM;
8dc3e9099e01d kernel/sched.c      (Oleg Nesterov              2006-09-29 02:00:50 -0700 4818) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4819) 			/* Can't increase priority: */
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4820) 			if (attr->sched_priority > p->rt_priority &&
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4821) 			    attr->sched_priority > rlim_rtprio)
8dc3e9099e01d kernel/sched.c      (Oleg Nesterov              2006-09-29 02:00:50 -0700 4822) 				return -EPERM;
8dc3e9099e01d kernel/sched.c      (Oleg Nesterov              2006-09-29 02:00:50 -0700 4823) 		}
c02aa73b1d18e kernel/sched.c      (Darren Hart                2011-02-17 15:37:07 -0800 4824) 
d44753b843e09 kernel/sched/core.c (Juri Lelli                 2014-03-03 12:09:21 +0100 4825) 		 /*
d44753b843e09 kernel/sched/core.c (Juri Lelli                 2014-03-03 12:09:21 +0100 4826) 		  * Can't set/change SCHED_DEADLINE policy at all for now
d44753b843e09 kernel/sched/core.c (Juri Lelli                 2014-03-03 12:09:21 +0100 4827) 		  * (safest behavior); in the future we would like to allow
d44753b843e09 kernel/sched/core.c (Juri Lelli                 2014-03-03 12:09:21 +0100 4828) 		  * unprivileged DL tasks to increase their relative deadline
d44753b843e09 kernel/sched/core.c (Juri Lelli                 2014-03-03 12:09:21 +0100 4829) 		  * or reduce their runtime (both ways reducing utilization)
d44753b843e09 kernel/sched/core.c (Juri Lelli                 2014-03-03 12:09:21 +0100 4830) 		  */
d44753b843e09 kernel/sched/core.c (Juri Lelli                 2014-03-03 12:09:21 +0100 4831) 		if (dl_policy(policy))
d44753b843e09 kernel/sched/core.c (Juri Lelli                 2014-03-03 12:09:21 +0100 4832) 			return -EPERM;
d44753b843e09 kernel/sched/core.c (Juri Lelli                 2014-03-03 12:09:21 +0100 4833) 
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4834) 		/*
c02aa73b1d18e kernel/sched.c      (Darren Hart                2011-02-17 15:37:07 -0800 4835) 		 * Treat SCHED_IDLE as nice 20. Only allow a switch to
c02aa73b1d18e kernel/sched.c      (Darren Hart                2011-02-17 15:37:07 -0800 4836) 		 * SCHED_NORMAL if the RLIMIT_NICE would normally permit it.
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 4837) 		 */
1da1843f9f033 kernel/sched/core.c (Viresh Kumar               2018-11-05 16:51:55 +0530 4838) 		if (task_has_idle_policy(p) && !idle_policy(policy)) {
d0ea026808ad8 kernel/sched/core.c (Dongsheng Yang             2014-01-27 22:00:45 -0500 4839) 			if (!can_nice(p, task_nice(p)))
c02aa73b1d18e kernel/sched.c      (Darren Hart                2011-02-17 15:37:07 -0800 4840) 				return -EPERM;
c02aa73b1d18e kernel/sched.c      (Darren Hart                2011-02-17 15:37:07 -0800 4841) 		}
5fe1d75f34974 kernel/sched.c      (Oleg Nesterov              2006-09-29 02:00:48 -0700 4842) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4843) 		/* Can't change other user's priorities: */
c69e8d9c01db2 kernel/sched.c      (David Howells              2008-11-14 10:39:19 +1100 4844) 		if (!check_same_owner(p))
37e4ab3f0cba1 kernel/sched.c      (Olivier Croquette          2005-06-25 14:57:32 -0700 4845) 			return -EPERM;
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 4846) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4847) 		/* Normal users shall not reset the sched_reset_on_fork flag: */
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 4848) 		if (p->sched_reset_on_fork && !reset_on_fork)
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 4849) 			return -EPERM;
37e4ab3f0cba1 kernel/sched.c      (Olivier Croquette          2005-06-25 14:57:32 -0700 4850) 	}
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4851) 
725aad24c3ba9 kernel/sched.c      (Jeremy Fitzhardinge        2008-08-03 09:33:03 -0700 4852) 	if (user) {
794a56ebd9a57 kernel/sched/core.c (Juri Lelli                 2017-12-04 11:23:20 +0100 4853) 		if (attr->sched_flags & SCHED_FLAG_SUGOV)
794a56ebd9a57 kernel/sched/core.c (Juri Lelli                 2017-12-04 11:23:20 +0100 4854) 			return -EINVAL;
794a56ebd9a57 kernel/sched/core.c (Juri Lelli                 2017-12-04 11:23:20 +0100 4855) 
b0ae198113750 kernel/sched.c      (KOSAKI Motohiro            2010-10-15 04:21:18 +0900 4856) 		retval = security_task_setscheduler(p);
725aad24c3ba9 kernel/sched.c      (Jeremy Fitzhardinge        2008-08-03 09:33:03 -0700 4857) 		if (retval)
725aad24c3ba9 kernel/sched.c      (Jeremy Fitzhardinge        2008-08-03 09:33:03 -0700 4858) 			return retval;
725aad24c3ba9 kernel/sched.c      (Jeremy Fitzhardinge        2008-08-03 09:33:03 -0700 4859) 	}
725aad24c3ba9 kernel/sched.c      (Jeremy Fitzhardinge        2008-08-03 09:33:03 -0700 4860) 
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4861) 	/* Update task specific "requested" clamps */
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4862) 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP) {
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4863) 		retval = uclamp_validate(p, attr);
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4864) 		if (retval)
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4865) 			return retval;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4866) 	}
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4867) 
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 4868) 	if (pi)
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 4869) 		cpuset_read_lock();
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 4870) 
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4871) 	/*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4872) 	 * Make sure no PI-waiters arrive (or leave) while we are
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 4873) 	 * changing the priority of the task:
0122ec5b02f76 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:51 +0200 4874) 	 *
25985edcedea6 kernel/sched.c      (Lucas De Marchi            2011-03-30 22:57:33 -0300 4875) 	 * To be able to change p->policy safely, the appropriate
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4876) 	 * runqueue lock must be held.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4877) 	 */
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 4878) 	rq = task_rq_lock(p, &rf);
80f5c1b84baa8 kernel/sched/core.c (Peter Zijlstra             2016-10-03 16:28:37 +0200 4879) 	update_rq_clock(rq);
dc61b1d65e353 kernel/sched.c      (Peter Zijlstra             2010-06-08 11:40:42 +0200 4880) 
34f971f6f7988 kernel/sched.c      (Peter Zijlstra             2010-09-22 13:53:15 +0200 4881) 	/*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4882) 	 * Changing the policy of the stop threads its a very bad idea:
34f971f6f7988 kernel/sched.c      (Peter Zijlstra             2010-09-22 13:53:15 +0200 4883) 	 */
34f971f6f7988 kernel/sched.c      (Peter Zijlstra             2010-09-22 13:53:15 +0200 4884) 	if (p == rq->stop) {
4b211f2b129dd kernel/sched/core.c (Mathieu Poirier            2019-07-19 15:59:54 +0200 4885) 		retval = -EINVAL;
4b211f2b129dd kernel/sched/core.c (Mathieu Poirier            2019-07-19 15:59:54 +0200 4886) 		goto unlock;
34f971f6f7988 kernel/sched.c      (Peter Zijlstra             2010-09-22 13:53:15 +0200 4887) 	}
34f971f6f7988 kernel/sched.c      (Peter Zijlstra             2010-09-22 13:53:15 +0200 4888) 
a51e91981870d kernel/sched.c      (Dario Faggioli             2011-03-24 14:00:18 +0100 4889) 	/*
d6b1e9119787f kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:40 +0100 4890) 	 * If not changing anything there's no need to proceed further,
d6b1e9119787f kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:40 +0100 4891) 	 * but store a possible modification of reset_on_fork.
a51e91981870d kernel/sched.c      (Dario Faggioli             2011-03-24 14:00:18 +0100 4892) 	 */
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4893) 	if (unlikely(policy == p->policy)) {
d0ea026808ad8 kernel/sched/core.c (Dongsheng Yang             2014-01-27 22:00:45 -0500 4894) 		if (fair_policy(policy) && attr->sched_nice != task_nice(p))
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4895) 			goto change;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4896) 		if (rt_policy(policy) && attr->sched_priority != p->rt_priority)
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4897) 			goto change;
75381608e8410 kernel/sched/core.c (Wanpeng Li                 2014-11-26 08:44:04 +0800 4898) 		if (dl_policy(policy) && dl_param_changed(p, attr))
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 4899) 			goto change;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4900) 		if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4901) 			goto change;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4902) 
d6b1e9119787f kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:40 +0100 4903) 		p->sched_reset_on_fork = reset_on_fork;
4b211f2b129dd kernel/sched/core.c (Mathieu Poirier            2019-07-19 15:59:54 +0200 4904) 		retval = 0;
4b211f2b129dd kernel/sched/core.c (Mathieu Poirier            2019-07-19 15:59:54 +0200 4905) 		goto unlock;
a51e91981870d kernel/sched.c      (Dario Faggioli             2011-03-24 14:00:18 +0100 4906) 	}
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 4907) change:
a51e91981870d kernel/sched.c      (Dario Faggioli             2011-03-24 14:00:18 +0100 4908) 
dc61b1d65e353 kernel/sched.c      (Peter Zijlstra             2010-06-08 11:40:42 +0200 4909) 	if (user) {
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4910) #ifdef CONFIG_RT_GROUP_SCHED
dc61b1d65e353 kernel/sched.c      (Peter Zijlstra             2010-06-08 11:40:42 +0200 4911) 		/*
dc61b1d65e353 kernel/sched.c      (Peter Zijlstra             2010-06-08 11:40:42 +0200 4912) 		 * Do not allow realtime tasks into groups that have no runtime
dc61b1d65e353 kernel/sched.c      (Peter Zijlstra             2010-06-08 11:40:42 +0200 4913) 		 * assigned.
dc61b1d65e353 kernel/sched.c      (Peter Zijlstra             2010-06-08 11:40:42 +0200 4914) 		 */
dc61b1d65e353 kernel/sched.c      (Peter Zijlstra             2010-06-08 11:40:42 +0200 4915) 		if (rt_bandwidth_enabled() && rt_policy(policy) &&
f44937718ce3b kernel/sched.c      (Mike Galbraith             2011-01-13 04:54:50 +0100 4916) 				task_group(p)->rt_bandwidth.rt_runtime == 0 &&
f44937718ce3b kernel/sched.c      (Mike Galbraith             2011-01-13 04:54:50 +0100 4917) 				!task_group_is_autogroup(task_group(p))) {
4b211f2b129dd kernel/sched/core.c (Mathieu Poirier            2019-07-19 15:59:54 +0200 4918) 			retval = -EPERM;
4b211f2b129dd kernel/sched/core.c (Mathieu Poirier            2019-07-19 15:59:54 +0200 4919) 			goto unlock;
dc61b1d65e353 kernel/sched.c      (Peter Zijlstra             2010-06-08 11:40:42 +0200 4920) 		}
dc61b1d65e353 kernel/sched.c      (Peter Zijlstra             2010-06-08 11:40:42 +0200 4921) #endif
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4922) #ifdef CONFIG_SMP
794a56ebd9a57 kernel/sched/core.c (Juri Lelli                 2017-12-04 11:23:20 +0100 4923) 		if (dl_bandwidth_enabled() && dl_policy(policy) &&
794a56ebd9a57 kernel/sched/core.c (Juri Lelli                 2017-12-04 11:23:20 +0100 4924) 				!(attr->sched_flags & SCHED_FLAG_SUGOV)) {
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4925) 			cpumask_t *span = rq->rd->span;
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4926) 
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4927) 			/*
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4928) 			 * Don't allow tasks with an affinity mask smaller than
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4929) 			 * the entire root_domain to become SCHED_DEADLINE. We
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4930) 			 * will also fail if there's no bandwidth available.
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4931) 			 */
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 4932) 			if (!cpumask_subset(span, p->cpus_ptr) ||
e4099a5e92943 kernel/sched/core.c (Peter Zijlstra             2013-12-17 10:03:34 +0100 4933) 			    rq->rd->dl_bw.bw == 0) {
4b211f2b129dd kernel/sched/core.c (Mathieu Poirier            2019-07-19 15:59:54 +0200 4934) 				retval = -EPERM;
4b211f2b129dd kernel/sched/core.c (Mathieu Poirier            2019-07-19 15:59:54 +0200 4935) 				goto unlock;
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4936) 			}
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4937) 		}
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4938) #endif
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4939) 	}
dc61b1d65e353 kernel/sched.c      (Peter Zijlstra             2010-06-08 11:40:42 +0200 4940) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 4941) 	/* Re-check policy now with rq lock held: */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4942) 	if (unlikely(oldpolicy != -1 && oldpolicy != p->policy)) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4943) 		policy = oldpolicy = -1;
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 4944) 		task_rq_unlock(rq, p, &rf);
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 4945) 		if (pi)
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 4946) 			cpuset_read_unlock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4947) 		goto recheck;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 4948) 	}
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4949) 
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4950) 	/*
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4951) 	 * If setscheduling to SCHED_DEADLINE (or changing the parameters
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4952) 	 * of a SCHED_DEADLINE task) we need to check if enough bandwidth
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4953) 	 * is available.
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4954) 	 */
06a76fe08d4da kernel/sched/core.c (Nicolas Pitre              2017-06-21 14:22:01 -0400 4955) 	if ((dl_policy(policy) || dl_task(p)) && sched_dl_overflow(p, policy, attr)) {
4b211f2b129dd kernel/sched/core.c (Mathieu Poirier            2019-07-19 15:59:54 +0200 4956) 		retval = -EBUSY;
4b211f2b129dd kernel/sched/core.c (Mathieu Poirier            2019-07-19 15:59:54 +0200 4957) 		goto unlock;
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4958) 	}
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 4959) 
c365c292d0590 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:42 +0100 4960) 	p->sched_reset_on_fork = reset_on_fork;
c365c292d0590 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:42 +0100 4961) 	oldprio = p->prio;
c365c292d0590 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:42 +0100 4962) 
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 4963) 	if (pi) {
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 4964) 		/*
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 4965) 		 * Take priority boosted tasks into account. If the new
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 4966) 		 * effective priority is unchanged, we just store the new
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 4967) 		 * normal parameters and do not touch the scheduler class and
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 4968) 		 * the runqueue. This will be done when the task deboost
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 4969) 		 * itself.
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 4970) 		 */
acd58620e415a kernel/sched/core.c (Peter Zijlstra             2017-03-23 15:56:11 +0100 4971) 		new_effective_prio = rt_effective_prio(p, newprio);
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 4972) 		if (new_effective_prio == oldprio)
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 4973) 			queue_flags &= ~DEQUEUE_MOVE;
c365c292d0590 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:42 +0100 4974) 	}
c365c292d0590 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:42 +0100 4975) 
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 4976) 	queued = task_on_rq_queued(p);
051a1d1afa472 kernel/sched.c      (Dmitry Adamushko           2007-12-18 15:21:13 +0100 4977) 	running = task_current(rq, p);
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 4978) 	if (queued)
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 4979) 		dequeue_task(rq, p, queue_flags);
0e1f34833bd91 kernel/sched.c      (Hiroshi Shimamoto          2008-03-10 11:01:20 -0700 4980) 	if (running)
f3cd1c4ec059c kernel/sched/core.c (Kirill Tkhai               2014-09-12 17:41:40 +0400 4981) 		put_prev_task(rq, p);
f6b53205e17c8 kernel/sched.c      (Dmitry Adamushko           2007-10-15 17:00:08 +0200 4982) 
83ab0aa0d5623 kernel/sched.c      (Thomas Gleixner            2010-02-17 09:05:48 +0100 4983) 	prev_class = p->sched_class;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4984) 
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 4985) 	__setscheduler(rq, p, attr, pi);
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 4986) 	__setscheduler_uclamp(p, attr);
f6b53205e17c8 kernel/sched.c      (Dmitry Adamushko           2007-10-15 17:00:08 +0200 4987) 
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 4988) 	if (queued) {
81a44c5441d7f kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:41 +0100 4989) 		/*
81a44c5441d7f kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:41 +0100 4990) 		 * We enqueue to tail when the priority of a task is
81a44c5441d7f kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:41 +0100 4991) 		 * increased (user space view).
81a44c5441d7f kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:41 +0100 4992) 		 */
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 4993) 		if (oldprio < p->prio)
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 4994) 			queue_flags |= ENQUEUE_HEAD;
1de64443d755f kernel/sched/core.c (Peter Zijlstra             2015-09-30 17:44:13 +0200 4995) 
ff77e46853598 kernel/sched/core.c (Peter Zijlstra             2016-01-18 15:27:07 +0100 4996) 		enqueue_task(rq, p, queue_flags);
81a44c5441d7f kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:41 +0100 4997) 	}
a399d233078ed kernel/sched/core.c (Vincent Guittot            2016-09-12 09:47:52 +0200 4998) 	if (running)
03b7fad167efc kernel/sched/core.c (Peter Zijlstra             2019-05-29 20:36:41 +0000 4999) 		set_next_task(rq, p);
cb46984504048 kernel/sched.c      (Steven Rostedt             2008-01-25 21:08:22 +0100 5000) 
da7a735e51f96 kernel/sched.c      (Peter Zijlstra             2011-01-17 17:03:27 +0100 5001) 	check_class_changed(rq, p, prev_class, oldprio);
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5002) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5003) 	/* Avoid rq from going away on us: */
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5004) 	preempt_disable();
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 5005) 	task_rq_unlock(rq, p, &rf);
b29739f902ee7 kernel/sched.c      (Ingo Molnar                2006-06-27 02:54:51 -0700 5006) 
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 5007) 	if (pi) {
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 5008) 		cpuset_read_unlock();
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 5009) 		rt_mutex_adjust_pi(p);
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 5010) 	}
95e02ca9bb532 kernel/sched.c      (Thomas Gleixner            2006-06-27 02:55:02 -0700 5011) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5012) 	/* Run balance callbacks after we've adjusted the PI chain: */
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 5013) 	balance_callback(rq);
4c9a4bc89a9cc kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:39 +0200 5014) 	preempt_enable();
95e02ca9bb532 kernel/sched.c      (Thomas Gleixner            2006-06-27 02:55:02 -0700 5015) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5016) 	return 0;
4b211f2b129dd kernel/sched/core.c (Mathieu Poirier            2019-07-19 15:59:54 +0200 5017) 
4b211f2b129dd kernel/sched/core.c (Mathieu Poirier            2019-07-19 15:59:54 +0200 5018) unlock:
4b211f2b129dd kernel/sched/core.c (Mathieu Poirier            2019-07-19 15:59:54 +0200 5019) 	task_rq_unlock(rq, p, &rf);
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 5020) 	if (pi)
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 5021) 		cpuset_read_unlock();
4b211f2b129dd kernel/sched/core.c (Mathieu Poirier            2019-07-19 15:59:54 +0200 5022) 	return retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5023) }
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5024) 
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5025) static int _sched_setscheduler(struct task_struct *p, int policy,
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5026) 			       const struct sched_param *param, bool check)
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5027) {
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5028) 	struct sched_attr attr = {
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5029) 		.sched_policy   = policy,
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5030) 		.sched_priority = param->sched_priority,
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5031) 		.sched_nice	= PRIO_TO_NICE(p->static_prio),
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5032) 	};
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5033) 
c13db6b131a43 kernel/sched/core.c (Steven Rostedt             2014-07-23 11:28:26 -0400 5034) 	/* Fixup the legacy SCHED_RESET_ON_FORK hack. */
c13db6b131a43 kernel/sched/core.c (Steven Rostedt             2014-07-23 11:28:26 -0400 5035) 	if ((policy != SETPARAM_POLICY) && (policy & SCHED_RESET_ON_FORK)) {
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5036) 		attr.sched_flags |= SCHED_FLAG_RESET_ON_FORK;
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5037) 		policy &= ~SCHED_RESET_ON_FORK;
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5038) 		attr.sched_policy = policy;
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5039) 	}
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5040) 
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 5041) 	return __sched_setscheduler(p, &attr, check, true);
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5042) }
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5043) /**
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5044)  * sched_setscheduler - change the scheduling policy and/or RT priority of a thread.
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5045)  * @p: the task in question.
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5046)  * @policy: new policy.
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5047)  * @param: structure containing the new RT priority.
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5048)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5049)  * Return: 0 on success. An error code otherwise.
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5050)  *
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5051)  * NOTE that the task may be already dead.
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5052)  */
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5053) int sched_setscheduler(struct task_struct *p, int policy,
fe7de49f9d4e5 kernel/sched.c      (KOSAKI Motohiro            2010-10-20 16:01:12 -0700 5054) 		       const struct sched_param *param)
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5055) {
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5056) 	return _sched_setscheduler(p, policy, param, true);
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5057) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5058) EXPORT_SYMBOL_GPL(sched_setscheduler);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5059) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5060) int sched_setattr(struct task_struct *p, const struct sched_attr *attr)
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5061) {
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 5062) 	return __sched_setscheduler(p, attr, true, true);
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5063) }
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5064) EXPORT_SYMBOL_GPL(sched_setattr);
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5065) 
794a56ebd9a57 kernel/sched/core.c (Juri Lelli                 2017-12-04 11:23:20 +0100 5066) int sched_setattr_nocheck(struct task_struct *p, const struct sched_attr *attr)
794a56ebd9a57 kernel/sched/core.c (Juri Lelli                 2017-12-04 11:23:20 +0100 5067) {
794a56ebd9a57 kernel/sched/core.c (Juri Lelli                 2017-12-04 11:23:20 +0100 5068) 	return __sched_setscheduler(p, attr, false, true);
794a56ebd9a57 kernel/sched/core.c (Juri Lelli                 2017-12-04 11:23:20 +0100 5069) }
794a56ebd9a57 kernel/sched/core.c (Juri Lelli                 2017-12-04 11:23:20 +0100 5070) 
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5071) /**
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5072)  * sched_setscheduler_nocheck - change the scheduling policy and/or RT priority of a thread from kernelspace.
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5073)  * @p: the task in question.
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5074)  * @policy: new policy.
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5075)  * @param: structure containing the new RT priority.
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5076)  *
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5077)  * Just like sched_setscheduler, only don't bother checking if the
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5078)  * current context has permission.  For example, this is needed in
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5079)  * stop_machine(): we create temporary high priority worker threads,
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5080)  * but our caller might not have that capability.
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5081)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5082)  * Return: 0 on success. An error code otherwise.
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5083)  */
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5084) int sched_setscheduler_nocheck(struct task_struct *p, int policy,
fe7de49f9d4e5 kernel/sched.c      (KOSAKI Motohiro            2010-10-20 16:01:12 -0700 5085) 			       const struct sched_param *param)
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5086) {
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5087) 	return _sched_setscheduler(p, policy, param, false);
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5088) }
84778472e1b6a kernel/sched/core.c (Davidlohr Bueso            2015-09-02 01:28:44 -0700 5089) EXPORT_SYMBOL_GPL(sched_setscheduler_nocheck);
961ccddd59d62 kernel/sched.c      (Rusty Russell              2008-06-23 13:55:38 +1000 5090) 
95cdf3b799a48 kernel/sched.c      (Ingo Molnar                2005-09-10 00:26:11 -0700 5091) static int
95cdf3b799a48 kernel/sched.c      (Ingo Molnar                2005-09-10 00:26:11 -0700 5092) do_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5093) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5094) 	struct sched_param lparam;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5095) 	struct task_struct *p;
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 5096) 	int retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5097) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5098) 	if (!param || pid < 0)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5099) 		return -EINVAL;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5100) 	if (copy_from_user(&lparam, param, sizeof(struct sched_param)))
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5101) 		return -EFAULT;
5fe1d75f34974 kernel/sched.c      (Oleg Nesterov              2006-09-29 02:00:48 -0700 5102) 
5fe1d75f34974 kernel/sched.c      (Oleg Nesterov              2006-09-29 02:00:48 -0700 5103) 	rcu_read_lock();
5fe1d75f34974 kernel/sched.c      (Oleg Nesterov              2006-09-29 02:00:48 -0700 5104) 	retval = -ESRCH;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5105) 	p = find_process_by_pid(pid);
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 5106) 	if (likely(p))
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 5107) 		get_task_struct(p);
5fe1d75f34974 kernel/sched.c      (Oleg Nesterov              2006-09-29 02:00:48 -0700 5108) 	rcu_read_unlock();
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 5109) 
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 5110) 	if (likely(p)) {
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 5111) 		retval = sched_setscheduler(p, policy, &lparam);
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 5112) 		put_task_struct(p);
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 5113) 	}
710da3c8ea7df kernel/sched/core.c (Juri Lelli                 2019-07-19 16:00:00 +0200 5114) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5115) 	return retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5116) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5117) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5118) /*
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5119)  * Mimics kernel/events/core.c perf_copy_attr().
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5120)  */
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5121) static int sched_copy_attr(struct sched_attr __user *uattr, struct sched_attr *attr)
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5122) {
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5123) 	u32 size;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5124) 	int ret;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5125) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5126) 	/* Zero the full structure, so that a short copy will be nice: */
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5127) 	memset(attr, 0, sizeof(*attr));
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5128) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5129) 	ret = get_user(size, &uattr->size);
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5130) 	if (ret)
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5131) 		return ret;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5132) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5133) 	/* ABI compatibility quirk: */
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5134) 	if (!size)
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5135) 		size = SCHED_ATTR_SIZE_VER0;
dff3a85fecead kernel/sched/core.c (Aleksa Sarai               2019-10-01 11:10:54 +1000 5136) 	if (size < SCHED_ATTR_SIZE_VER0 || size > PAGE_SIZE)
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5137) 		goto err_size;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5138) 
dff3a85fecead kernel/sched/core.c (Aleksa Sarai               2019-10-01 11:10:54 +1000 5139) 	ret = copy_struct_from_user(attr, sizeof(*attr), uattr, size);
dff3a85fecead kernel/sched/core.c (Aleksa Sarai               2019-10-01 11:10:54 +1000 5140) 	if (ret) {
dff3a85fecead kernel/sched/core.c (Aleksa Sarai               2019-10-01 11:10:54 +1000 5141) 		if (ret == -E2BIG)
dff3a85fecead kernel/sched/core.c (Aleksa Sarai               2019-10-01 11:10:54 +1000 5142) 			goto err_size;
dff3a85fecead kernel/sched/core.c (Aleksa Sarai               2019-10-01 11:10:54 +1000 5143) 		return ret;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5144) 	}
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5145) 
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 5146) 	if ((attr->sched_flags & SCHED_FLAG_UTIL_CLAMP) &&
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 5147) 	    size < SCHED_ATTR_SIZE_VER1)
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 5148) 		return -EINVAL;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 5149) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5150) 	/*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5151) 	 * XXX: Do we want to be lenient like existing syscalls; or do we want
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5152) 	 * to be strict and return an error on out-of-bounds values?
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5153) 	 */
75e45d512f257 kernel/sched/core.c (Dongsheng Yang             2014-02-11 15:34:50 +0800 5154) 	attr->sched_nice = clamp(attr->sched_nice, MIN_NICE, MAX_NICE);
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5155) 
e78c7bca56dab kernel/sched/core.c (Michael Kerrisk            2014-05-09 16:54:28 +0200 5156) 	return 0;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5157) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5158) err_size:
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5159) 	put_user(sizeof(*attr), &uattr->size);
e78c7bca56dab kernel/sched/core.c (Michael Kerrisk            2014-05-09 16:54:28 +0200 5160) 	return -E2BIG;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5161) }
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5162) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5163) /**
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5164)  * sys_sched_setscheduler - set/change the scheduler policy and RT priority
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5165)  * @pid: the pid in question.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5166)  * @policy: new policy.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5167)  * @param: structure containing the new RT priority.
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5168)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5169)  * Return: 0 on success. An error code otherwise.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5170)  */
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5171) SYSCALL_DEFINE3(sched_setscheduler, pid_t, pid, int, policy, struct sched_param __user *, param)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5172) {
c21761f168894 kernel/sched.c      (Jason Baron                2006-01-18 17:43:03 -0800 5173) 	if (policy < 0)
c21761f168894 kernel/sched.c      (Jason Baron                2006-01-18 17:43:03 -0800 5174) 		return -EINVAL;
c21761f168894 kernel/sched.c      (Jason Baron                2006-01-18 17:43:03 -0800 5175) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5176) 	return do_sched_setscheduler(pid, policy, param);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5177) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5178) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5179) /**
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5180)  * sys_sched_setparam - set/change the RT priority of a thread
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5181)  * @pid: the pid in question.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5182)  * @param: structure containing the new RT priority.
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5183)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5184)  * Return: 0 on success. An error code otherwise.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5185)  */
5add95d4f7cf0 kernel/sched.c      (Heiko Carstens             2009-01-14 14:14:08 +0100 5186) SYSCALL_DEFINE2(sched_setparam, pid_t, pid, struct sched_param __user *, param)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5187) {
c13db6b131a43 kernel/sched/core.c (Steven Rostedt             2014-07-23 11:28:26 -0400 5188) 	return do_sched_setscheduler(pid, SETPARAM_POLICY, param);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5189) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5190) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5191) /**
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5192)  * sys_sched_setattr - same as above, but with extended sched_attr
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5193)  * @pid: the pid in question.
5778fccf361c9 kernel/sched/core.c (Juri Lelli                 2014-01-14 16:10:39 +0100 5194)  * @uattr: structure containing the extended parameters.
db66d756c74ac kernel/sched/core.c (Masanari Iida              2014-04-18 01:59:15 +0900 5195)  * @flags: for future extension.
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5196)  */
6d35ab48090b1 kernel/sched/core.c (Peter Zijlstra             2014-02-14 17:19:29 +0100 5197) SYSCALL_DEFINE3(sched_setattr, pid_t, pid, struct sched_attr __user *, uattr,
6d35ab48090b1 kernel/sched/core.c (Peter Zijlstra             2014-02-14 17:19:29 +0100 5198) 			       unsigned int, flags)
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5199) {
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5200) 	struct sched_attr attr;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5201) 	struct task_struct *p;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5202) 	int retval;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5203) 
6d35ab48090b1 kernel/sched/core.c (Peter Zijlstra             2014-02-14 17:19:29 +0100 5204) 	if (!uattr || pid < 0 || flags)
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5205) 		return -EINVAL;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5206) 
143cf23df25b7 kernel/sched/core.c (Michael Kerrisk            2014-05-09 16:54:15 +0200 5207) 	retval = sched_copy_attr(uattr, &attr);
143cf23df25b7 kernel/sched/core.c (Michael Kerrisk            2014-05-09 16:54:15 +0200 5208) 	if (retval)
143cf23df25b7 kernel/sched/core.c (Michael Kerrisk            2014-05-09 16:54:15 +0200 5209) 		return retval;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5210) 
b14ed2c273f8a kernel/sched/core.c (Richard Weinberger         2014-06-02 22:38:34 +0200 5211) 	if ((int)attr.sched_policy < 0)
dbdb22754fde6 kernel/sched/core.c (Peter Zijlstra             2014-05-09 10:49:03 +0200 5212) 		return -EINVAL;
1d6362fa0cfc8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:06 +0100 5213) 	if (attr.sched_flags & SCHED_FLAG_KEEP_POLICY)
1d6362fa0cfc8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:06 +0100 5214) 		attr.sched_policy = SETPARAM_POLICY;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5215) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5216) 	rcu_read_lock();
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5217) 	retval = -ESRCH;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5218) 	p = find_process_by_pid(pid);
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 5219) 	if (likely(p))
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 5220) 		get_task_struct(p);
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5221) 	rcu_read_unlock();
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5222) 
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 5223) 	if (likely(p)) {
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 5224) 		retval = sched_setattr(p, &attr);
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 5225) 		put_task_struct(p);
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 5226) 	}
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 5227) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5228) 	return retval;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5229) }
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5230) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5231) /**
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5232)  * sys_sched_getscheduler - get the policy (scheduling class) of a thread
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5233)  * @pid: the pid in question.
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5234)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5235)  * Return: On success, the policy of the thread. Otherwise, a negative error
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5236)  * code.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5237)  */
5add95d4f7cf0 kernel/sched.c      (Heiko Carstens             2009-01-14 14:14:08 +0100 5238) SYSCALL_DEFINE1(sched_getscheduler, pid_t, pid)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5239) {
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 5240) 	struct task_struct *p;
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 5241) 	int retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5242) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5243) 	if (pid < 0)
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 5244) 		return -EINVAL;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5245) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5246) 	retval = -ESRCH;
5fe85be081edf kernel/sched.c      (Thomas Gleixner            2009-12-09 10:14:58 +0000 5247) 	rcu_read_lock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5248) 	p = find_process_by_pid(pid);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5249) 	if (p) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5250) 		retval = security_task_getscheduler(p);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5251) 		if (!retval)
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 5252) 			retval = p->policy
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 5253) 				| (p->sched_reset_on_fork ? SCHED_RESET_ON_FORK : 0);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5254) 	}
5fe85be081edf kernel/sched.c      (Thomas Gleixner            2009-12-09 10:14:58 +0000 5255) 	rcu_read_unlock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5256) 	return retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5257) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5258) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5259) /**
ca94c442535a4 kernel/sched.c      (Lennart Poettering         2009-06-15 17:17:47 +0200 5260)  * sys_sched_getparam - get the RT priority of a thread
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5261)  * @pid: the pid in question.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5262)  * @param: structure containing the RT priority.
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5263)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5264)  * Return: On success, 0 and the RT priority is in @param. Otherwise, an error
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5265)  * code.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5266)  */
5add95d4f7cf0 kernel/sched.c      (Heiko Carstens             2009-01-14 14:14:08 +0100 5267) SYSCALL_DEFINE2(sched_getparam, pid_t, pid, struct sched_param __user *, param)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5268) {
ce5f7f8200ca2 kernel/sched/core.c (Peter Zijlstra             2014-05-12 22:50:34 +0200 5269) 	struct sched_param lp = { .sched_priority = 0 };
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 5270) 	struct task_struct *p;
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 5271) 	int retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5272) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5273) 	if (!param || pid < 0)
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 5274) 		return -EINVAL;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5275) 
5fe85be081edf kernel/sched.c      (Thomas Gleixner            2009-12-09 10:14:58 +0000 5276) 	rcu_read_lock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5277) 	p = find_process_by_pid(pid);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5278) 	retval = -ESRCH;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5279) 	if (!p)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5280) 		goto out_unlock;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5281) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5282) 	retval = security_task_getscheduler(p);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5283) 	if (retval)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5284) 		goto out_unlock;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5285) 
ce5f7f8200ca2 kernel/sched/core.c (Peter Zijlstra             2014-05-12 22:50:34 +0200 5286) 	if (task_has_rt_policy(p))
ce5f7f8200ca2 kernel/sched/core.c (Peter Zijlstra             2014-05-12 22:50:34 +0200 5287) 		lp.sched_priority = p->rt_priority;
5fe85be081edf kernel/sched.c      (Thomas Gleixner            2009-12-09 10:14:58 +0000 5288) 	rcu_read_unlock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5289) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5290) 	/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5291) 	 * This one might sleep, we cannot do it with a spinlock held ...
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5292) 	 */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5293) 	retval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5294) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5295) 	return retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5296) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5297) out_unlock:
5fe85be081edf kernel/sched.c      (Thomas Gleixner            2009-12-09 10:14:58 +0000 5298) 	rcu_read_unlock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5299) 	return retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5300) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5301) 
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5302) /*
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5303)  * Copy the kernel size attribute structure (which might be larger
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5304)  * than what user-space knows about) to user-space.
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5305)  *
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5306)  * Note that all cases are valid: user-space buffer can be larger or
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5307)  * smaller than the kernel-space buffer. The usual case is that both
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5308)  * have the same size.
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5309)  */
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5310) static int
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5311) sched_attr_copy_to_user(struct sched_attr __user *uattr,
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5312) 			struct sched_attr *kattr,
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5313) 			unsigned int usize)
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5314) {
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5315) 	unsigned int ksize = sizeof(*kattr);
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5316) 
96d4f267e40f9 kernel/sched/core.c (Linus Torvalds             2019-01-03 18:57:57 -0800 5317) 	if (!access_ok(uattr, usize))
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5318) 		return -EFAULT;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5319) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5320) 	/*
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5321) 	 * sched_getattr() ABI forwards and backwards compatibility:
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5322) 	 *
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5323) 	 * If usize == ksize then we just copy everything to user-space and all is good.
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5324) 	 *
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5325) 	 * If usize < ksize then we only copy as much as user-space has space for,
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5326) 	 * this keeps ABI compatibility as well. We skip the rest.
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5327) 	 *
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5328) 	 * If usize > ksize then user-space is using a newer version of the ABI,
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5329) 	 * which part the kernel doesn't know about. Just ignore it - tooling can
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5330) 	 * detect the kernel's knowledge of attributes from the attr->size value
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5331) 	 * which is set to ksize in this case.
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5332) 	 */
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5333) 	kattr->size = min(usize, ksize);
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5334) 
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5335) 	if (copy_to_user(uattr, kattr, kattr->size))
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5336) 		return -EFAULT;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5337) 
22400674945c5 kernel/sched/core.c (Michael Kerrisk            2014-05-09 16:54:33 +0200 5338) 	return 0;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5339) }
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5340) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5341) /**
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 5342)  * sys_sched_getattr - similar to sched_getparam, but with sched_attr
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5343)  * @pid: the pid in question.
5778fccf361c9 kernel/sched/core.c (Juri Lelli                 2014-01-14 16:10:39 +0100 5344)  * @uattr: structure containing the extended parameters.
dff3a85fecead kernel/sched/core.c (Aleksa Sarai               2019-10-01 11:10:54 +1000 5345)  * @usize: sizeof(attr) for fwd/bwd comp.
db66d756c74ac kernel/sched/core.c (Masanari Iida              2014-04-18 01:59:15 +0900 5346)  * @flags: for future extension.
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5347)  */
6d35ab48090b1 kernel/sched/core.c (Peter Zijlstra             2014-02-14 17:19:29 +0100 5348) SYSCALL_DEFINE4(sched_getattr, pid_t, pid, struct sched_attr __user *, uattr,
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5349) 		unsigned int, usize, unsigned int, flags)
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5350) {
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5351) 	struct sched_attr kattr = { };
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5352) 	struct task_struct *p;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5353) 	int retval;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5354) 
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5355) 	if (!uattr || pid < 0 || usize > PAGE_SIZE ||
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5356) 	    usize < SCHED_ATTR_SIZE_VER0 || flags)
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5357) 		return -EINVAL;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5358) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5359) 	rcu_read_lock();
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5360) 	p = find_process_by_pid(pid);
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5361) 	retval = -ESRCH;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5362) 	if (!p)
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5363) 		goto out_unlock;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5364) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5365) 	retval = security_task_getscheduler(p);
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5366) 	if (retval)
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5367) 		goto out_unlock;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5368) 
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5369) 	kattr.sched_policy = p->policy;
7479f3c9cf67e kernel/sched/core.c (Peter Zijlstra             2014-01-15 17:05:04 +0100 5370) 	if (p->sched_reset_on_fork)
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5371) 		kattr.sched_flags |= SCHED_FLAG_RESET_ON_FORK;
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 5372) 	if (task_has_dl_policy(p))
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5373) 		__getparam_dl(p, &kattr);
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 5374) 	else if (task_has_rt_policy(p))
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5375) 		kattr.sched_priority = p->rt_priority;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5376) 	else
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5377) 		kattr.sched_nice = task_nice(p);
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5378) 
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 5379) #ifdef CONFIG_UCLAMP_TASK
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5380) 	kattr.sched_util_min = p->uclamp_req[UCLAMP_MIN].value;
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5381) 	kattr.sched_util_max = p->uclamp_req[UCLAMP_MAX].value;
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 5382) #endif
a509a7cd79747 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:07 +0100 5383) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5384) 	rcu_read_unlock();
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5385) 
1251201c0d34f kernel/sched/core.c (Ingo Molnar                2019-09-04 09:55:32 +0200 5386) 	return sched_attr_copy_to_user(uattr, &kattr, usize);
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5387) 
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5388) out_unlock:
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5389) 	rcu_read_unlock();
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5390) 	return retval;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5391) }
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 5392) 
96f874e26428a kernel/sched.c      (Rusty Russell              2008-11-25 02:35:14 +1030 5393) long sched_setaffinity(pid_t pid, const struct cpumask *in_mask)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5394) {
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5395) 	cpumask_var_t cpus_allowed, new_mask;
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 5396) 	struct task_struct *p;
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 5397) 	int retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5398) 
23f5d14251962 kernel/sched.c      (Thomas Gleixner            2009-12-09 10:15:01 +0000 5399) 	rcu_read_lock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5400) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5401) 	p = find_process_by_pid(pid);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5402) 	if (!p) {
23f5d14251962 kernel/sched.c      (Thomas Gleixner            2009-12-09 10:15:01 +0000 5403) 		rcu_read_unlock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5404) 		return -ESRCH;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5405) 	}
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5406) 
23f5d14251962 kernel/sched.c      (Thomas Gleixner            2009-12-09 10:15:01 +0000 5407) 	/* Prevent p going away */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5408) 	get_task_struct(p);
23f5d14251962 kernel/sched.c      (Thomas Gleixner            2009-12-09 10:15:01 +0000 5409) 	rcu_read_unlock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5410) 
14a40ffccd616 kernel/sched/core.c (Tejun Heo                  2013-03-19 13:45:20 -0700 5411) 	if (p->flags & PF_NO_SETAFFINITY) {
14a40ffccd616 kernel/sched/core.c (Tejun Heo                  2013-03-19 13:45:20 -0700 5412) 		retval = -EINVAL;
14a40ffccd616 kernel/sched/core.c (Tejun Heo                  2013-03-19 13:45:20 -0700 5413) 		goto out_put_task;
14a40ffccd616 kernel/sched/core.c (Tejun Heo                  2013-03-19 13:45:20 -0700 5414) 	}
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5415) 	if (!alloc_cpumask_var(&cpus_allowed, GFP_KERNEL)) {
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5416) 		retval = -ENOMEM;
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5417) 		goto out_put_task;
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5418) 	}
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5419) 	if (!alloc_cpumask_var(&new_mask, GFP_KERNEL)) {
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5420) 		retval = -ENOMEM;
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5421) 		goto out_free_cpus_allowed;
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5422) 	}
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5423) 	retval = -EPERM;
4c44aaafa8108 kernel/sched/core.c (Eric W. Biederman          2012-07-26 05:05:21 -0700 5424) 	if (!check_same_owner(p)) {
4c44aaafa8108 kernel/sched/core.c (Eric W. Biederman          2012-07-26 05:05:21 -0700 5425) 		rcu_read_lock();
4c44aaafa8108 kernel/sched/core.c (Eric W. Biederman          2012-07-26 05:05:21 -0700 5426) 		if (!ns_capable(__task_cred(p)->user_ns, CAP_SYS_NICE)) {
4c44aaafa8108 kernel/sched/core.c (Eric W. Biederman          2012-07-26 05:05:21 -0700 5427) 			rcu_read_unlock();
16303ab2fe214 kernel/sched/core.c (Kirill Tkhai               2014-09-22 22:36:30 +0400 5428) 			goto out_free_new_mask;
4c44aaafa8108 kernel/sched/core.c (Eric W. Biederman          2012-07-26 05:05:21 -0700 5429) 		}
4c44aaafa8108 kernel/sched/core.c (Eric W. Biederman          2012-07-26 05:05:21 -0700 5430) 		rcu_read_unlock();
4c44aaafa8108 kernel/sched/core.c (Eric W. Biederman          2012-07-26 05:05:21 -0700 5431) 	}
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5432) 
b0ae198113750 kernel/sched.c      (KOSAKI Motohiro            2010-10-15 04:21:18 +0900 5433) 	retval = security_task_setscheduler(p);
e7834f8fccd79 kernel/sched.c      (David Quigley              2006-06-23 02:03:59 -0700 5434) 	if (retval)
16303ab2fe214 kernel/sched/core.c (Kirill Tkhai               2014-09-22 22:36:30 +0400 5435) 		goto out_free_new_mask;
e7834f8fccd79 kernel/sched.c      (David Quigley              2006-06-23 02:03:59 -0700 5436) 
e4099a5e92943 kernel/sched/core.c (Peter Zijlstra             2013-12-17 10:03:34 +0100 5437) 
e4099a5e92943 kernel/sched/core.c (Peter Zijlstra             2013-12-17 10:03:34 +0100 5438) 	cpuset_cpus_allowed(p, cpus_allowed);
e4099a5e92943 kernel/sched/core.c (Peter Zijlstra             2013-12-17 10:03:34 +0100 5439) 	cpumask_and(new_mask, in_mask, cpus_allowed);
e4099a5e92943 kernel/sched/core.c (Peter Zijlstra             2013-12-17 10:03:34 +0100 5440) 
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 5441) 	/*
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 5442) 	 * Since bandwidth control happens on root_domain basis,
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 5443) 	 * if admission test is enabled, we only admit -deadline
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 5444) 	 * tasks allowed to run on all the CPUs in the task's
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 5445) 	 * root_domain.
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 5446) 	 */
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 5447) #ifdef CONFIG_SMP
f1e3a0932f3a9 kernel/sched/core.c (Kirill Tkhai               2014-09-22 22:36:36 +0400 5448) 	if (task_has_dl_policy(p) && dl_bandwidth_enabled()) {
f1e3a0932f3a9 kernel/sched/core.c (Kirill Tkhai               2014-09-22 22:36:36 +0400 5449) 		rcu_read_lock();
f1e3a0932f3a9 kernel/sched/core.c (Kirill Tkhai               2014-09-22 22:36:36 +0400 5450) 		if (!cpumask_subset(task_rq(p)->rd->span, new_mask)) {
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 5451) 			retval = -EBUSY;
f1e3a0932f3a9 kernel/sched/core.c (Kirill Tkhai               2014-09-22 22:36:36 +0400 5452) 			rcu_read_unlock();
16303ab2fe214 kernel/sched/core.c (Kirill Tkhai               2014-09-22 22:36:30 +0400 5453) 			goto out_free_new_mask;
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 5454) 		}
f1e3a0932f3a9 kernel/sched/core.c (Kirill Tkhai               2014-09-22 22:36:36 +0400 5455) 		rcu_read_unlock();
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 5456) 	}
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 5457) #endif
4924627423d5e kernel/sched.c      (Peter Zijlstra             2010-10-17 21:46:10 +0200 5458) again:
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 5459) 	retval = __set_cpus_allowed_ptr(p, new_mask, true);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5460) 
8707d8b8c0cbd kernel/sched.c      (Paul Menage                2007-10-18 23:40:22 -0700 5461) 	if (!retval) {
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5462) 		cpuset_cpus_allowed(p, cpus_allowed);
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5463) 		if (!cpumask_subset(new_mask, cpus_allowed)) {
8707d8b8c0cbd kernel/sched.c      (Paul Menage                2007-10-18 23:40:22 -0700 5464) 			/*
8707d8b8c0cbd kernel/sched.c      (Paul Menage                2007-10-18 23:40:22 -0700 5465) 			 * We must have raced with a concurrent cpuset
8707d8b8c0cbd kernel/sched.c      (Paul Menage                2007-10-18 23:40:22 -0700 5466) 			 * update. Just reset the cpus_allowed to the
8707d8b8c0cbd kernel/sched.c      (Paul Menage                2007-10-18 23:40:22 -0700 5467) 			 * cpuset's cpus_allowed
8707d8b8c0cbd kernel/sched.c      (Paul Menage                2007-10-18 23:40:22 -0700 5468) 			 */
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5469) 			cpumask_copy(new_mask, cpus_allowed);
8707d8b8c0cbd kernel/sched.c      (Paul Menage                2007-10-18 23:40:22 -0700 5470) 			goto again;
8707d8b8c0cbd kernel/sched.c      (Paul Menage                2007-10-18 23:40:22 -0700 5471) 		}
8707d8b8c0cbd kernel/sched.c      (Paul Menage                2007-10-18 23:40:22 -0700 5472) 	}
16303ab2fe214 kernel/sched/core.c (Kirill Tkhai               2014-09-22 22:36:30 +0400 5473) out_free_new_mask:
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5474) 	free_cpumask_var(new_mask);
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5475) out_free_cpus_allowed:
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5476) 	free_cpumask_var(cpus_allowed);
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5477) out_put_task:
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5478) 	put_task_struct(p);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5479) 	return retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5480) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5481) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5482) static int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,
96f874e26428a kernel/sched.c      (Rusty Russell              2008-11-25 02:35:14 +1030 5483) 			     struct cpumask *new_mask)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5484) {
96f874e26428a kernel/sched.c      (Rusty Russell              2008-11-25 02:35:14 +1030 5485) 	if (len < cpumask_size())
96f874e26428a kernel/sched.c      (Rusty Russell              2008-11-25 02:35:14 +1030 5486) 		cpumask_clear(new_mask);
96f874e26428a kernel/sched.c      (Rusty Russell              2008-11-25 02:35:14 +1030 5487) 	else if (len > cpumask_size())
96f874e26428a kernel/sched.c      (Rusty Russell              2008-11-25 02:35:14 +1030 5488) 		len = cpumask_size();
96f874e26428a kernel/sched.c      (Rusty Russell              2008-11-25 02:35:14 +1030 5489) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5490) 	return copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5491) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5492) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5493) /**
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5494)  * sys_sched_setaffinity - set the CPU affinity of a process
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5495)  * @pid: pid of the process
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5496)  * @len: length in bytes of the bitmask pointed to by user_mask_ptr
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5497)  * @user_mask_ptr: user-space pointer to the new CPU mask
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5498)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5499)  * Return: 0 on success. An error code otherwise.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5500)  */
5add95d4f7cf0 kernel/sched.c      (Heiko Carstens             2009-01-14 14:14:08 +0100 5501) SYSCALL_DEFINE3(sched_setaffinity, pid_t, pid, unsigned int, len,
5add95d4f7cf0 kernel/sched.c      (Heiko Carstens             2009-01-14 14:14:08 +0100 5502) 		unsigned long __user *, user_mask_ptr)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5503) {
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5504) 	cpumask_var_t new_mask;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5505) 	int retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5506) 
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5507) 	if (!alloc_cpumask_var(&new_mask, GFP_KERNEL))
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5508) 		return -ENOMEM;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5509) 
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5510) 	retval = get_user_cpu_mask(user_mask_ptr, len, new_mask);
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5511) 	if (retval == 0)
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5512) 		retval = sched_setaffinity(pid, new_mask);
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5513) 	free_cpumask_var(new_mask);
5a16f3d30ca4e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5514) 	return retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5515) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5516) 
96f874e26428a kernel/sched.c      (Rusty Russell              2008-11-25 02:35:14 +1030 5517) long sched_getaffinity(pid_t pid, struct cpumask *mask)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5518) {
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 5519) 	struct task_struct *p;
3160568371da4 kernel/sched.c      (Thomas Gleixner            2009-12-08 20:24:16 +0000 5520) 	unsigned long flags;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5521) 	int retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5522) 
23f5d14251962 kernel/sched.c      (Thomas Gleixner            2009-12-09 10:15:01 +0000 5523) 	rcu_read_lock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5524) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5525) 	retval = -ESRCH;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5526) 	p = find_process_by_pid(pid);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5527) 	if (!p)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5528) 		goto out_unlock;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5529) 
e7834f8fccd79 kernel/sched.c      (David Quigley              2006-06-23 02:03:59 -0700 5530) 	retval = security_task_getscheduler(p);
e7834f8fccd79 kernel/sched.c      (David Quigley              2006-06-23 02:03:59 -0700 5531) 	if (retval)
e7834f8fccd79 kernel/sched.c      (David Quigley              2006-06-23 02:03:59 -0700 5532) 		goto out_unlock;
e7834f8fccd79 kernel/sched.c      (David Quigley              2006-06-23 02:03:59 -0700 5533) 
013fdb8086aca kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:45 +0200 5534) 	raw_spin_lock_irqsave(&p->pi_lock, flags);
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 5535) 	cpumask_and(mask, &p->cpus_mask, cpu_active_mask);
013fdb8086aca kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:45 +0200 5536) 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5537) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5538) out_unlock:
23f5d14251962 kernel/sched.c      (Thomas Gleixner            2009-12-09 10:15:01 +0000 5539) 	rcu_read_unlock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5540) 
9531b62f5ebf2 kernel/sched.c      (Ulrich Drepper             2007-08-09 11:16:46 +0200 5541) 	return retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5542) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5543) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5544) /**
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5545)  * sys_sched_getaffinity - get the CPU affinity of a process
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5546)  * @pid: pid of the process
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5547)  * @len: length in bytes of the bitmask pointed to by user_mask_ptr
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5548)  * @user_mask_ptr: user-space pointer to hold the current CPU mask
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5549)  *
599b4840b0ea4 kernel/sched/core.c (Zev Weiss                  2016-06-26 16:13:23 -0500 5550)  * Return: size of CPU mask copied to user_mask_ptr on success. An
599b4840b0ea4 kernel/sched/core.c (Zev Weiss                  2016-06-26 16:13:23 -0500 5551)  * error code otherwise.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5552)  */
5add95d4f7cf0 kernel/sched.c      (Heiko Carstens             2009-01-14 14:14:08 +0100 5553) SYSCALL_DEFINE3(sched_getaffinity, pid_t, pid, unsigned int, len,
5add95d4f7cf0 kernel/sched.c      (Heiko Carstens             2009-01-14 14:14:08 +0100 5554) 		unsigned long __user *, user_mask_ptr)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5555) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5556) 	int ret;
f17c860760927 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5557) 	cpumask_var_t mask;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5558) 
84fba5ec91f11 kernel/sched.c      (Anton Blanchard            2010-04-06 17:02:19 +1000 5559) 	if ((len * BITS_PER_BYTE) < nr_cpu_ids)
cd3d8031eb431 kernel/sched.c      (KOSAKI Motohiro            2010-03-12 16:15:36 +0900 5560) 		return -EINVAL;
cd3d8031eb431 kernel/sched.c      (KOSAKI Motohiro            2010-03-12 16:15:36 +0900 5561) 	if (len & (sizeof(unsigned long)-1))
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5562) 		return -EINVAL;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5563) 
f17c860760927 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5564) 	if (!alloc_cpumask_var(&mask, GFP_KERNEL))
f17c860760927 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5565) 		return -ENOMEM;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5566) 
f17c860760927 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5567) 	ret = sched_getaffinity(pid, mask);
f17c860760927 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5568) 	if (ret == 0) {
4de373a12f3c5 kernel/sched/core.c (Alexey Dobriyan            2018-02-06 15:39:37 -0800 5569) 		unsigned int retlen = min(len, cpumask_size());
cd3d8031eb431 kernel/sched.c      (KOSAKI Motohiro            2010-03-12 16:15:36 +0900 5570) 
cd3d8031eb431 kernel/sched.c      (KOSAKI Motohiro            2010-03-12 16:15:36 +0900 5571) 		if (copy_to_user(user_mask_ptr, mask, retlen))
f17c860760927 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5572) 			ret = -EFAULT;
f17c860760927 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5573) 		else
cd3d8031eb431 kernel/sched.c      (KOSAKI Motohiro            2010-03-12 16:15:36 +0900 5574) 			ret = retlen;
f17c860760927 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5575) 	}
f17c860760927 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5576) 	free_cpumask_var(mask);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5577) 
f17c860760927 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 5578) 	return ret;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5579) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5580) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5581) /**
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5582)  * sys_sched_yield - yield the current processor to other threads.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5583)  *
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 5584)  * This function yields the current CPU to other tasks. If there are no
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 5585)  * other threads running on this CPU then this function will return.
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5586)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5587)  * Return: 0.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5588)  */
7d4dd4f159b94 kernel/sched/core.c (Dominik Brodowski          2018-03-14 22:40:35 +0100 5589) static void do_sched_yield(void)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5590) {
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 5591) 	struct rq_flags rf;
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 5592) 	struct rq *rq;
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 5593) 
246b3b3342c9b kernel/sched/core.c (Johannes Weiner            2018-10-26 15:06:23 -0700 5594) 	rq = this_rq_lock_irq(&rf);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5595) 
ae92882e5646d kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:24 -0500 5596) 	schedstat_inc(rq->yld_count);
4530d7ab0fb8d kernel/sched.c      (Dmitry Adamushko           2007-10-15 17:00:08 +0200 5597) 	current->sched_class->yield_task(rq);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5598) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5599) 	/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5600) 	 * Since we are going to call schedule() anyway, there's
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5601) 	 * no need to preempt or enable interrupts:
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5602) 	 */
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 5603) 	preempt_disable();
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 5604) 	rq_unlock(rq, &rf);
ba74c1448f127 kernel/sched/core.c (Thomas Gleixner            2011-03-21 13:32:17 +0100 5605) 	sched_preempt_enable_no_resched();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5606) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5607) 	schedule();
7d4dd4f159b94 kernel/sched/core.c (Dominik Brodowski          2018-03-14 22:40:35 +0100 5608) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5609) 
7d4dd4f159b94 kernel/sched/core.c (Dominik Brodowski          2018-03-14 22:40:35 +0100 5610) SYSCALL_DEFINE0(sched_yield)
7d4dd4f159b94 kernel/sched/core.c (Dominik Brodowski          2018-03-14 22:40:35 +0100 5611) {
7d4dd4f159b94 kernel/sched/core.c (Dominik Brodowski          2018-03-14 22:40:35 +0100 5612) 	do_sched_yield();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5613) 	return 0;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5614) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5615) 
c1a280b68d4e6 kernel/sched/core.c (Thomas Gleixner            2019-07-26 23:19:37 +0200 5616) #ifndef CONFIG_PREEMPTION
02b67cc3ba36b kernel/sched.c      (Herbert Xu                 2008-01-25 21:08:28 +0100 5617) int __sched _cond_resched(void)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5618) {
fe32d3cd5e8eb kernel/sched/core.c (Konstantin Khlebnikov      2015-07-15 12:52:04 +0300 5619) 	if (should_resched(0)) {
a18b5d0181923 kernel/sched/core.c (Frederic Weisbecker        2015-01-22 18:08:04 +0100 5620) 		preempt_schedule_common();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5621) 		return 1;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5622) 	}
f79c3ad618962 kernel/sched/core.c (Paul E. McKenney           2016-11-30 06:24:30 -0800 5623) 	rcu_all_qs();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5624) 	return 0;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5625) }
02b67cc3ba36b kernel/sched.c      (Herbert Xu                 2008-01-25 21:08:28 +0100 5626) EXPORT_SYMBOL(_cond_resched);
35a773a07926a kernel/sched/core.c (Peter Zijlstra             2016-09-19 12:57:53 +0200 5627) #endif
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5628) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5629) /*
613afbf83298e kernel/sched.c      (Frederic Weisbecker        2009-07-16 15:44:29 +0200 5630)  * __cond_resched_lock() - if a reschedule is pending, drop the given lock,
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5631)  * call schedule, and on return reacquire the lock.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5632)  *
c1a280b68d4e6 kernel/sched/core.c (Thomas Gleixner            2019-07-26 23:19:37 +0200 5633)  * This works OK both with and without CONFIG_PREEMPTION. We do strange low-level
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5634)  * operations here to prevent schedule() from being called twice (once via
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5635)  * spin_unlock(), once by hand).
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5636)  */
613afbf83298e kernel/sched.c      (Frederic Weisbecker        2009-07-16 15:44:29 +0200 5637) int __cond_resched_lock(spinlock_t *lock)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5638) {
fe32d3cd5e8eb kernel/sched/core.c (Konstantin Khlebnikov      2015-07-15 12:52:04 +0300 5639) 	int resched = should_resched(PREEMPT_LOCK_OFFSET);
6df3cecbb9534 kernel/sched.c      (Jan Kara                   2005-06-13 15:52:32 -0700 5640) 	int ret = 0;
6df3cecbb9534 kernel/sched.c      (Jan Kara                   2005-06-13 15:52:32 -0700 5641) 
f607c66857748 kernel/sched.c      (Peter Zijlstra             2009-07-20 19:16:29 +0200 5642) 	lockdep_assert_held(lock);
f607c66857748 kernel/sched.c      (Peter Zijlstra             2009-07-20 19:16:29 +0200 5643) 
4a81e8328d379 kernel/sched/core.c (Paul E. McKenney           2014-06-20 16:49:01 -0700 5644) 	if (spin_needbreak(lock) || resched) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5645) 		spin_unlock(lock);
d86ee4809d032 kernel/sched.c      (Peter Zijlstra             2009-07-10 14:57:57 +0200 5646) 		if (resched)
a18b5d0181923 kernel/sched/core.c (Frederic Weisbecker        2015-01-22 18:08:04 +0100 5647) 			preempt_schedule_common();
95c354fe9f7d6 kernel/sched.c      (Nick Piggin                2008-01-30 13:31:20 +0100 5648) 		else
95c354fe9f7d6 kernel/sched.c      (Nick Piggin                2008-01-30 13:31:20 +0100 5649) 			cpu_relax();
6df3cecbb9534 kernel/sched.c      (Jan Kara                   2005-06-13 15:52:32 -0700 5650) 		ret = 1;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5651) 		spin_lock(lock);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5652) 	}
6df3cecbb9534 kernel/sched.c      (Jan Kara                   2005-06-13 15:52:32 -0700 5653) 	return ret;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5654) }
613afbf83298e kernel/sched.c      (Frederic Weisbecker        2009-07-16 15:44:29 +0200 5655) EXPORT_SYMBOL(__cond_resched_lock);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5656) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5657) /**
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5658)  * yield - yield the current processor to other threads.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5659)  *
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5660)  * Do not ever use this function, there's a 99% chance you're doing it wrong.
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5661)  *
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5662)  * The scheduler is at all times free to pick the calling task as the most
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5663)  * eligible task to run, if removing the yield() call from your code breaks
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5664)  * it, its already broken.
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5665)  *
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5666)  * Typical broken usage is:
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5667)  *
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5668)  * while (!event)
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 5669)  *	yield();
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5670)  *
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5671)  * where one assumes that yield() will let 'the other' process run that will
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5672)  * make event true. If the current task is a SCHED_FIFO task that will never
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5673)  * happen. Never use yield() as a progress guarantee!!
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5674)  *
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5675)  * If you want to use yield() to wait for something, use wait_event().
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5676)  * If you want to use yield() to be 'nice' for others, use cond_resched().
8e3fabfde445a kernel/sched/core.c (Peter Zijlstra             2012-03-06 18:54:26 +0100 5677)  * If you still want to use yield(), do not!
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5678)  */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5679) void __sched yield(void)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5680) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5681) 	set_current_state(TASK_RUNNING);
7d4dd4f159b94 kernel/sched/core.c (Dominik Brodowski          2018-03-14 22:40:35 +0100 5682) 	do_sched_yield();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5683) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5684) EXPORT_SYMBOL(yield);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5685) 
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5686) /**
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5687)  * yield_to - yield the current processor to another thread in
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5688)  * your thread group, or accelerate that thread toward the
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5689)  * processor it's on.
16addf954d395 kernel/sched.c      (Randy Dunlap               2011-03-18 09:34:53 -0700 5690)  * @p: target task
16addf954d395 kernel/sched.c      (Randy Dunlap               2011-03-18 09:34:53 -0700 5691)  * @preempt: whether task preemption is allowed or not
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5692)  *
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5693)  * It's the caller's job to ensure that the target task struct
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5694)  * can't go away on us before we can do any checks.
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5695)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5696)  * Return:
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5697)  *	true (>0) if we indeed boosted the target task.
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5698)  *	false (0) if we failed to boost the target.
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5699)  *	-ESRCH if there's no task to yield to.
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5700)  */
fa93384f40dee kernel/sched/core.c (Dan Carpenter              2014-05-23 13:20:42 +0300 5701) int __sched yield_to(struct task_struct *p, bool preempt)
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5702) {
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5703) 	struct task_struct *curr = current;
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5704) 	struct rq *rq, *p_rq;
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5705) 	unsigned long flags;
c3c186403c6ab kernel/sched/core.c (Dan Carpenter              2013-02-05 14:37:51 +0300 5706) 	int yielded = 0;
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5707) 
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5708) 	local_irq_save(flags);
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5709) 	rq = this_rq();
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5710) 
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5711) again:
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5712) 	p_rq = task_rq(p);
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5713) 	/*
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5714) 	 * If we're the only runnable task on the rq and target rq also
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5715) 	 * has only one task, there's absolutely no point in yielding.
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5716) 	 */
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5717) 	if (rq->nr_running == 1 && p_rq->nr_running == 1) {
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5718) 		yielded = -ESRCH;
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5719) 		goto out_irq;
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5720) 	}
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5721) 
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5722) 	double_rq_lock(rq, p_rq);
39e24d8ffbb6a kernel/sched/core.c (Shigeru Yoshida            2013-11-23 18:38:01 +0900 5723) 	if (task_rq(p) != p_rq) {
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5724) 		double_rq_unlock(rq, p_rq);
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5725) 		goto again;
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5726) 	}
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5727) 
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5728) 	if (!curr->sched_class->yield_to_task)
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5729) 		goto out_unlock;
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5730) 
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5731) 	if (curr->sched_class != p->sched_class)
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5732) 		goto out_unlock;
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5733) 
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5734) 	if (task_running(p_rq, p) || p->state)
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5735) 		goto out_unlock;
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5736) 
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5737) 	yielded = curr->sched_class->yield_to_task(rq, p, preempt);
6d1cafd8b56ea kernel/sched.c      (Venkatesh Pallipadi        2011-03-01 16:28:21 -0800 5738) 	if (yielded) {
ae92882e5646d kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:24 -0500 5739) 		schedstat_inc(rq->yld_count);
6d1cafd8b56ea kernel/sched.c      (Venkatesh Pallipadi        2011-03-01 16:28:21 -0800 5740) 		/*
6d1cafd8b56ea kernel/sched.c      (Venkatesh Pallipadi        2011-03-01 16:28:21 -0800 5741) 		 * Make p's CPU reschedule; pick_next_entity takes care of
6d1cafd8b56ea kernel/sched.c      (Venkatesh Pallipadi        2011-03-01 16:28:21 -0800 5742) 		 * fairness.
6d1cafd8b56ea kernel/sched.c      (Venkatesh Pallipadi        2011-03-01 16:28:21 -0800 5743) 		 */
6d1cafd8b56ea kernel/sched.c      (Venkatesh Pallipadi        2011-03-01 16:28:21 -0800 5744) 		if (preempt && rq != p_rq)
8875125efe840 kernel/sched/core.c (Kirill Tkhai               2014-06-29 00:03:57 +0400 5745) 			resched_curr(p_rq);
6d1cafd8b56ea kernel/sched.c      (Venkatesh Pallipadi        2011-03-01 16:28:21 -0800 5746) 	}
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5747) 
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5748) out_unlock:
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5749) 	double_rq_unlock(rq, p_rq);
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5750) out_irq:
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5751) 	local_irq_restore(flags);
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5752) 
7b270f609982f kernel/sched/core.c (Peter Zijlstra             2013-01-22 13:09:13 +0530 5753) 	if (yielded > 0)
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5754) 		schedule();
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5755) 
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5756) 	return yielded;
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5757) }
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5758) EXPORT_SYMBOL_GPL(yield_to);
d95f412200652 kernel/sched.c      (Mike Galbraith             2011-02-01 09:50:51 -0500 5759) 
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5760) int io_schedule_prepare(void)
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5761) {
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5762) 	int old_iowait = current->in_iowait;
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5763) 
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5764) 	current->in_iowait = 1;
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5765) 	blk_schedule_flush_plug(current);
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5766) 
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5767) 	return old_iowait;
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5768) }
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5769) 
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5770) void io_schedule_finish(int token)
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5771) {
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5772) 	current->in_iowait = token;
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5773) }
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5774) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5775) /*
41a2d6cfa3f77 kernel/sched.c      (Ingo Molnar                2007-12-05 15:46:09 +0100 5776)  * This task is about to go to sleep on IO. Increment rq->nr_iowait so
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5777)  * that process accounting knows that this is a task in IO wait state.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5778)  */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5779) long __sched io_schedule_timeout(long timeout)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5780) {
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5781) 	int token;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5782) 	long ret;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5783) 
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5784) 	token = io_schedule_prepare();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5785) 	ret = schedule_timeout(timeout);
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5786) 	io_schedule_finish(token);
9cff8adeaa34b kernel/sched/core.c (NeilBrown                  2015-02-13 15:49:17 +1100 5787) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5788) 	return ret;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5789) }
9cff8adeaa34b kernel/sched/core.c (NeilBrown                  2015-02-13 15:49:17 +1100 5790) EXPORT_SYMBOL(io_schedule_timeout);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5791) 
e3b929b0a184e kernel/sched/core.c (Gao Xiang                  2019-06-03 17:13:38 +0800 5792) void __sched io_schedule(void)
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5793) {
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5794) 	int token;
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5795) 
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5796) 	token = io_schedule_prepare();
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5797) 	schedule();
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5798) 	io_schedule_finish(token);
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5799) }
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5800) EXPORT_SYMBOL(io_schedule);
10ab56434f2f6 kernel/sched/core.c (Tejun Heo                  2016-10-28 12:58:10 -0400 5801) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5802) /**
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5803)  * sys_sched_get_priority_max - return maximum RT priority.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5804)  * @policy: scheduling class.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5805)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5806)  * Return: On success, this syscall returns the maximum
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5807)  * rt_priority that can be used by a given scheduling class.
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5808)  * On failure, a negative error code is returned.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5809)  */
5add95d4f7cf0 kernel/sched.c      (Heiko Carstens             2009-01-14 14:14:08 +0100 5810) SYSCALL_DEFINE1(sched_get_priority_max, int, policy)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5811) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5812) 	int ret = -EINVAL;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5813) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5814) 	switch (policy) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5815) 	case SCHED_FIFO:
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5816) 	case SCHED_RR:
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5817) 		ret = MAX_USER_RT_PRIO-1;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5818) 		break;
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 5819) 	case SCHED_DEADLINE:
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5820) 	case SCHED_NORMAL:
b0a9499c3dd50 kernel/sched.c      (Ingo Molnar                2006-01-14 13:20:41 -0800 5821) 	case SCHED_BATCH:
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 5822) 	case SCHED_IDLE:
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5823) 		ret = 0;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5824) 		break;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5825) 	}
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5826) 	return ret;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5827) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5828) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5829) /**
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5830)  * sys_sched_get_priority_min - return minimum RT priority.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5831)  * @policy: scheduling class.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5832)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5833)  * Return: On success, this syscall returns the minimum
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5834)  * rt_priority that can be used by a given scheduling class.
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 5835)  * On failure, a negative error code is returned.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5836)  */
5add95d4f7cf0 kernel/sched.c      (Heiko Carstens             2009-01-14 14:14:08 +0100 5837) SYSCALL_DEFINE1(sched_get_priority_min, int, policy)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5838) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5839) 	int ret = -EINVAL;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5840) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5841) 	switch (policy) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5842) 	case SCHED_FIFO:
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5843) 	case SCHED_RR:
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5844) 		ret = 1;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5845) 		break;
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 5846) 	case SCHED_DEADLINE:
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5847) 	case SCHED_NORMAL:
b0a9499c3dd50 kernel/sched.c      (Ingo Molnar                2006-01-14 13:20:41 -0800 5848) 	case SCHED_BATCH:
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 5849) 	case SCHED_IDLE:
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5850) 		ret = 0;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5851) 	}
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5852) 	return ret;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5853) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5854) 
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5855) static int sched_rr_get_interval(pid_t pid, struct timespec64 *t)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5856) {
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 5857) 	struct task_struct *p;
a4ec24b48ddef kernel/sched.c      (Dmitry Adamushko           2007-10-15 17:00:13 +0200 5858) 	unsigned int time_slice;
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 5859) 	struct rq_flags rf;
dba091b9e3522 kernel/sched.c      (Thomas Gleixner            2009-12-09 09:32:03 +0100 5860) 	struct rq *rq;
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 5861) 	int retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5862) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5863) 	if (pid < 0)
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 5864) 		return -EINVAL;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5865) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5866) 	retval = -ESRCH;
1a551ae715825 kernel/sched.c      (Thomas Gleixner            2009-12-09 10:15:11 +0000 5867) 	rcu_read_lock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5868) 	p = find_process_by_pid(pid);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5869) 	if (!p)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5870) 		goto out_unlock;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5871) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5872) 	retval = security_task_getscheduler(p);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5873) 	if (retval)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5874) 		goto out_unlock;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5875) 
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 5876) 	rq = task_rq_lock(p, &rf);
a57beec5d4270 kernel/sched/core.c (Peter Zijlstra             2014-01-27 11:54:13 +0100 5877) 	time_slice = 0;
a57beec5d4270 kernel/sched/core.c (Peter Zijlstra             2014-01-27 11:54:13 +0100 5878) 	if (p->sched_class->get_rr_interval)
a57beec5d4270 kernel/sched/core.c (Peter Zijlstra             2014-01-27 11:54:13 +0100 5879) 		time_slice = p->sched_class->get_rr_interval(rq, p);
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 5880) 	task_rq_unlock(rq, p, &rf);
a4ec24b48ddef kernel/sched.c      (Dmitry Adamushko           2007-10-15 17:00:13 +0200 5881) 
1a551ae715825 kernel/sched.c      (Thomas Gleixner            2009-12-09 10:15:11 +0000 5882) 	rcu_read_unlock();
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5883) 	jiffies_to_timespec64(time_slice, t);
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5884) 	return 0;
3a5c359a58c39 kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:14 +0200 5885) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5886) out_unlock:
1a551ae715825 kernel/sched.c      (Thomas Gleixner            2009-12-09 10:15:11 +0000 5887) 	rcu_read_unlock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5888) 	return retval;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5889) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5890) 
2064a5ab04707 kernel/sched/core.c (Randy Dunlap               2017-12-03 13:19:00 -0800 5891) /**
2064a5ab04707 kernel/sched/core.c (Randy Dunlap               2017-12-03 13:19:00 -0800 5892)  * sys_sched_rr_get_interval - return the default timeslice of a process.
2064a5ab04707 kernel/sched/core.c (Randy Dunlap               2017-12-03 13:19:00 -0800 5893)  * @pid: pid of the process.
2064a5ab04707 kernel/sched/core.c (Randy Dunlap               2017-12-03 13:19:00 -0800 5894)  * @interval: userspace pointer to the timeslice value.
2064a5ab04707 kernel/sched/core.c (Randy Dunlap               2017-12-03 13:19:00 -0800 5895)  *
2064a5ab04707 kernel/sched/core.c (Randy Dunlap               2017-12-03 13:19:00 -0800 5896)  * this syscall writes the default timeslice value of a given process
2064a5ab04707 kernel/sched/core.c (Randy Dunlap               2017-12-03 13:19:00 -0800 5897)  * into the user-space timespec buffer. A value of '0' means infinity.
2064a5ab04707 kernel/sched/core.c (Randy Dunlap               2017-12-03 13:19:00 -0800 5898)  *
2064a5ab04707 kernel/sched/core.c (Randy Dunlap               2017-12-03 13:19:00 -0800 5899)  * Return: On success, 0 and the timeslice is in @interval. Otherwise,
2064a5ab04707 kernel/sched/core.c (Randy Dunlap               2017-12-03 13:19:00 -0800 5900)  * an error code.
2064a5ab04707 kernel/sched/core.c (Randy Dunlap               2017-12-03 13:19:00 -0800 5901)  */
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5902) SYSCALL_DEFINE2(sched_rr_get_interval, pid_t, pid,
474b9c777b20b kernel/sched/core.c (Arnd Bergmann              2018-04-17 21:59:47 +0200 5903) 		struct __kernel_timespec __user *, interval)
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5904) {
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5905) 	struct timespec64 t;
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5906) 	int retval = sched_rr_get_interval(pid, &t);
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5907) 
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5908) 	if (retval == 0)
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5909) 		retval = put_timespec64(&t, interval);
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5910) 
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5911) 	return retval;
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5912) }
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5913) 
474b9c777b20b kernel/sched/core.c (Arnd Bergmann              2018-04-17 21:59:47 +0200 5914) #ifdef CONFIG_COMPAT_32BIT_TIME
8dabe7245bbc1 kernel/sched/core.c (Arnd Bergmann              2019-01-07 00:33:08 +0100 5915) SYSCALL_DEFINE2(sched_rr_get_interval_time32, pid_t, pid,
8dabe7245bbc1 kernel/sched/core.c (Arnd Bergmann              2019-01-07 00:33:08 +0100 5916) 		struct old_timespec32 __user *, interval)
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5917) {
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5918) 	struct timespec64 t;
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5919) 	int retval = sched_rr_get_interval(pid, &t);
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5920) 
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5921) 	if (retval == 0)
9afc5eee65ca7 kernel/sched/core.c (Arnd Bergmann              2018-07-13 12:52:28 +0200 5922) 		retval = put_old_timespec32(&t, interval);
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5923) 	return retval;
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5924) }
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5925) #endif
abca5fc535a3e kernel/sched/core.c (Al Viro                    2017-09-19 18:17:46 -0400 5926) 
82a1fcb902870 kernel/sched.c      (Ingo Molnar                2008-01-25 21:08:02 +0100 5927) void sched_show_task(struct task_struct *p)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5928) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5929) 	unsigned long free = 0;
4e79752c25ec2 kernel/sched/core.c (Paul E. McKenney           2012-11-07 13:35:32 -0800 5930) 	int ppid;
c930b2c0de32f kernel/sched/core.c (Ingo Molnar                2017-02-03 12:22:54 +0100 5931) 
382005027fedc kernel/sched/core.c (Tetsuo Handa               2016-11-02 19:50:29 +0900 5932) 	if (!try_get_task_stack(p))
382005027fedc kernel/sched/core.c (Tetsuo Handa               2016-11-02 19:50:29 +0900 5933) 		return;
20435d84e5f20 kernel/sched/core.c (Xie XiuQi                  2017-08-07 16:44:23 +0800 5934) 
20435d84e5f20 kernel/sched/core.c (Xie XiuQi                  2017-08-07 16:44:23 +0800 5935) 	printk(KERN_INFO "%-15.15s %c", p->comm, task_state_to_char(p));
20435d84e5f20 kernel/sched/core.c (Xie XiuQi                  2017-08-07 16:44:23 +0800 5936) 
20435d84e5f20 kernel/sched/core.c (Xie XiuQi                  2017-08-07 16:44:23 +0800 5937) 	if (p->state == TASK_RUNNING)
3df0fc5b2e9d8 kernel/sched.c      (Peter Zijlstra             2009-12-20 14:23:57 +0100 5938) 		printk(KERN_CONT "  running task    ");
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5939) #ifdef CONFIG_DEBUG_STACK_USAGE
7c9f8861e6c9c kernel/sched.c      (Eric Sandeen               2008-04-22 16:38:23 -0500 5940) 	free = stack_not_used(p);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5941) #endif
a90e984c8a660 kernel/sched/core.c (Oleg Nesterov              2014-12-10 15:45:21 -0800 5942) 	ppid = 0;
4e79752c25ec2 kernel/sched/core.c (Paul E. McKenney           2012-11-07 13:35:32 -0800 5943) 	rcu_read_lock();
a90e984c8a660 kernel/sched/core.c (Oleg Nesterov              2014-12-10 15:45:21 -0800 5944) 	if (pid_alive(p))
a90e984c8a660 kernel/sched/core.c (Oleg Nesterov              2014-12-10 15:45:21 -0800 5945) 		ppid = task_pid_nr(rcu_dereference(p->real_parent));
4e79752c25ec2 kernel/sched/core.c (Paul E. McKenney           2012-11-07 13:35:32 -0800 5946) 	rcu_read_unlock();
3df0fc5b2e9d8 kernel/sched.c      (Peter Zijlstra             2009-12-20 14:23:57 +0100 5947) 	printk(KERN_CONT "%5lu %5d %6d 0x%08lx\n", free,
4e79752c25ec2 kernel/sched/core.c (Paul E. McKenney           2012-11-07 13:35:32 -0800 5948) 		task_pid_nr(p), ppid,
aa47b7e0f89b9 kernel/sched.c      (David Rientjes             2009-05-04 01:38:05 -0700 5949) 		(unsigned long)task_thread_info(p)->flags);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5950) 
3d1cb2059d937 kernel/sched/core.c (Tejun Heo                  2013-04-30 15:27:22 -0700 5951) 	print_worker_info(KERN_INFO, p);
5fb5e6de55860 kernel/sched.c      (Nick Piggin                2008-01-25 21:08:34 +0100 5952) 	show_stack(p, NULL);
382005027fedc kernel/sched/core.c (Tetsuo Handa               2016-11-02 19:50:29 +0900 5953) 	put_task_stack(p);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5954) }
0032f4e889764 kernel/sched/core.c (Paul E. McKenney           2017-08-30 10:40:17 -0700 5955) EXPORT_SYMBOL_GPL(sched_show_task);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5956) 
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5957) static inline bool
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5958) state_filter_match(unsigned long state_filter, struct task_struct *p)
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5959) {
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5960) 	/* no filter, everything matches */
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5961) 	if (!state_filter)
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5962) 		return true;
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5963) 
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5964) 	/* filter, but doesn't match */
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5965) 	if (!(p->state & state_filter))
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5966) 		return false;
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5967) 
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5968) 	/*
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5969) 	 * When looking for TASK_UNINTERRUPTIBLE skip TASK_IDLE (allows
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5970) 	 * TASK_KILLABLE).
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5971) 	 */
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5972) 	if (state_filter == TASK_UNINTERRUPTIBLE && p->state == TASK_IDLE)
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5973) 		return false;
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5974) 
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5975) 	return true;
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5976) }
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5977) 
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 5978) 
e59e2ae2c2970 kernel/sched.c      (Ingo Molnar                2006-12-06 20:35:59 -0800 5979) void show_state_filter(unsigned long state_filter)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5980) {
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 5981) 	struct task_struct *g, *p;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5982) 
4bd77321a8330 kernel/sched.c      (Ingo Molnar                2007-07-11 21:21:47 +0200 5983) #if BITS_PER_LONG == 32
3df0fc5b2e9d8 kernel/sched.c      (Peter Zijlstra             2009-12-20 14:23:57 +0100 5984) 	printk(KERN_INFO
3df0fc5b2e9d8 kernel/sched.c      (Peter Zijlstra             2009-12-20 14:23:57 +0100 5985) 		"  task                PC stack   pid father\n");
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5986) #else
3df0fc5b2e9d8 kernel/sched.c      (Peter Zijlstra             2009-12-20 14:23:57 +0100 5987) 	printk(KERN_INFO
3df0fc5b2e9d8 kernel/sched.c      (Peter Zijlstra             2009-12-20 14:23:57 +0100 5988) 		"  task                        PC stack   pid father\n");
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5989) #endif
510f5acc4f4fb kernel/sched.c      (Thomas Gleixner            2011-07-17 20:47:54 +0200 5990) 	rcu_read_lock();
5d07f4202c5d6 kernel/sched/core.c (Oleg Nesterov              2014-08-13 21:19:53 +0200 5991) 	for_each_process_thread(g, p) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5992) 		/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5993) 		 * reset the NMI-timeout, listing all files on a slow
25985edcedea6 kernel/sched.c      (Lucas De Marchi            2011-03-30 22:57:33 -0300 5994) 		 * console might take a lot of time:
57675cb976eff kernel/sched/core.c (Andrey Ryabinin            2016-06-09 15:20:05 +0300 5995) 		 * Also, reset softlockup watchdogs on all CPUs, because
57675cb976eff kernel/sched/core.c (Andrey Ryabinin            2016-06-09 15:20:05 +0300 5996) 		 * another CPU might be blocked waiting for us to process
57675cb976eff kernel/sched/core.c (Andrey Ryabinin            2016-06-09 15:20:05 +0300 5997) 		 * an IPI.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5998) 		 */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 5999) 		touch_nmi_watchdog();
57675cb976eff kernel/sched/core.c (Andrey Ryabinin            2016-06-09 15:20:05 +0300 6000) 		touch_all_softlockup_watchdogs();
5d68cc95fb24b kernel/sched/core.c (Peter Zijlstra             2017-09-22 18:32:41 +0200 6001) 		if (state_filter_match(state_filter, p))
82a1fcb902870 kernel/sched.c      (Ingo Molnar                2008-01-25 21:08:02 +0100 6002) 			sched_show_task(p);
5d07f4202c5d6 kernel/sched/core.c (Oleg Nesterov              2014-08-13 21:19:53 +0200 6003) 	}
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6004) 
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6005) #ifdef CONFIG_SCHED_DEBUG
fb90a6e93c068 kernel/sched/core.c (Rabin Vincent              2016-04-04 15:42:02 +0200 6006) 	if (!state_filter)
fb90a6e93c068 kernel/sched/core.c (Rabin Vincent              2016-04-04 15:42:02 +0200 6007) 		sysrq_sched_debug_show();
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6008) #endif
510f5acc4f4fb kernel/sched.c      (Thomas Gleixner            2011-07-17 20:47:54 +0200 6009) 	rcu_read_unlock();
e59e2ae2c2970 kernel/sched.c      (Ingo Molnar                2006-12-06 20:35:59 -0800 6010) 	/*
e59e2ae2c2970 kernel/sched.c      (Ingo Molnar                2006-12-06 20:35:59 -0800 6011) 	 * Only show locks if all tasks are dumped:
e59e2ae2c2970 kernel/sched.c      (Ingo Molnar                2006-12-06 20:35:59 -0800 6012) 	 */
93335a21557e8 kernel/sched.c      (Shmulik Ladkani            2009-11-25 15:23:41 +0200 6013) 	if (!state_filter)
e59e2ae2c2970 kernel/sched.c      (Ingo Molnar                2006-12-06 20:35:59 -0800 6014) 		debug_show_all_locks();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6015) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6016) 
f340c0d1a3f40 kernel/sched.c      (Ingo Molnar                2005-06-28 16:40:42 +0200 6017) /**
f340c0d1a3f40 kernel/sched.c      (Ingo Molnar                2005-06-28 16:40:42 +0200 6018)  * init_idle - set up an idle thread for a given CPU
f340c0d1a3f40 kernel/sched.c      (Ingo Molnar                2005-06-28 16:40:42 +0200 6019)  * @idle: task in question
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6020)  * @cpu: CPU the idle task belongs to
f340c0d1a3f40 kernel/sched.c      (Ingo Molnar                2005-06-28 16:40:42 +0200 6021)  *
f340c0d1a3f40 kernel/sched.c      (Ingo Molnar                2005-06-28 16:40:42 +0200 6022)  * NOTE: this function does not set the idle thread's NEED_RESCHED
f340c0d1a3f40 kernel/sched.c      (Ingo Molnar                2005-06-28 16:40:42 +0200 6023)  * flag, to make booting more robust.
f340c0d1a3f40 kernel/sched.c      (Ingo Molnar                2005-06-28 16:40:42 +0200 6024)  */
0db0628d90125 kernel/sched/core.c (Paul Gortmaker             2013-06-19 14:53:51 -0400 6025) void init_idle(struct task_struct *idle, int cpu)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6026) {
70b97a7f0b19c kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:42 -0700 6027) 	struct rq *rq = cpu_rq(cpu);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6028) 	unsigned long flags;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6029) 
ff51ff84d82ae kernel/sched/core.c (Peter Zijlstra             2019-10-01 11:18:37 +0200 6030) 	__sched_fork(0, idle);
ff51ff84d82ae kernel/sched/core.c (Peter Zijlstra             2019-10-01 11:18:37 +0200 6031) 
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 6032) 	raw_spin_lock_irqsave(&idle->pi_lock, flags);
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 6033) 	raw_spin_lock(&rq->lock);
5cbd54ef470d8 kernel/sched.c      (Ingo Molnar                2008-11-12 20:05:50 +0100 6034) 
06b83b5fbea27 kernel/sched.c      (Peter Zijlstra             2009-12-16 18:04:35 +0100 6035) 	idle->state = TASK_RUNNING;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6036) 	idle->se.exec_start = sched_clock();
c1de45ca831ac kernel/sched/core.c (Peter Zijlstra             2016-11-28 23:03:05 -0800 6037) 	idle->flags |= PF_IDLE;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6038) 
e1b77c92981a5 kernel/sched/core.c (Mark Rutland               2016-03-09 14:08:18 -0800 6039) 	kasan_unpoison_task_stack(idle);
e1b77c92981a5 kernel/sched/core.c (Mark Rutland               2016-03-09 14:08:18 -0800 6040) 
de9b8f5dcbd94 kernel/sched/core.c (Peter Zijlstra             2015-08-13 23:09:29 +0200 6041) #ifdef CONFIG_SMP
de9b8f5dcbd94 kernel/sched/core.c (Peter Zijlstra             2015-08-13 23:09:29 +0200 6042) 	/*
de9b8f5dcbd94 kernel/sched/core.c (Peter Zijlstra             2015-08-13 23:09:29 +0200 6043) 	 * Its possible that init_idle() gets called multiple times on a task,
de9b8f5dcbd94 kernel/sched/core.c (Peter Zijlstra             2015-08-13 23:09:29 +0200 6044) 	 * in that case do_set_cpus_allowed() will not do the right thing.
de9b8f5dcbd94 kernel/sched/core.c (Peter Zijlstra             2015-08-13 23:09:29 +0200 6045) 	 *
de9b8f5dcbd94 kernel/sched/core.c (Peter Zijlstra             2015-08-13 23:09:29 +0200 6046) 	 * And since this is boot we can forgo the serialization.
de9b8f5dcbd94 kernel/sched/core.c (Peter Zijlstra             2015-08-13 23:09:29 +0200 6047) 	 */
de9b8f5dcbd94 kernel/sched/core.c (Peter Zijlstra             2015-08-13 23:09:29 +0200 6048) 	set_cpus_allowed_common(idle, cpumask_of(cpu));
de9b8f5dcbd94 kernel/sched/core.c (Peter Zijlstra             2015-08-13 23:09:29 +0200 6049) #endif
6506cf6ce68d7 kernel/sched.c      (Peter Zijlstra             2010-09-16 17:50:31 +0200 6050) 	/*
6506cf6ce68d7 kernel/sched.c      (Peter Zijlstra             2010-09-16 17:50:31 +0200 6051) 	 * We're having a chicken and egg problem, even though we are
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6052) 	 * holding rq->lock, the CPU isn't yet set to this CPU so the
6506cf6ce68d7 kernel/sched.c      (Peter Zijlstra             2010-09-16 17:50:31 +0200 6053) 	 * lockdep check in task_group() will fail.
6506cf6ce68d7 kernel/sched.c      (Peter Zijlstra             2010-09-16 17:50:31 +0200 6054) 	 *
6506cf6ce68d7 kernel/sched.c      (Peter Zijlstra             2010-09-16 17:50:31 +0200 6055) 	 * Similar case to sched_fork(). / Alternatively we could
6506cf6ce68d7 kernel/sched.c      (Peter Zijlstra             2010-09-16 17:50:31 +0200 6056) 	 * use task_rq_lock() here and obtain the other rq->lock.
6506cf6ce68d7 kernel/sched.c      (Peter Zijlstra             2010-09-16 17:50:31 +0200 6057) 	 *
6506cf6ce68d7 kernel/sched.c      (Peter Zijlstra             2010-09-16 17:50:31 +0200 6058) 	 * Silence PROVE_RCU
6506cf6ce68d7 kernel/sched.c      (Peter Zijlstra             2010-09-16 17:50:31 +0200 6059) 	 */
6506cf6ce68d7 kernel/sched.c      (Peter Zijlstra             2010-09-16 17:50:31 +0200 6060) 	rcu_read_lock();
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6061) 	__set_task_cpu(idle, cpu);
6506cf6ce68d7 kernel/sched.c      (Peter Zijlstra             2010-09-16 17:50:31 +0200 6062) 	rcu_read_unlock();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6063) 
5311a98fef7d0 kernel/sched/core.c (Eric W. Biederman          2019-09-14 07:35:02 -0500 6064) 	rq->idle = idle;
5311a98fef7d0 kernel/sched/core.c (Eric W. Biederman          2019-09-14 07:35:02 -0500 6065) 	rcu_assign_pointer(rq->curr, idle);
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 6066) 	idle->on_rq = TASK_ON_RQ_QUEUED;
de9b8f5dcbd94 kernel/sched/core.c (Peter Zijlstra             2015-08-13 23:09:29 +0200 6067) #ifdef CONFIG_SMP
3ca7a440da394 kernel/sched.c      (Peter Zijlstra             2011-04-05 17:23:40 +0200 6068) 	idle->on_cpu = 1;
4866cde064afb kernel/sched.c      (Nick Piggin                2005-06-25 14:57:23 -0700 6069) #endif
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 6070) 	raw_spin_unlock(&rq->lock);
25834c73f93af kernel/sched/core.c (Peter Zijlstra             2015-05-15 17:43:34 +0200 6071) 	raw_spin_unlock_irqrestore(&idle->pi_lock, flags);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6072) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6073) 	/* Set the preempt count _outside_ the spinlocks! */
01028747559ac kernel/sched/core.c (Peter Zijlstra             2013-08-14 14:55:46 +0200 6074) 	init_idle_preempt_count(idle, cpu);
55cd53404c5cc kernel/sched.c      (Peter Zijlstra             2008-08-04 08:54:26 +0200 6075) 
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6076) 	/*
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6077) 	 * The idle tasks have their own, simple scheduling class:
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6078) 	 */
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6079) 	idle->sched_class = &idle_sched_class;
868baf07b1a25 kernel/sched.c      (Steven Rostedt             2011-02-10 21:26:13 -0500 6080) 	ftrace_graph_init_idle_task(idle, cpu);
45eacc692771b kernel/sched/core.c (Frederic Weisbecker        2013-05-15 22:16:32 +0200 6081) 	vtime_init_idle(idle, cpu);
de9b8f5dcbd94 kernel/sched/core.c (Peter Zijlstra             2015-08-13 23:09:29 +0200 6082) #ifdef CONFIG_SMP
f1c6f1a7eed96 kernel/sched.c      (Carsten Emde               2011-10-26 23:14:16 +0200 6083) 	sprintf(idle->comm, "%s/%d", INIT_TASK_COMM, cpu);
f1c6f1a7eed96 kernel/sched.c      (Carsten Emde               2011-10-26 23:14:16 +0200 6084) #endif
19978ca610946 kernel/sched.c      (Ingo Molnar                2007-11-09 22:39:38 +0100 6085) }
19978ca610946 kernel/sched.c      (Ingo Molnar                2007-11-09 22:39:38 +0100 6086) 
e1d4eeec5aaa2 kernel/sched/core.c (Nicolas Pitre              2017-06-14 13:19:23 -0400 6087) #ifdef CONFIG_SMP
e1d4eeec5aaa2 kernel/sched/core.c (Nicolas Pitre              2017-06-14 13:19:23 -0400 6088) 
f82f80426f7af kernel/sched/core.c (Juri Lelli                 2014-10-07 09:52:11 +0100 6089) int cpuset_cpumask_can_shrink(const struct cpumask *cur,
f82f80426f7af kernel/sched/core.c (Juri Lelli                 2014-10-07 09:52:11 +0100 6090) 			      const struct cpumask *trial)
f82f80426f7af kernel/sched/core.c (Juri Lelli                 2014-10-07 09:52:11 +0100 6091) {
06a76fe08d4da kernel/sched/core.c (Nicolas Pitre              2017-06-21 14:22:01 -0400 6092) 	int ret = 1;
f82f80426f7af kernel/sched/core.c (Juri Lelli                 2014-10-07 09:52:11 +0100 6093) 
bb2bc55a694d4 kernel/sched/core.c (Mike Galbraith             2015-01-28 04:53:55 +0100 6094) 	if (!cpumask_weight(cur))
bb2bc55a694d4 kernel/sched/core.c (Mike Galbraith             2015-01-28 04:53:55 +0100 6095) 		return ret;
bb2bc55a694d4 kernel/sched/core.c (Mike Galbraith             2015-01-28 04:53:55 +0100 6096) 
06a76fe08d4da kernel/sched/core.c (Nicolas Pitre              2017-06-21 14:22:01 -0400 6097) 	ret = dl_cpuset_cpumask_can_shrink(cur, trial);
f82f80426f7af kernel/sched/core.c (Juri Lelli                 2014-10-07 09:52:11 +0100 6098) 
f82f80426f7af kernel/sched/core.c (Juri Lelli                 2014-10-07 09:52:11 +0100 6099) 	return ret;
f82f80426f7af kernel/sched/core.c (Juri Lelli                 2014-10-07 09:52:11 +0100 6100) }
f82f80426f7af kernel/sched/core.c (Juri Lelli                 2014-10-07 09:52:11 +0100 6101) 
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6102) int task_can_attach(struct task_struct *p,
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6103) 		    const struct cpumask *cs_cpus_allowed)
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6104) {
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6105) 	int ret = 0;
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6106) 
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6107) 	/*
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6108) 	 * Kthreads which disallow setaffinity shouldn't be moved
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6109) 	 * to a new cpuset; we don't want to change their CPU
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6110) 	 * affinity and isolating such threads by their set of
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6111) 	 * allowed nodes is unnecessary.  Thus, cpusets are not
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6112) 	 * applicable for such threads.  This prevents checking for
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6113) 	 * success of set_cpus_allowed_ptr() on all attached tasks
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 6114) 	 * before cpus_mask may be changed.
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6115) 	 */
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6116) 	if (p->flags & PF_NO_SETAFFINITY) {
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6117) 		ret = -EINVAL;
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6118) 		goto out;
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6119) 	}
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6120) 
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6121) 	if (dl_task(p) && !cpumask_intersects(task_rq(p)->rd->span,
06a76fe08d4da kernel/sched/core.c (Nicolas Pitre              2017-06-21 14:22:01 -0400 6122) 					      cs_cpus_allowed))
06a76fe08d4da kernel/sched/core.c (Nicolas Pitre              2017-06-21 14:22:01 -0400 6123) 		ret = dl_task_can_attach(p, cs_cpus_allowed);
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6124) 
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6125) out:
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6126) 	return ret;
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6127) }
7f51412a415d8 kernel/sched/core.c (Juri Lelli                 2014-09-19 10:22:40 +0100 6128) 
f2cb13609d539 kernel/sched/core.c (Ingo Molnar                2017-02-01 13:10:18 +0100 6129) bool sched_smp_initialized __read_mostly;
e26fbffd32c28 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:10 +0100 6130) 
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6131) #ifdef CONFIG_NUMA_BALANCING
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6132) /* Migrate current task p to target_cpu */
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6133) int migrate_task_to(struct task_struct *p, int target_cpu)
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6134) {
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6135) 	struct migration_arg arg = { p, target_cpu };
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6136) 	int curr_cpu = task_cpu(p);
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6137) 
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6138) 	if (curr_cpu == target_cpu)
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6139) 		return 0;
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6140) 
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 6141) 	if (!cpumask_test_cpu(target_cpu, p->cpus_ptr))
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6142) 		return -EINVAL;
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6143) 
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6144) 	/* TODO: This is not properly updating schedstats */
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6145) 
286549dcaf4f1 kernel/sched/core.c (Mel Gorman                 2014-01-21 15:51:03 -0800 6146) 	trace_sched_move_numa(p, curr_cpu, target_cpu);
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6147) 	return stop_one_cpu(curr_cpu, migration_cpu_stop, &arg);
e6628d5b0a297 kernel/sched/core.c (Mel Gorman                 2013-10-07 11:29:02 +0100 6148) }
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6149) 
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6150) /*
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6151)  * Requeue a task on a given node and accurately track the number of NUMA
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6152)  * tasks on the runqueues
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6153)  */
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6154) void sched_setnuma(struct task_struct *p, int nid)
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6155) {
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 6156) 	bool queued, running;
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 6157) 	struct rq_flags rf;
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 6158) 	struct rq *rq;
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6159) 
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 6160) 	rq = task_rq_lock(p, &rf);
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 6161) 	queued = task_on_rq_queued(p);
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6162) 	running = task_current(rq, p);
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6163) 
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 6164) 	if (queued)
1de64443d755f kernel/sched/core.c (Peter Zijlstra             2015-09-30 17:44:13 +0200 6165) 		dequeue_task(rq, p, DEQUEUE_SAVE);
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6166) 	if (running)
f3cd1c4ec059c kernel/sched/core.c (Kirill Tkhai               2014-09-12 17:41:40 +0400 6167) 		put_prev_task(rq, p);
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6168) 
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6169) 	p->numa_preferred_nid = nid;
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6170) 
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 6171) 	if (queued)
7134b3e941613 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:23:38 +0100 6172) 		enqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);
a399d233078ed kernel/sched/core.c (Vincent Guittot            2016-09-12 09:47:52 +0200 6173) 	if (running)
03b7fad167efc kernel/sched/core.c (Peter Zijlstra             2019-05-29 20:36:41 +0000 6174) 		set_next_task(rq, p);
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 6175) 	task_rq_unlock(rq, p, &rf);
0ec8aa00f2b4d kernel/sched/core.c (Peter Zijlstra             2013-10-07 11:29:33 +0100 6176) }
5cc389bcee088 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:50 +0200 6177) #endif /* CONFIG_NUMA_BALANCING */
f7b4cddcc5aca kernel/sched.c      (Oleg Nesterov              2007-10-16 23:30:56 -0700 6178) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6179) #ifdef CONFIG_HOTPLUG_CPU
054b9108e01ef kernel/sched.c      (Kirill Korotaev            2006-12-10 02:20:11 -0800 6180) /*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6181)  * Ensure that the idle task is using init_mm right before its CPU goes
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6182)  * offline.
054b9108e01ef kernel/sched.c      (Kirill Korotaev            2006-12-10 02:20:11 -0800 6183)  */
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6184) void idle_task_exit(void)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6185) {
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6186) 	struct mm_struct *mm = current->active_mm;
e76bd8d9850c2 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 6187) 
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6188) 	BUG_ON(cpu_online(smp_processor_id()));
e76bd8d9850c2 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:11 +1030 6189) 
a53efe5ff88d0 kernel/sched/core.c (Martin Schwidefsky         2012-10-26 17:17:44 +0200 6190) 	if (mm != &init_mm) {
252d2a4117bc1 kernel/sched/core.c (Andy Lutomirski            2017-06-09 11:49:15 -0700 6191) 		switch_mm(mm, &init_mm, current);
3eda69c92d475 kernel/sched/core.c (Mark Rutland               2018-04-05 16:25:12 -0700 6192) 		current->active_mm = &init_mm;
a53efe5ff88d0 kernel/sched/core.c (Martin Schwidefsky         2012-10-26 17:17:44 +0200 6193) 		finish_arch_post_lock_switch();
a53efe5ff88d0 kernel/sched/core.c (Martin Schwidefsky         2012-10-26 17:17:44 +0200 6194) 	}
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6195) 	mmdrop(mm);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6196) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6197) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6198) /*
5d18023294abc kernel/sched/core.c (Peter Zijlstra             2012-08-20 11:26:57 +0200 6199)  * Since this CPU is going 'away' for a while, fold any nr_active delta
5d18023294abc kernel/sched/core.c (Peter Zijlstra             2012-08-20 11:26:57 +0200 6200)  * we might have. Assumes we're called after migrate_tasks() so that the
d60585c5766e9 kernel/sched/core.c (Thomas Gleixner            2016-07-12 18:33:56 +0200 6201)  * nr_active count is stable. We need to take the teardown thread which
d60585c5766e9 kernel/sched/core.c (Thomas Gleixner            2016-07-12 18:33:56 +0200 6202)  * is calling this into account, so we hand in adjust = 1 to the load
d60585c5766e9 kernel/sched/core.c (Thomas Gleixner            2016-07-12 18:33:56 +0200 6203)  * calculation.
5d18023294abc kernel/sched/core.c (Peter Zijlstra             2012-08-20 11:26:57 +0200 6204)  *
5d18023294abc kernel/sched/core.c (Peter Zijlstra             2012-08-20 11:26:57 +0200 6205)  * Also see the comment "Global load-average calculations".
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6206)  */
5d18023294abc kernel/sched/core.c (Peter Zijlstra             2012-08-20 11:26:57 +0200 6207) static void calc_load_migrate(struct rq *rq)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6208) {
d60585c5766e9 kernel/sched/core.c (Thomas Gleixner            2016-07-12 18:33:56 +0200 6209) 	long delta = calc_load_fold_active(rq, 1);
5d18023294abc kernel/sched/core.c (Peter Zijlstra             2012-08-20 11:26:57 +0200 6210) 	if (delta)
5d18023294abc kernel/sched/core.c (Peter Zijlstra             2012-08-20 11:26:57 +0200 6211) 		atomic_long_add(delta, &calc_load_tasks);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6212) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6213) 
10e7071b2f491 kernel/sched/core.c (Peter Zijlstra             2019-08-06 15:13:17 +0200 6214) static struct task_struct *__pick_migrate_task(struct rq *rq)
3f1d2a318171b kernel/sched/core.c (Peter Zijlstra             2014-02-12 10:49:30 +0100 6215) {
10e7071b2f491 kernel/sched/core.c (Peter Zijlstra             2019-08-06 15:13:17 +0200 6216) 	const struct sched_class *class;
10e7071b2f491 kernel/sched/core.c (Peter Zijlstra             2019-08-06 15:13:17 +0200 6217) 	struct task_struct *next;
3f1d2a318171b kernel/sched/core.c (Peter Zijlstra             2014-02-12 10:49:30 +0100 6218) 
10e7071b2f491 kernel/sched/core.c (Peter Zijlstra             2019-08-06 15:13:17 +0200 6219) 	for_each_class(class) {
98c2f700edb41 kernel/sched/core.c (Peter Zijlstra             2019-11-08 14:15:58 +0100 6220) 		next = class->pick_next_task(rq);
10e7071b2f491 kernel/sched/core.c (Peter Zijlstra             2019-08-06 15:13:17 +0200 6221) 		if (next) {
6e2df0581f569 kernel/sched/core.c (Peter Zijlstra             2019-11-08 11:11:52 +0100 6222) 			next->sched_class->put_prev_task(rq, next);
10e7071b2f491 kernel/sched/core.c (Peter Zijlstra             2019-08-06 15:13:17 +0200 6223) 			return next;
10e7071b2f491 kernel/sched/core.c (Peter Zijlstra             2019-08-06 15:13:17 +0200 6224) 		}
10e7071b2f491 kernel/sched/core.c (Peter Zijlstra             2019-08-06 15:13:17 +0200 6225) 	}
3f1d2a318171b kernel/sched/core.c (Peter Zijlstra             2014-02-12 10:49:30 +0100 6226) 
10e7071b2f491 kernel/sched/core.c (Peter Zijlstra             2019-08-06 15:13:17 +0200 6227) 	/* The idle class should always have a runnable task */
10e7071b2f491 kernel/sched/core.c (Peter Zijlstra             2019-08-06 15:13:17 +0200 6228) 	BUG();
10e7071b2f491 kernel/sched/core.c (Peter Zijlstra             2019-08-06 15:13:17 +0200 6229) }
3f1d2a318171b kernel/sched/core.c (Peter Zijlstra             2014-02-12 10:49:30 +0100 6230) 
48f24c4da1ee7 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:40 -0700 6231) /*
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6232)  * Migrate all tasks from the rq, sleeping tasks will be migrated by
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6233)  * try_to_wake_up()->select_task_rq().
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6234)  *
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6235)  * Called with rq->lock held even though we'er in stop_machine() and
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6236)  * there's no concurrency possible, we hold the required locks anyway
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6237)  * because of lock validation efforts.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6238)  */
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6239) static void migrate_tasks(struct rq *dead_rq, struct rq_flags *rf)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6240) {
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 6241) 	struct rq *rq = dead_rq;
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6242) 	struct task_struct *next, *stop = rq->stop;
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6243) 	struct rq_flags orf = *rf;
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6244) 	int dest_cpu;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6245) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6246) 	/*
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6247) 	 * Fudge the rq selection such that the below task selection loop
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6248) 	 * doesn't get stuck on the currently eligible stop task.
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6249) 	 *
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6250) 	 * We're currently inside stop_machine() and the rq is either stuck
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6251) 	 * in the stop_machine_cpu_stop() loop, or we're executing this code,
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6252) 	 * either way we should never end up calling schedule() until we're
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6253) 	 * done here.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6254) 	 */
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6255) 	rq->stop = NULL;
48f24c4da1ee7 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:40 -0700 6256) 
77bd39702f0b3 kernel/sched/core.c (Frederic Weisbecker        2013-04-12 01:50:58 +0200 6257) 	/*
77bd39702f0b3 kernel/sched/core.c (Frederic Weisbecker        2013-04-12 01:50:58 +0200 6258) 	 * put_prev_task() and pick_next_task() sched
77bd39702f0b3 kernel/sched/core.c (Frederic Weisbecker        2013-04-12 01:50:58 +0200 6259) 	 * class method both need to have an up-to-date
77bd39702f0b3 kernel/sched/core.c (Frederic Weisbecker        2013-04-12 01:50:58 +0200 6260) 	 * value of rq->clock[_task]
77bd39702f0b3 kernel/sched/core.c (Frederic Weisbecker        2013-04-12 01:50:58 +0200 6261) 	 */
77bd39702f0b3 kernel/sched/core.c (Frederic Weisbecker        2013-04-12 01:50:58 +0200 6262) 	update_rq_clock(rq);
77bd39702f0b3 kernel/sched/core.c (Frederic Weisbecker        2013-04-12 01:50:58 +0200 6263) 
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 6264) 	for (;;) {
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6265) 		/*
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6266) 		 * There's this thread running, bail when that's the only
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6267) 		 * remaining thread:
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6268) 		 */
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6269) 		if (rq->nr_running == 1)
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6270) 			break;
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6271) 
10e7071b2f491 kernel/sched/core.c (Peter Zijlstra             2019-08-06 15:13:17 +0200 6272) 		next = __pick_migrate_task(rq);
e692ab53473c9 kernel/sched.c      (Nick Piggin                2007-07-26 13:40:43 +0200 6273) 
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6274) 		/*
3bd3706251ee8 kernel/sched/core.c (Sebastian Andrzej Siewior  2019-04-23 16:26:36 +0200 6275) 		 * Rules for changing task_struct::cpus_mask are holding
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6276) 		 * both pi_lock and rq->lock, such that holding either
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6277) 		 * stabilizes the mask.
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6278) 		 *
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6279) 		 * Drop rq->lock is not quite as disastrous as it usually is
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6280) 		 * because !cpu_active at this point, which means load-balance
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6281) 		 * will not interfere. Also, stop-machine.
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6282) 		 */
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6283) 		rq_unlock(rq, rf);
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6284) 		raw_spin_lock(&next->pi_lock);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6285) 		rq_relock(rq, rf);
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6286) 
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6287) 		/*
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6288) 		 * Since we're inside stop-machine, _nothing_ should have
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6289) 		 * changed the task, WARN if weird stuff happened, because in
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6290) 		 * that case the above rq->lock drop is a fail too.
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6291) 		 */
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6292) 		if (WARN_ON(task_rq(next) != rq || !task_on_rq_queued(next))) {
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6293) 			raw_spin_unlock(&next->pi_lock);
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6294) 			continue;
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6295) 		}
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6296) 
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6297) 		/* Find suitable destination for @next, with force if needed. */
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 6298) 		dest_cpu = select_fallback_rq(dead_rq->cpu, next);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6299) 		rq = __migrate_task(rq, rf, next, dest_cpu);
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 6300) 		if (rq != dead_rq) {
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6301) 			rq_unlock(rq, rf);
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 6302) 			rq = dead_rq;
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6303) 			*rf = orf;
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6304) 			rq_relock(rq, rf);
5e16bbc2fb405 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:51 +0200 6305) 		}
5473e0cc37c03 kernel/sched/core.c (Wanpeng Li                 2015-08-28 14:55:56 +0800 6306) 		raw_spin_unlock(&next->pi_lock);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6307) 	}
dce48a84adf18 kernel/sched.c      (Thomas Gleixner            2009-04-11 10:43:41 +0200 6308) 
48c5ccae88dcd kernel/sched.c      (Peter Zijlstra             2010-11-13 19:32:29 +0100 6309) 	rq->stop = stop;
dce48a84adf18 kernel/sched.c      (Thomas Gleixner            2009-04-11 10:43:41 +0200 6310) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6311) #endif /* CONFIG_HOTPLUG_CPU */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6312) 
f2cb13609d539 kernel/sched/core.c (Ingo Molnar                2017-02-01 13:10:18 +0100 6313) void set_rq_online(struct rq *rq)
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6314) {
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6315) 	if (!rq->online) {
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6316) 		const struct sched_class *class;
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6317) 
c6c4927b22a35 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:05 +1030 6318) 		cpumask_set_cpu(rq->cpu, rq->rd->online);
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6319) 		rq->online = 1;
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6320) 
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6321) 		for_each_class(class) {
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6322) 			if (class->rq_online)
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6323) 				class->rq_online(rq);
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6324) 		}
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6325) 	}
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6326) }
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6327) 
f2cb13609d539 kernel/sched/core.c (Ingo Molnar                2017-02-01 13:10:18 +0100 6328) void set_rq_offline(struct rq *rq)
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6329) {
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6330) 	if (rq->online) {
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6331) 		const struct sched_class *class;
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6332) 
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6333) 		for_each_class(class) {
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6334) 			if (class->rq_offline)
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6335) 				class->rq_offline(rq);
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6336) 		}
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6337) 
c6c4927b22a35 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:05 +1030 6338) 		cpumask_clear_cpu(rq->cpu, rq->rd->online);
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6339) 		rq->online = 0;
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6340) 	}
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6341) }
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6342) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6343) /*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6344)  * used to mark begin/end of suspend/resume:
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6345)  */
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6346) static int num_cpus_frozen;
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6347) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6348) /*
3a101d0548e92 kernel/sched.c      (Tejun Heo                  2010-06-08 21:40:36 +0200 6349)  * Update cpusets according to cpu_active mask.  If cpusets are
3a101d0548e92 kernel/sched.c      (Tejun Heo                  2010-06-08 21:40:36 +0200 6350)  * disabled, cpuset_update_active_cpus() becomes a simple wrapper
3a101d0548e92 kernel/sched.c      (Tejun Heo                  2010-06-08 21:40:36 +0200 6351)  * around partition_sched_domains().
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6352)  *
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6353)  * If we come here as part of a suspend/resume, don't touch cpusets because we
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6354)  * want to restore it back to its original state upon resume anyway.
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6355)  */
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6356) static void cpuset_cpu_active(void)
e761b77252342 kernel/sched.c      (Max Krasnyansky            2008-07-15 04:43:49 -0700 6357) {
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6358) 	if (cpuhp_tasks_frozen) {
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6359) 		/*
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6360) 		 * num_cpus_frozen tracks how many CPUs are involved in suspend
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6361) 		 * resume sequence. As long as this is not the last online
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6362) 		 * operation in the resume sequence, just build a single sched
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6363) 		 * domain, ignoring cpusets.
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6364) 		 */
50e76632339d4 kernel/sched/core.c (Peter Zijlstra             2017-09-07 11:13:38 +0200 6365) 		partition_sched_domains(1, NULL, NULL);
50e76632339d4 kernel/sched/core.c (Peter Zijlstra             2017-09-07 11:13:38 +0200 6366) 		if (--num_cpus_frozen)
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6367) 			return;
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6368) 		/*
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6369) 		 * This is the last CPU online operation. So fall through and
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6370) 		 * restore the original sched domains by considering the
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6371) 		 * cpuset configurations.
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6372) 		 */
50e76632339d4 kernel/sched/core.c (Peter Zijlstra             2017-09-07 11:13:38 +0200 6373) 		cpuset_force_rebuild();
3a101d0548e92 kernel/sched.c      (Tejun Heo                  2010-06-08 21:40:36 +0200 6374) 	}
30e03acda5fd9 kernel/sched/core.c (Rakib Mullick              2017-04-09 07:36:14 +0600 6375) 	cpuset_update_active_cpus();
3a101d0548e92 kernel/sched.c      (Tejun Heo                  2010-06-08 21:40:36 +0200 6376) }
e761b77252342 kernel/sched.c      (Max Krasnyansky            2008-07-15 04:43:49 -0700 6377) 
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6378) static int cpuset_cpu_inactive(unsigned int cpu)
3a101d0548e92 kernel/sched.c      (Tejun Heo                  2010-06-08 21:40:36 +0200 6379) {
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6380) 	if (!cpuhp_tasks_frozen) {
06a76fe08d4da kernel/sched/core.c (Nicolas Pitre              2017-06-21 14:22:01 -0400 6381) 		if (dl_cpu_busy(cpu))
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6382) 			return -EBUSY;
30e03acda5fd9 kernel/sched/core.c (Rakib Mullick              2017-04-09 07:36:14 +0600 6383) 		cpuset_update_active_cpus();
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6384) 	} else {
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6385) 		num_cpus_frozen++;
d35be8bab9b0c kernel/sched/core.c (Srivatsa S. Bhat           2012-05-24 19:46:26 +0530 6386) 		partition_sched_domains(1, NULL, NULL);
e761b77252342 kernel/sched.c      (Max Krasnyansky            2008-07-15 04:43:49 -0700 6387) 	}
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6388) 	return 0;
e761b77252342 kernel/sched.c      (Max Krasnyansky            2008-07-15 04:43:49 -0700 6389) }
e761b77252342 kernel/sched.c      (Max Krasnyansky            2008-07-15 04:43:49 -0700 6390) 
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6391) int sched_cpu_activate(unsigned int cpu)
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6392) {
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6393) 	struct rq *rq = cpu_rq(cpu);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6394) 	struct rq_flags rf;
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6395) 
ba2591a5993ea kernel/sched/core.c (Peter Zijlstra             2018-05-29 16:43:46 +0200 6396) #ifdef CONFIG_SCHED_SMT
ba2591a5993ea kernel/sched/core.c (Peter Zijlstra             2018-05-29 16:43:46 +0200 6397) 	/*
c5511d03ec090 kernel/sched/core.c (Peter Zijlstra (Intel)     2018-11-25 19:33:36 +0100 6398) 	 * When going up, increment the number of cores with SMT present.
ba2591a5993ea kernel/sched/core.c (Peter Zijlstra             2018-05-29 16:43:46 +0200 6399) 	 */
c5511d03ec090 kernel/sched/core.c (Peter Zijlstra (Intel)     2018-11-25 19:33:36 +0100 6400) 	if (cpumask_weight(cpu_smt_mask(cpu)) == 2)
c5511d03ec090 kernel/sched/core.c (Peter Zijlstra (Intel)     2018-11-25 19:33:36 +0100 6401) 		static_branch_inc_cpuslocked(&sched_smt_present);
ba2591a5993ea kernel/sched/core.c (Peter Zijlstra             2018-05-29 16:43:46 +0200 6402) #endif
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6403) 	set_cpu_active(cpu, true);
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6404) 
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6405) 	if (sched_smp_initialized) {
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6406) 		sched_domains_numa_masks_set(cpu);
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6407) 		cpuset_cpu_active();
e761b77252342 kernel/sched.c      (Max Krasnyansky            2008-07-15 04:43:49 -0700 6408) 	}
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6409) 
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6410) 	/*
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6411) 	 * Put the rq online, if not already. This happens:
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6412) 	 *
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6413) 	 * 1) In the early boot process, because we build the real domains
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6414) 	 *    after all CPUs have been brought up.
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6415) 	 *
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6416) 	 * 2) At runtime, if cpuset_cpu_active() fails to rebuild the
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6417) 	 *    domains.
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6418) 	 */
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6419) 	rq_lock_irqsave(rq, &rf);
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6420) 	if (rq->rd) {
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6421) 		BUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6422) 		set_rq_online(rq);
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6423) 	}
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6424) 	rq_unlock_irqrestore(rq, &rf);
7d97669933eb9 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:17 +0100 6425) 
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6426) 	return 0;
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6427) }
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6428) 
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6429) int sched_cpu_deactivate(unsigned int cpu)
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6430) {
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6431) 	int ret;
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6432) 
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6433) 	set_cpu_active(cpu, false);
b2454caa8977a kernel/sched/core.c (Peter Zijlstra             2016-03-10 12:54:14 +0100 6434) 	/*
b2454caa8977a kernel/sched/core.c (Peter Zijlstra             2016-03-10 12:54:14 +0100 6435) 	 * We've cleared cpu_active_mask, wait for all preempt-disabled and RCU
b2454caa8977a kernel/sched/core.c (Peter Zijlstra             2016-03-10 12:54:14 +0100 6436) 	 * users of this state to go away such that all new such users will
b2454caa8977a kernel/sched/core.c (Peter Zijlstra             2016-03-10 12:54:14 +0100 6437) 	 * observe it.
b2454caa8977a kernel/sched/core.c (Peter Zijlstra             2016-03-10 12:54:14 +0100 6438) 	 *
b2454caa8977a kernel/sched/core.c (Peter Zijlstra             2016-03-10 12:54:14 +0100 6439) 	 * Do sync before park smpboot threads to take care the rcu boost case.
b2454caa8977a kernel/sched/core.c (Peter Zijlstra             2016-03-10 12:54:14 +0100 6440) 	 */
309ba859b9508 kernel/sched/core.c (Paul E. McKenney           2018-07-11 14:36:49 -0700 6441) 	synchronize_rcu();
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6442) 
c5511d03ec090 kernel/sched/core.c (Peter Zijlstra (Intel)     2018-11-25 19:33:36 +0100 6443) #ifdef CONFIG_SCHED_SMT
c5511d03ec090 kernel/sched/core.c (Peter Zijlstra (Intel)     2018-11-25 19:33:36 +0100 6444) 	/*
c5511d03ec090 kernel/sched/core.c (Peter Zijlstra (Intel)     2018-11-25 19:33:36 +0100 6445) 	 * When going down, decrement the number of cores with SMT present.
c5511d03ec090 kernel/sched/core.c (Peter Zijlstra (Intel)     2018-11-25 19:33:36 +0100 6446) 	 */
c5511d03ec090 kernel/sched/core.c (Peter Zijlstra (Intel)     2018-11-25 19:33:36 +0100 6447) 	if (cpumask_weight(cpu_smt_mask(cpu)) == 2)
c5511d03ec090 kernel/sched/core.c (Peter Zijlstra (Intel)     2018-11-25 19:33:36 +0100 6448) 		static_branch_dec_cpuslocked(&sched_smt_present);
c5511d03ec090 kernel/sched/core.c (Peter Zijlstra (Intel)     2018-11-25 19:33:36 +0100 6449) #endif
c5511d03ec090 kernel/sched/core.c (Peter Zijlstra (Intel)     2018-11-25 19:33:36 +0100 6450) 
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6451) 	if (!sched_smp_initialized)
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6452) 		return 0;
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6453) 
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6454) 	ret = cpuset_cpu_inactive(cpu);
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6455) 	if (ret) {
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6456) 		set_cpu_active(cpu, true);
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6457) 		return ret;
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6458) 	}
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6459) 	sched_domains_numa_masks_clear(cpu);
40190a78f85fe kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:13 +0100 6460) 	return 0;
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6461) }
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6462) 
94baf7a5d882c kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:15 +0100 6463) static void sched_rq_cpu_starting(unsigned int cpu)
94baf7a5d882c kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:15 +0100 6464) {
94baf7a5d882c kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:15 +0100 6465) 	struct rq *rq = cpu_rq(cpu);
94baf7a5d882c kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:15 +0100 6466) 
94baf7a5d882c kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:15 +0100 6467) 	rq->calc_load_update = calc_load_update;
94baf7a5d882c kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:15 +0100 6468) 	update_max_interval();
94baf7a5d882c kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:15 +0100 6469) }
94baf7a5d882c kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:15 +0100 6470) 
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6471) int sched_cpu_starting(unsigned int cpu)
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6472) {
94baf7a5d882c kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:15 +0100 6473) 	sched_rq_cpu_starting(cpu);
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 6474) 	sched_tick_start(cpu);
135fb3e19773e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:11 +0100 6475) 	return 0;
e761b77252342 kernel/sched.c      (Max Krasnyansky            2008-07-15 04:43:49 -0700 6476) }
e761b77252342 kernel/sched.c      (Max Krasnyansky            2008-07-15 04:43:49 -0700 6477) 
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6478) #ifdef CONFIG_HOTPLUG_CPU
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6479) int sched_cpu_dying(unsigned int cpu)
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6480) {
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6481) 	struct rq *rq = cpu_rq(cpu);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6482) 	struct rq_flags rf;
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6483) 
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6484) 	/* Handle pending wakeups and then migrate everything off */
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6485) 	sched_ttwu_pending();
d84b31313ef8a kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:27 +0100 6486) 	sched_tick_stop(cpu);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6487) 
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6488) 	rq_lock_irqsave(rq, &rf);
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6489) 	if (rq->rd) {
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6490) 		BUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6491) 		set_rq_offline(rq);
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6492) 	}
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6493) 	migrate_tasks(rq, &rf);
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6494) 	BUG_ON(rq->nr_running != 1);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6495) 	rq_unlock_irqrestore(rq, &rf);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 6496) 
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6497) 	calc_load_migrate(rq);
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6498) 	update_max_interval();
00357f5ec5d67 kernel/sched/core.c (Peter Zijlstra             2017-12-21 15:06:50 +0100 6499) 	nohz_balance_exit_idle(rq);
e5ef27d0f5acf kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:21 +0100 6500) 	hrtick_clear(rq);
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6501) 	return 0;
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6502) }
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6503) #endif
f2785ddb5367e kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:18 +0100 6504) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6505) void __init sched_init_smp(void)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6506) {
cb83b629bae03 kernel/sched/core.c (Peter Zijlstra             2012-04-17 15:49:36 +0200 6507) 	sched_init_numa();
cb83b629bae03 kernel/sched/core.c (Peter Zijlstra             2012-04-17 15:49:36 +0200 6508) 
6acce3ef84520 kernel/sched/core.c (Peter Zijlstra             2013-10-11 14:38:20 +0200 6509) 	/*
6acce3ef84520 kernel/sched/core.c (Peter Zijlstra             2013-10-11 14:38:20 +0200 6510) 	 * There's no userspace yet to cause hotplug operations; hence all the
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6511) 	 * CPU masks are stable and all blatant races in the below code cannot
b5a4e2bb0f4c8 kernel/sched/core.c (Valentin Schneider         2018-12-19 18:23:16 +0000 6512) 	 * happen.
6acce3ef84520 kernel/sched/core.c (Peter Zijlstra             2013-10-11 14:38:20 +0200 6513) 	 */
712555ee4f873 kernel/sched.c      (Heiko Carstens             2008-04-28 11:33:07 +0200 6514) 	mutex_lock(&sched_domains_mutex);
8d5dc5126bb2b kernel/sched/core.c (Peter Zijlstra             2017-04-25 15:29:40 +0200 6515) 	sched_init_domains(cpu_active_mask);
712555ee4f873 kernel/sched.c      (Heiko Carstens             2008-04-28 11:33:07 +0200 6516) 	mutex_unlock(&sched_domains_mutex);
e761b77252342 kernel/sched.c      (Max Krasnyansky            2008-07-15 04:43:49 -0700 6517) 
5c1e176781f43 kernel/sched.c      (Nick Piggin                2006-10-03 01:14:04 -0700 6518) 	/* Move init over to a non-isolated CPU */
edb9382175c3e kernel/sched/core.c (Frederic Weisbecker        2017-10-27 04:42:37 +0200 6519) 	if (set_cpus_allowed_ptr(current, housekeeping_cpumask(HK_FLAG_DOMAIN)) < 0)
5c1e176781f43 kernel/sched.c      (Nick Piggin                2006-10-03 01:14:04 -0700 6520) 		BUG();
19978ca610946 kernel/sched.c      (Ingo Molnar                2007-11-09 22:39:38 +0100 6521) 	sched_init_granularity();
4212823fb459e kernel/sched.c      (Rusty Russell              2008-11-25 02:35:12 +1030 6522) 
0e3900e6d3b04 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:13 +1030 6523) 	init_sched_rt_class();
1baca4ce16b8c kernel/sched/core.c (Juri Lelli                 2013-11-07 14:43:38 +0100 6524) 	init_sched_dl_class();
1b568f0aabf28 kernel/sched/core.c (Peter Zijlstra             2016-05-09 10:38:41 +0200 6525) 
e26fbffd32c28 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:10 +0100 6526) 	sched_smp_initialized = true;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6527) }
e26fbffd32c28 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:10 +0100 6528) 
e26fbffd32c28 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:10 +0100 6529) static int __init migration_init(void)
e26fbffd32c28 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:10 +0100 6530) {
77a5352ba977d kernel/sched/core.c (Nicholas Piggin            2019-04-11 13:34:44 +1000 6531) 	sched_cpu_starting(smp_processor_id());
e26fbffd32c28 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:10 +0100 6532) 	return 0;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6533) }
e26fbffd32c28 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:10 +0100 6534) early_initcall(migration_init);
e26fbffd32c28 kernel/sched/core.c (Thomas Gleixner            2016-03-10 12:54:10 +0100 6535) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6536) #else
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6537) void __init sched_init_smp(void)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6538) {
19978ca610946 kernel/sched.c      (Ingo Molnar                2007-11-09 22:39:38 +0100 6539) 	sched_init_granularity();
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6540) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6541) #endif /* CONFIG_SMP */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6542) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6543) int in_sched_functions(unsigned long addr)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6544) {
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6545) 	return in_lock_functions(addr) ||
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6546) 		(addr >= (unsigned long)__sched_text_start
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6547) 		&& addr < (unsigned long)__sched_text_end);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6548) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6549) 
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 6550) #ifdef CONFIG_CGROUP_SCHED
27b4b9319a3c2 kernel/sched/core.c (Li Zefan                   2013-03-05 16:07:52 +0800 6551) /*
27b4b9319a3c2 kernel/sched/core.c (Li Zefan                   2013-03-05 16:07:52 +0800 6552)  * Default task group.
27b4b9319a3c2 kernel/sched/core.c (Li Zefan                   2013-03-05 16:07:52 +0800 6553)  * Every task in system belongs to this group at bootup.
27b4b9319a3c2 kernel/sched/core.c (Li Zefan                   2013-03-05 16:07:52 +0800 6554)  */
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 6555) struct task_group root_task_group;
35cf4e50b1633 kernel/sched/core.c (Mike Galbraith             2012-08-07 05:00:13 +0200 6556) LIST_HEAD(task_groups);
b0367629acf62 kernel/sched/core.c (Waiman Long                2015-12-02 13:41:49 -0500 6557) 
b0367629acf62 kernel/sched/core.c (Waiman Long                2015-12-02 13:41:49 -0500 6558) /* Cacheline aligned slab cache for task_group */
b0367629acf62 kernel/sched/core.c (Waiman Long                2015-12-02 13:41:49 -0500 6559) static struct kmem_cache *task_group_cache __read_mostly;
052f1dc7eb023 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6560) #endif
6f505b16425a5 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:30 +0100 6561) 
e6252c3ef4b9c kernel/sched/core.c (Joonsoo Kim                2013-04-23 17:27:41 +0900 6562) DECLARE_PER_CPU(cpumask_var_t, load_balance_mask);
10e2f1acd0106 kernel/sched/core.c (Peter Zijlstra             2016-05-09 10:38:05 +0200 6563) DECLARE_PER_CPU(cpumask_var_t, select_idle_mask);
6f505b16425a5 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:30 +0100 6564) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6565) void __init sched_init(void)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6566) {
a1dc0446d6496 kernel/sched/core.c (Qian Cai                   2019-07-19 21:23:19 -0400 6567) 	unsigned long ptr = 0;
55627e3cd22c3 kernel/sched/core.c (Dietmar Eggemann           2019-05-27 07:21:13 +0100 6568) 	int i;
434d53b00d6bb kernel/sched.c      (Mike Travis                2008-04-04 18:11:04 -0700 6569) 
5822a454d6d22 kernel/sched/core.c (Ingo Molnar                2017-03-05 13:09:07 +0100 6570) 	wait_bit_init();
9dcb8b685fc30 kernel/sched/core.c (Linus Torvalds             2016-10-26 10:15:30 -0700 6571) 
434d53b00d6bb kernel/sched.c      (Mike Travis                2008-04-04 18:11:04 -0700 6572) #ifdef CONFIG_FAIR_GROUP_SCHED
a1dc0446d6496 kernel/sched/core.c (Qian Cai                   2019-07-19 21:23:19 -0400 6573) 	ptr += 2 * nr_cpu_ids * sizeof(void **);
434d53b00d6bb kernel/sched.c      (Mike Travis                2008-04-04 18:11:04 -0700 6574) #endif
434d53b00d6bb kernel/sched.c      (Mike Travis                2008-04-04 18:11:04 -0700 6575) #ifdef CONFIG_RT_GROUP_SCHED
a1dc0446d6496 kernel/sched/core.c (Qian Cai                   2019-07-19 21:23:19 -0400 6576) 	ptr += 2 * nr_cpu_ids * sizeof(void **);
434d53b00d6bb kernel/sched.c      (Mike Travis                2008-04-04 18:11:04 -0700 6577) #endif
a1dc0446d6496 kernel/sched/core.c (Qian Cai                   2019-07-19 21:23:19 -0400 6578) 	if (ptr) {
a1dc0446d6496 kernel/sched/core.c (Qian Cai                   2019-07-19 21:23:19 -0400 6579) 		ptr = (unsigned long)kzalloc(ptr, GFP_NOWAIT);
434d53b00d6bb kernel/sched.c      (Mike Travis                2008-04-04 18:11:04 -0700 6580) 
434d53b00d6bb kernel/sched.c      (Mike Travis                2008-04-04 18:11:04 -0700 6581) #ifdef CONFIG_FAIR_GROUP_SCHED
07e06b011db2b kernel/sched.c      (Yong Zhang                 2011-01-07 15:17:36 +0800 6582) 		root_task_group.se = (struct sched_entity **)ptr;
434d53b00d6bb kernel/sched.c      (Mike Travis                2008-04-04 18:11:04 -0700 6583) 		ptr += nr_cpu_ids * sizeof(void **);
434d53b00d6bb kernel/sched.c      (Mike Travis                2008-04-04 18:11:04 -0700 6584) 
07e06b011db2b kernel/sched.c      (Yong Zhang                 2011-01-07 15:17:36 +0800 6585) 		root_task_group.cfs_rq = (struct cfs_rq **)ptr;
434d53b00d6bb kernel/sched.c      (Mike Travis                2008-04-04 18:11:04 -0700 6586) 		ptr += nr_cpu_ids * sizeof(void **);
eff766a65c602 kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200 6587) 
6d6bc0ad867c4 kernel/sched.c      (Dhaval Giani               2008-05-30 14:23:45 +0200 6588) #endif /* CONFIG_FAIR_GROUP_SCHED */
434d53b00d6bb kernel/sched.c      (Mike Travis                2008-04-04 18:11:04 -0700 6589) #ifdef CONFIG_RT_GROUP_SCHED
07e06b011db2b kernel/sched.c      (Yong Zhang                 2011-01-07 15:17:36 +0800 6590) 		root_task_group.rt_se = (struct sched_rt_entity **)ptr;
434d53b00d6bb kernel/sched.c      (Mike Travis                2008-04-04 18:11:04 -0700 6591) 		ptr += nr_cpu_ids * sizeof(void **);
434d53b00d6bb kernel/sched.c      (Mike Travis                2008-04-04 18:11:04 -0700 6592) 
07e06b011db2b kernel/sched.c      (Yong Zhang                 2011-01-07 15:17:36 +0800 6593) 		root_task_group.rt_rq = (struct rt_rq **)ptr;
eff766a65c602 kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200 6594) 		ptr += nr_cpu_ids * sizeof(void **);
eff766a65c602 kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200 6595) 
6d6bc0ad867c4 kernel/sched.c      (Dhaval Giani               2008-05-30 14:23:45 +0200 6596) #endif /* CONFIG_RT_GROUP_SCHED */
b74e6278fd6db kernel/sched/core.c (Alex Thorlton              2014-12-18 12:44:30 -0600 6597) 	}
df7c8e845e8e2 kernel/sched.c      (Rusty Russell              2009-03-19 15:22:20 +1030 6598) #ifdef CONFIG_CPUMASK_OFFSTACK
b74e6278fd6db kernel/sched/core.c (Alex Thorlton              2014-12-18 12:44:30 -0600 6599) 	for_each_possible_cpu(i) {
b74e6278fd6db kernel/sched/core.c (Alex Thorlton              2014-12-18 12:44:30 -0600 6600) 		per_cpu(load_balance_mask, i) = (cpumask_var_t)kzalloc_node(
b74e6278fd6db kernel/sched/core.c (Alex Thorlton              2014-12-18 12:44:30 -0600 6601) 			cpumask_size(), GFP_KERNEL, cpu_to_node(i));
10e2f1acd0106 kernel/sched/core.c (Peter Zijlstra             2016-05-09 10:38:05 +0200 6602) 		per_cpu(select_idle_mask, i) = (cpumask_var_t)kzalloc_node(
10e2f1acd0106 kernel/sched/core.c (Peter Zijlstra             2016-05-09 10:38:05 +0200 6603) 			cpumask_size(), GFP_KERNEL, cpu_to_node(i));
434d53b00d6bb kernel/sched.c      (Mike Travis                2008-04-04 18:11:04 -0700 6604) 	}
b74e6278fd6db kernel/sched/core.c (Alex Thorlton              2014-12-18 12:44:30 -0600 6605) #endif /* CONFIG_CPUMASK_OFFSTACK */
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6606) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6607) 	init_rt_bandwidth(&def_rt_bandwidth, global_rt_period(), global_rt_runtime());
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6608) 	init_dl_bandwidth(&def_dl_bandwidth, global_rt_period(), global_rt_runtime());
332ac17ef5bfc kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:45 +0100 6609) 
57d885fea0da0 kernel/sched.c      (Gregory Haskins            2008-01-25 21:08:18 +0100 6610) #ifdef CONFIG_SMP
57d885fea0da0 kernel/sched.c      (Gregory Haskins            2008-01-25 21:08:18 +0100 6611) 	init_defrootdomain();
57d885fea0da0 kernel/sched.c      (Gregory Haskins            2008-01-25 21:08:18 +0100 6612) #endif
57d885fea0da0 kernel/sched.c      (Gregory Haskins            2008-01-25 21:08:18 +0100 6613) 
d0b27fa77854b kernel/sched.c      (Peter Zijlstra             2008-04-19 19:44:57 +0200 6614) #ifdef CONFIG_RT_GROUP_SCHED
07e06b011db2b kernel/sched.c      (Yong Zhang                 2011-01-07 15:17:36 +0800 6615) 	init_rt_bandwidth(&root_task_group.rt_bandwidth,
d0b27fa77854b kernel/sched.c      (Peter Zijlstra             2008-04-19 19:44:57 +0200 6616) 			global_rt_period(), global_rt_runtime());
6d6bc0ad867c4 kernel/sched.c      (Dhaval Giani               2008-05-30 14:23:45 +0200 6617) #endif /* CONFIG_RT_GROUP_SCHED */
d0b27fa77854b kernel/sched.c      (Peter Zijlstra             2008-04-19 19:44:57 +0200 6618) 
7c9414385ebfd kernel/sched.c      (Dhaval Giani               2010-01-20 13:26:18 +0100 6619) #ifdef CONFIG_CGROUP_SCHED
b0367629acf62 kernel/sched/core.c (Waiman Long                2015-12-02 13:41:49 -0500 6620) 	task_group_cache = KMEM_CACHE(task_group, 0);
b0367629acf62 kernel/sched/core.c (Waiman Long                2015-12-02 13:41:49 -0500 6621) 
07e06b011db2b kernel/sched.c      (Yong Zhang                 2011-01-07 15:17:36 +0800 6622) 	list_add(&root_task_group.list, &task_groups);
07e06b011db2b kernel/sched.c      (Yong Zhang                 2011-01-07 15:17:36 +0800 6623) 	INIT_LIST_HEAD(&root_task_group.children);
f4d6f6c2649c2 kernel/sched.c      (Glauber Costa              2011-11-01 19:19:07 -0200 6624) 	INIT_LIST_HEAD(&root_task_group.siblings);
5091faa449ee0 kernel/sched.c      (Mike Galbraith             2010-11-30 14:18:03 +0100 6625) 	autogroup_init(&init_task);
7c9414385ebfd kernel/sched.c      (Dhaval Giani               2010-01-20 13:26:18 +0100 6626) #endif /* CONFIG_CGROUP_SCHED */
6f505b16425a5 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:30 +0100 6627) 
0a945022778f1 kernel/sched.c      (KAMEZAWA Hiroyuki          2006-03-28 01:56:37 -0800 6628) 	for_each_possible_cpu(i) {
70b97a7f0b19c kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:42 -0700 6629) 		struct rq *rq;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6630) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6631) 		rq = cpu_rq(i);
05fa785cf80c9 kernel/sched.c      (Thomas Gleixner            2009-11-17 14:28:38 +0100 6632) 		raw_spin_lock_init(&rq->lock);
7897986bad8f6 kernel/sched.c      (Nick Piggin                2005-06-25 14:57:13 -0700 6633) 		rq->nr_running = 0;
dce48a84adf18 kernel/sched.c      (Thomas Gleixner            2009-04-11 10:43:41 +0200 6634) 		rq->calc_load_active = 0;
dce48a84adf18 kernel/sched.c      (Thomas Gleixner            2009-04-11 10:43:41 +0200 6635) 		rq->calc_load_update = jiffies + LOAD_FREQ;
acb5a9ba3bd7c kernel/sched.c      (Jan H. Schnherr           2011-07-14 18:32:43 +0200 6636) 		init_cfs_rq(&rq->cfs);
07c54f7a7ff77 kernel/sched/core.c (Abel Vesa                  2015-03-03 13:50:27 +0200 6637) 		init_rt_rq(&rq->rt);
07c54f7a7ff77 kernel/sched/core.c (Abel Vesa                  2015-03-03 13:50:27 +0200 6638) 		init_dl_rq(&rq->dl);
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6639) #ifdef CONFIG_FAIR_GROUP_SCHED
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 6640) 		root_task_group.shares = ROOT_TASK_GROUP_LOAD;
6f505b16425a5 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:30 +0100 6641) 		INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);
9c2791f936ef5 kernel/sched/core.c (Vincent Guittot            2016-11-08 10:53:43 +0100 6642) 		rq->tmp_alone_branch = &rq->leaf_cfs_rq_list;
354d60c2ff72d kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6643) 		/*
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6644) 		 * How much CPU bandwidth does root_task_group get?
354d60c2ff72d kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6645) 		 *
354d60c2ff72d kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6646) 		 * In case of task-groups formed thr' the cgroup filesystem, it
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6647) 		 * gets 100% of the CPU resources in the system. This overall
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6648) 		 * system CPU resource is divided among the tasks of
07e06b011db2b kernel/sched.c      (Yong Zhang                 2011-01-07 15:17:36 +0800 6649) 		 * root_task_group and its child task-groups in a fair manner,
354d60c2ff72d kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6650) 		 * based on each entity's (task or task-group's) weight
354d60c2ff72d kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6651) 		 * (se->load.weight).
354d60c2ff72d kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6652) 		 *
07e06b011db2b kernel/sched.c      (Yong Zhang                 2011-01-07 15:17:36 +0800 6653) 		 * In other words, if root_task_group has 10 tasks of weight
354d60c2ff72d kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6654) 		 * 1024) and two child groups A0 and A1 (of weight 1024 each),
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6655) 		 * then A0's share of the CPU resource is:
354d60c2ff72d kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6656) 		 *
0d905bca23aca kernel/sched.c      (Ingo Molnar                2009-05-04 19:13:30 +0200 6657) 		 *	A0's bandwidth = 1024 / (10*1024 + 1024 + 1024) = 8.33%
354d60c2ff72d kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6658) 		 *
07e06b011db2b kernel/sched.c      (Yong Zhang                 2011-01-07 15:17:36 +0800 6659) 		 * We achieve this by letting root_task_group's tasks sit
07e06b011db2b kernel/sched.c      (Yong Zhang                 2011-01-07 15:17:36 +0800 6660) 		 * directly in rq->cfs (i.e root_task_group->se[] = NULL).
354d60c2ff72d kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6661) 		 */
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 6662) 		init_cfs_bandwidth(&root_task_group.cfs_bandwidth);
07e06b011db2b kernel/sched.c      (Yong Zhang                 2011-01-07 15:17:36 +0800 6663) 		init_tg_cfs_entry(&root_task_group, &rq->cfs, NULL, i, NULL);
354d60c2ff72d kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6664) #endif /* CONFIG_FAIR_GROUP_SCHED */
354d60c2ff72d kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6665) 
354d60c2ff72d kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6666) 		rq->rt.rt_runtime = def_rt_bandwidth.rt_runtime;
052f1dc7eb023 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6667) #ifdef CONFIG_RT_GROUP_SCHED
07e06b011db2b kernel/sched.c      (Yong Zhang                 2011-01-07 15:17:36 +0800 6668) 		init_tg_rt_entry(&root_task_group, &rq->rt, NULL, i, NULL);
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6669) #endif
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6670) #ifdef CONFIG_SMP
41c7ce9ad9a85 kernel/sched.c      (Nick Piggin                2005-06-25 14:57:24 -0700 6671) 		rq->sd = NULL;
57d885fea0da0 kernel/sched.c      (Gregory Haskins            2008-01-25 21:08:18 +0100 6672) 		rq->rd = NULL;
ca6d75e6908ef kernel/sched/core.c (Vincent Guittot            2015-02-27 16:54:09 +0100 6673) 		rq->cpu_capacity = rq->cpu_capacity_orig = SCHED_CAPACITY_SCALE;
e3fca9e7cbfb7 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:37 +0200 6674) 		rq->balance_callback = NULL;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6675) 		rq->active_balance = 0;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6676) 		rq->next_balance = jiffies;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6677) 		rq->push_cpu = 0;
0a2966b48fb78 kernel/sched.c      (Christoph Lameter          2006-09-25 23:30:51 -0700 6678) 		rq->cpu = i;
1f11eb6a8bc92 kernel/sched.c      (Gregory Haskins            2008-06-04 15:04:05 -0400 6679) 		rq->online = 0;
eae0c9dfb534c kernel/sched.c      (Mike Galbraith             2009-11-10 03:50:02 +0100 6680) 		rq->idle_stamp = 0;
eae0c9dfb534c kernel/sched.c      (Mike Galbraith             2009-11-10 03:50:02 +0100 6681) 		rq->avg_idle = 2*sysctl_sched_migration_cost;
9bd721c55c8a8 kernel/sched/core.c (Jason Low                  2013-09-13 11:26:52 -0700 6682) 		rq->max_idle_balance_cost = sysctl_sched_migration_cost;
367456c756a6b kernel/sched/core.c (Peter Zijlstra             2012-02-20 21:49:09 +0100 6683) 
367456c756a6b kernel/sched/core.c (Peter Zijlstra             2012-02-20 21:49:09 +0100 6684) 		INIT_LIST_HEAD(&rq->cfs_tasks);
367456c756a6b kernel/sched/core.c (Peter Zijlstra             2012-02-20 21:49:09 +0100 6685) 
dc938520d2bf3 kernel/sched.c      (Gregory Haskins            2008-01-25 21:08:26 +0100 6686) 		rq_attach_root(rq, &def_root_domain);
3451d0243c3cd kernel/sched/core.c (Frederic Weisbecker        2011-08-10 23:21:01 +0200 6687) #ifdef CONFIG_NO_HZ_COMMON
9fd81dd5ce0b1 kernel/sched/core.c (Frederic Weisbecker        2016-04-19 17:36:51 +0200 6688) 		rq->last_load_update_tick = jiffies;
e022e0d38ad47 kernel/sched/core.c (Peter Zijlstra             2017-12-21 11:20:23 +0100 6689) 		rq->last_blocked_load_update_tick = jiffies;
a22e47a4e3f5a kernel/sched/core.c (Peter Zijlstra             2017-12-21 10:01:24 +0100 6690) 		atomic_set(&rq->nohz_flags, 0);
83cd4fe27ad84 kernel/sched.c      (Venkatesh Pallipadi        2010-05-21 17:09:41 -0700 6691) #endif
9fd81dd5ce0b1 kernel/sched/core.c (Frederic Weisbecker        2016-04-19 17:36:51 +0200 6692) #endif /* CONFIG_SMP */
77a021be383eb kernel/sched/core.c (Frederic Weisbecker        2018-02-21 05:17:23 +0100 6693) 		hrtick_rq_init(rq);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6694) 		atomic_set(&rq->nr_iowait, 0);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6695) 	}
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6696) 
9059393e4ec1c kernel/sched/core.c (Vincent Guittot            2017-05-17 11:50:45 +0200 6697) 	set_load_weight(&init_task, false);
b50f60ceeef2e kernel/sched.c      (Heiko Carstens             2006-07-30 03:03:52 -0700 6698) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6699) 	/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6700) 	 * The boot idle thread does lazy MMU switching as well:
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6701) 	 */
f1f1007644ffc kernel/sched/core.c (Vegard Nossum              2017-02-27 14:30:07 -0800 6702) 	mmgrab(&init_mm);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6703) 	enter_lazy_tlb(&init_mm, current);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6704) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6705) 	/*
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6706) 	 * Make us the idle thread. Technically, schedule() should not be
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6707) 	 * called from this thread, however somewhere below it might be,
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6708) 	 * but because we are the idle thread, we just pick up running again
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6709) 	 * when this runqueue becomes "idle".
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6710) 	 */
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6711) 	init_idle(current, smp_processor_id());
dce48a84adf18 kernel/sched.c      (Thomas Gleixner            2009-04-11 10:43:41 +0200 6712) 
dce48a84adf18 kernel/sched.c      (Thomas Gleixner            2009-04-11 10:43:41 +0200 6713) 	calc_load_update = jiffies + LOAD_FREQ;
dce48a84adf18 kernel/sched.c      (Thomas Gleixner            2009-04-11 10:43:41 +0200 6714) 
bf4d83f664760 kernel/sched.c      (Rusty Russell              2008-11-25 09:57:51 +1030 6715) #ifdef CONFIG_SMP
29d5e0476e1c4 kernel/sched/core.c (Thomas Gleixner            2012-04-20 13:05:45 +0000 6716) 	idle_thread_set_boot_cpu();
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 6717) #endif
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 6718) 	init_sched_fair_class();
6a7b3dc3440f7 kernel/sched.c      (Rusty Russell              2008-11-25 02:35:04 +1030 6719) 
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 6720) 	init_schedstats();
4698f88c06b89 kernel/sched/core.c (Josh Poimboeuf             2016-06-07 14:43:16 -0500 6721) 
eb414681d5a07 kernel/sched/core.c (Johannes Weiner            2018-10-26 15:06:27 -0700 6722) 	psi_init();
eb414681d5a07 kernel/sched/core.c (Johannes Weiner            2018-10-26 15:06:27 -0700 6723) 
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 6724) 	init_uclamp();
69842cba9ace8 kernel/sched/core.c (Patrick Bellasi            2019-06-21 09:42:02 +0100 6725) 
6892b75e60557 kernel/sched.c      (Ingo Molnar                2008-02-13 14:02:36 +0100 6726) 	scheduler_running = 1;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6727) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6728) 
d902db1eb6038 kernel/sched.c      (Frederic Weisbecker        2011-06-08 19:31:56 +0200 6729) #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
e4aafea2d4bde kernel/sched.c      (Frederic Weisbecker        2009-07-16 15:44:29 +0200 6730) static inline int preempt_count_equals(int preempt_offset)
e4aafea2d4bde kernel/sched.c      (Frederic Weisbecker        2009-07-16 15:44:29 +0200 6731) {
da7142e2ed735 kernel/sched/core.c (Peter Zijlstra             2015-09-28 18:11:45 +0200 6732) 	int nested = preempt_count() + rcu_preempt_depth();
e4aafea2d4bde kernel/sched.c      (Frederic Weisbecker        2009-07-16 15:44:29 +0200 6733) 
4ba8216cd9056 kernel/sched.c      (Arnd Bergmann              2011-01-25 22:52:22 +0100 6734) 	return (nested == preempt_offset);
e4aafea2d4bde kernel/sched.c      (Frederic Weisbecker        2009-07-16 15:44:29 +0200 6735) }
e4aafea2d4bde kernel/sched.c      (Frederic Weisbecker        2009-07-16 15:44:29 +0200 6736) 
d894837f23f49 kernel/sched.c      (Simon Kagstrom             2009-12-23 11:08:18 +0100 6737) void __might_sleep(const char *file, int line, int preempt_offset)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6738) {
8eb23b9f35aae kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:55 +0200 6739) 	/*
8eb23b9f35aae kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:55 +0200 6740) 	 * Blocking primitives will set (and therefore destroy) current->state,
8eb23b9f35aae kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:55 +0200 6741) 	 * since we will exit with TASK_RUNNING make sure we enter with it,
8eb23b9f35aae kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:55 +0200 6742) 	 * otherwise we will destroy state.
8eb23b9f35aae kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:55 +0200 6743) 	 */
00845eb968ead kernel/sched/core.c (Linus Torvalds             2015-02-01 12:23:32 -0800 6744) 	WARN_ONCE(current->state != TASK_RUNNING && current->task_state_change,
8eb23b9f35aae kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:55 +0200 6745) 			"do not call blocking ops when !TASK_RUNNING; "
8eb23b9f35aae kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:55 +0200 6746) 			"state=%lx set at [<%p>] %pS\n",
8eb23b9f35aae kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:55 +0200 6747) 			current->state,
8eb23b9f35aae kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:55 +0200 6748) 			(void *)current->task_state_change,
00845eb968ead kernel/sched/core.c (Linus Torvalds             2015-02-01 12:23:32 -0800 6749) 			(void *)current->task_state_change);
8eb23b9f35aae kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:55 +0200 6750) 
3427445afd26b kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:56 +0200 6751) 	___might_sleep(file, line, preempt_offset);
3427445afd26b kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:56 +0200 6752) }
3427445afd26b kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:56 +0200 6753) EXPORT_SYMBOL(__might_sleep);
3427445afd26b kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:56 +0200 6754) 
3427445afd26b kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:56 +0200 6755) void ___might_sleep(const char *file, int line, int preempt_offset)
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6756) {
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6757) 	/* Ratelimiting timestamp: */
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6758) 	static unsigned long prev_jiffy;
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6759) 
d1c6d149cf04d kernel/sched/core.c (Vegard Nossum              2016-07-23 09:46:39 +0200 6760) 	unsigned long preempt_disable_ip;
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6761) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6762) 	/* WARN_ON_ONCE() by default, no rate limit required: */
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6763) 	rcu_sleep_check();
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6764) 
db273be2a7d42 kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:38 +0100 6765) 	if ((preempt_count_equals(preempt_offset) && !irqs_disabled() &&
312364f3534cc kernel/sched/core.c (Daniel Vetter              2019-08-26 22:14:23 +0200 6766) 	     !is_idle_task(current) && !current->non_block_count) ||
1c3c5eab17159 kernel/sched/core.c (Thomas Gleixner            2017-05-16 20:42:48 +0200 6767) 	    system_state == SYSTEM_BOOTING || system_state > SYSTEM_RUNNING ||
1c3c5eab17159 kernel/sched/core.c (Thomas Gleixner            2017-05-16 20:42:48 +0200 6768) 	    oops_in_progress)
aef745fca016a kernel/sched.c      (Ingo Molnar                2008-08-28 11:34:43 +0200 6769) 		return;
1c3c5eab17159 kernel/sched/core.c (Thomas Gleixner            2017-05-16 20:42:48 +0200 6770) 
aef745fca016a kernel/sched.c      (Ingo Molnar                2008-08-28 11:34:43 +0200 6771) 	if (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)
aef745fca016a kernel/sched.c      (Ingo Molnar                2008-08-28 11:34:43 +0200 6772) 		return;
aef745fca016a kernel/sched.c      (Ingo Molnar                2008-08-28 11:34:43 +0200 6773) 	prev_jiffy = jiffies;
aef745fca016a kernel/sched.c      (Ingo Molnar                2008-08-28 11:34:43 +0200 6774) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6775) 	/* Save this before calling printk(), since that will clobber it: */
d1c6d149cf04d kernel/sched/core.c (Vegard Nossum              2016-07-23 09:46:39 +0200 6776) 	preempt_disable_ip = get_preempt_disable_ip(current);
d1c6d149cf04d kernel/sched/core.c (Vegard Nossum              2016-07-23 09:46:39 +0200 6777) 
3df0fc5b2e9d8 kernel/sched.c      (Peter Zijlstra             2009-12-20 14:23:57 +0100 6778) 	printk(KERN_ERR
3df0fc5b2e9d8 kernel/sched.c      (Peter Zijlstra             2009-12-20 14:23:57 +0100 6779) 		"BUG: sleeping function called from invalid context at %s:%d\n",
3df0fc5b2e9d8 kernel/sched.c      (Peter Zijlstra             2009-12-20 14:23:57 +0100 6780) 			file, line);
3df0fc5b2e9d8 kernel/sched.c      (Peter Zijlstra             2009-12-20 14:23:57 +0100 6781) 	printk(KERN_ERR
312364f3534cc kernel/sched/core.c (Daniel Vetter              2019-08-26 22:14:23 +0200 6782) 		"in_atomic(): %d, irqs_disabled(): %d, non_block: %d, pid: %d, name: %s\n",
312364f3534cc kernel/sched/core.c (Daniel Vetter              2019-08-26 22:14:23 +0200 6783) 			in_atomic(), irqs_disabled(), current->non_block_count,
3df0fc5b2e9d8 kernel/sched.c      (Peter Zijlstra             2009-12-20 14:23:57 +0100 6784) 			current->pid, current->comm);
aef745fca016a kernel/sched.c      (Ingo Molnar                2008-08-28 11:34:43 +0200 6785) 
a8b686b3af441 kernel/sched/core.c (Eric Sandeen               2014-12-16 16:25:28 -0600 6786) 	if (task_stack_end_corrupted(current))
a8b686b3af441 kernel/sched/core.c (Eric Sandeen               2014-12-16 16:25:28 -0600 6787) 		printk(KERN_EMERG "Thread overran stack, or stack corrupted\n");
a8b686b3af441 kernel/sched/core.c (Eric Sandeen               2014-12-16 16:25:28 -0600 6788) 
aef745fca016a kernel/sched.c      (Ingo Molnar                2008-08-28 11:34:43 +0200 6789) 	debug_show_held_locks(current);
aef745fca016a kernel/sched.c      (Ingo Molnar                2008-08-28 11:34:43 +0200 6790) 	if (irqs_disabled())
aef745fca016a kernel/sched.c      (Ingo Molnar                2008-08-28 11:34:43 +0200 6791) 		print_irqtrace_events(current);
d1c6d149cf04d kernel/sched/core.c (Vegard Nossum              2016-07-23 09:46:39 +0200 6792) 	if (IS_ENABLED(CONFIG_DEBUG_PREEMPT)
d1c6d149cf04d kernel/sched/core.c (Vegard Nossum              2016-07-23 09:46:39 +0200 6793) 	    && !preempt_count_equals(preempt_offset)) {
8f47b1871b8aa kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:39 +0100 6794) 		pr_err("Preemption disabled at:");
d1c6d149cf04d kernel/sched/core.c (Vegard Nossum              2016-07-23 09:46:39 +0200 6795) 		print_ip_sym(preempt_disable_ip);
8f47b1871b8aa kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:39 +0100 6796) 		pr_cont("\n");
8f47b1871b8aa kernel/sched/core.c (Thomas Gleixner            2014-02-07 20:58:39 +0100 6797) 	}
aef745fca016a kernel/sched.c      (Ingo Molnar                2008-08-28 11:34:43 +0200 6798) 	dump_stack();
f0b22e39e3409 kernel/sched/core.c (Vegard Nossum              2016-07-22 21:46:02 +0200 6799) 	add_taint(TAINT_WARN, LOCKDEP_STILL_OK);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6800) }
3427445afd26b kernel/sched/core.c (Peter Zijlstra             2014-09-24 10:18:56 +0200 6801) EXPORT_SYMBOL(___might_sleep);
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6802) 
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6803) void __cant_sleep(const char *file, int line, int preempt_offset)
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6804) {
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6805) 	static unsigned long prev_jiffy;
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6806) 
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6807) 	if (irqs_disabled())
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6808) 		return;
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6809) 
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6810) 	if (!IS_ENABLED(CONFIG_PREEMPT_COUNT))
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6811) 		return;
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6812) 
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6813) 	if (preempt_count() > preempt_offset)
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6814) 		return;
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6815) 
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6816) 	if (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6817) 		return;
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6818) 	prev_jiffy = jiffies;
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6819) 
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6820) 	printk(KERN_ERR "BUG: assuming atomic context at %s:%d\n", file, line);
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6821) 	printk(KERN_ERR "in_atomic(): %d, irqs_disabled(): %d, pid: %d, name: %s\n",
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6822) 			in_atomic(), irqs_disabled(),
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6823) 			current->pid, current->comm);
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6824) 
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6825) 	debug_show_held_locks(current);
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6826) 	dump_stack();
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6827) 	add_taint(TAINT_WARN, LOCKDEP_STILL_OK);
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6828) }
568f196756ad9 kernel/sched/core.c (Peter Zijlstra             2019-01-28 17:21:52 -0800 6829) EXPORT_SYMBOL_GPL(__cant_sleep);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6830) #endif
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6831) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6832) #ifdef CONFIG_MAGIC_SYSRQ
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 6833) void normalize_rt_tasks(void)
3a5e4dc12f23f kernel/sched.c      (Andi Kleen                 2007-10-15 17:00:15 +0200 6834) {
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 6835) 	struct task_struct *g, *p;
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 6836) 	struct sched_attr attr = {
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 6837) 		.sched_policy = SCHED_NORMAL,
d50dde5a10f30 kernel/sched/core.c (Dario Faggioli             2013-11-07 14:43:36 +0100 6838) 	};
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6839) 
3472eaa1f12e2 kernel/sched/core.c (Oleg Nesterov              2014-09-21 21:33:38 +0200 6840) 	read_lock(&tasklist_lock);
5d07f4202c5d6 kernel/sched/core.c (Oleg Nesterov              2014-08-13 21:19:53 +0200 6841) 	for_each_process_thread(g, p) {
178be793485d7 kernel/sched.c      (Ingo Molnar                2007-10-15 17:00:18 +0200 6842) 		/*
178be793485d7 kernel/sched.c      (Ingo Molnar                2007-10-15 17:00:18 +0200 6843) 		 * Only normalize user tasks:
178be793485d7 kernel/sched.c      (Ingo Molnar                2007-10-15 17:00:18 +0200 6844) 		 */
3472eaa1f12e2 kernel/sched/core.c (Oleg Nesterov              2014-09-21 21:33:38 +0200 6845) 		if (p->flags & PF_KTHREAD)
178be793485d7 kernel/sched.c      (Ingo Molnar                2007-10-15 17:00:18 +0200 6846) 			continue;
178be793485d7 kernel/sched.c      (Ingo Molnar                2007-10-15 17:00:18 +0200 6847) 
4fa8d299b43a9 kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:26 -0500 6848) 		p->se.exec_start = 0;
4fa8d299b43a9 kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:26 -0500 6849) 		schedstat_set(p->se.statistics.wait_start,  0);
4fa8d299b43a9 kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:26 -0500 6850) 		schedstat_set(p->se.statistics.sleep_start, 0);
4fa8d299b43a9 kernel/sched/core.c (Josh Poimboeuf             2016-06-17 12:43:26 -0500 6851) 		schedstat_set(p->se.statistics.block_start, 0);
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6852) 
aab03e05e8f7e kernel/sched/core.c (Dario Faggioli             2013-11-28 11:14:43 +0100 6853) 		if (!dl_task(p) && !rt_task(p)) {
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6854) 			/*
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6855) 			 * Renice negative nice level userspace
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6856) 			 * tasks back to 0:
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6857) 			 */
3472eaa1f12e2 kernel/sched/core.c (Oleg Nesterov              2014-09-21 21:33:38 +0200 6858) 			if (task_nice(p) < 0)
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6859) 				set_user_nice(p, 0);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6860) 			continue;
dd41f596cda0d kernel/sched.c      (Ingo Molnar                2007-07-09 18:51:59 +0200 6861) 		}
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6862) 
dbc7f069b93a2 kernel/sched/core.c (Peter Zijlstra             2015-06-11 14:46:38 +0200 6863) 		__sched_setscheduler(p, &attr, false, false);
5d07f4202c5d6 kernel/sched/core.c (Oleg Nesterov              2014-08-13 21:19:53 +0200 6864) 	}
3472eaa1f12e2 kernel/sched/core.c (Oleg Nesterov              2014-09-21 21:33:38 +0200 6865) 	read_unlock(&tasklist_lock);
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6866) }
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6867) 
^1da177e4c3f4 kernel/sched.c      (Linus Torvalds             2005-04-16 15:20:36 -0700 6868) #endif /* CONFIG_MAGIC_SYSRQ */
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6869) 
67fc4e0cb931d kernel/sched.c      (Jason Wessel               2010-05-20 21:04:21 -0500 6870) #if defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB)
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6871) /*
67fc4e0cb931d kernel/sched.c      (Jason Wessel               2010-05-20 21:04:21 -0500 6872)  * These functions are only useful for the IA64 MCA handling, or kdb.
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6873)  *
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6874)  * They can only be called when the whole system has been
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6875)  * stopped - every CPU needs to be quiescent, and no scheduling
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6876)  * activity can take place. Using them for anything else would
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6877)  * be a serious bug, and as a result, they aren't even visible
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6878)  * under any other configuration.
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6879)  */
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6880) 
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6881) /**
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6882)  * curr_task - return the current task for a given CPU.
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6883)  * @cpu: the processor in question.
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6884)  *
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6885)  * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 6886)  *
e69f61862ab83 kernel/sched/core.c (Yacine Belkadi             2013-07-12 20:45:47 +0200 6887)  * Return: The current task for @cpu.
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6888)  */
36c8b586896f6 kernel/sched.c      (Ingo Molnar                2006-07-03 00:25:41 -0700 6889) struct task_struct *curr_task(int cpu)
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6890) {
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6891) 	return cpu_curr(cpu);
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6892) }
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6893) 
67fc4e0cb931d kernel/sched.c      (Jason Wessel               2010-05-20 21:04:21 -0500 6894) #endif /* defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB) */
67fc4e0cb931d kernel/sched.c      (Jason Wessel               2010-05-20 21:04:21 -0500 6895) 
67fc4e0cb931d kernel/sched.c      (Jason Wessel               2010-05-20 21:04:21 -0500 6896) #ifdef CONFIG_IA64
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6897) /**
5feeb7837a448 kernel/sched/core.c (Peter Zijlstra             2019-05-29 20:36:38 +0000 6898)  * ia64_set_curr_task - set the current task for a given CPU.
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6899)  * @cpu: the processor in question.
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6900)  * @p: the task pointer to set.
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6901)  *
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6902)  * Description: This function must only be used when non-maskable interrupts
41a2d6cfa3f77 kernel/sched.c      (Ingo Molnar                2007-12-05 15:46:09 +0100 6903)  * are serviced on a separate stack. It allows the architecture to switch the
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6904)  * notion of the current task on a CPU in a non-blocking manner. This function
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6905)  * must be called with all CPU's synchronized, and interrupts disabled, the
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6906)  * and caller must save the original value of the current task (see
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6907)  * curr_task() above) and restore that value before reenabling interrupts and
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6908)  * re-starting the system.
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6909)  *
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6910)  * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6911)  */
a458ae2ea6164 kernel/sched/core.c (Peter Zijlstra             2016-09-20 20:29:40 +0200 6912) void ia64_set_curr_task(int cpu, struct task_struct *p)
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6913) {
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6914) 	cpu_curr(cpu) = p;
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6915) }
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6916) 
1df5c10a5b40d kernel/sched.c      (Linus Torvalds             2005-09-12 07:59:21 -0700 6917) #endif
29f59db3a74b0 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:07 +0200 6918) 
7c9414385ebfd kernel/sched.c      (Dhaval Giani               2010-01-20 13:26:18 +0100 6919) #ifdef CONFIG_CGROUP_SCHED
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 6920) /* task_group_lock serializes the addition/removal of task groups */
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 6921) static DEFINE_SPINLOCK(task_group_lock);
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 6922) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 6923) static inline void alloc_uclamp_sched_group(struct task_group *tg,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 6924) 					    struct task_group *parent)
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 6925) {
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 6926) #ifdef CONFIG_UCLAMP_TASK_GROUP
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100 6927) 	enum uclamp_id clamp_id;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 6928) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 6929) 	for_each_clamp_id(clamp_id) {
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 6930) 		uclamp_se_set(&tg->uclamp_req[clamp_id],
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 6931) 			      uclamp_none(clamp_id), false);
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 6932) 		tg->uclamp[clamp_id] = parent->uclamp[clamp_id];
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 6933) 	}
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 6934) #endif
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 6935) }
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 6936) 
2f5177f0fd7e5 kernel/sched/core.c (Peter Zijlstra             2016-03-16 16:22:45 +0100 6937) static void sched_free_group(struct task_group *tg)
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6938) {
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6939) 	free_fair_sched_group(tg);
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6940) 	free_rt_sched_group(tg);
e9aa1dd19fe49 kernel/sched.c      (Mike Galbraith             2011-01-05 11:11:25 +0100 6941) 	autogroup_free(tg);
b0367629acf62 kernel/sched/core.c (Waiman Long                2015-12-02 13:41:49 -0500 6942) 	kmem_cache_free(task_group_cache, tg);
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6943) }
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6944) 
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6945) /* allocate runqueue etc for a new task group */
ec7dc8ac73e4a kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6946) struct task_group *sched_create_group(struct task_group *parent)
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6947) {
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6948) 	struct task_group *tg;
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6949) 
b0367629acf62 kernel/sched/core.c (Waiman Long                2015-12-02 13:41:49 -0500 6950) 	tg = kmem_cache_alloc(task_group_cache, GFP_KERNEL | __GFP_ZERO);
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6951) 	if (!tg)
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6952) 		return ERR_PTR(-ENOMEM);
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6953) 
ec7dc8ac73e4a kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6954) 	if (!alloc_fair_sched_group(tg, parent))
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6955) 		goto err;
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6956) 
ec7dc8ac73e4a kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 6957) 	if (!alloc_rt_sched_group(tg, parent))
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6958) 		goto err;
bccbe08a60973 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 6959) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 6960) 	alloc_uclamp_sched_group(tg, parent);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 6961) 
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 6962) 	return tg;
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 6963) 
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 6964) err:
2f5177f0fd7e5 kernel/sched/core.c (Peter Zijlstra             2016-03-16 16:22:45 +0100 6965) 	sched_free_group(tg);
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 6966) 	return ERR_PTR(-ENOMEM);
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 6967) }
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 6968) 
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 6969) void sched_online_group(struct task_group *tg, struct task_group *parent)
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 6970) {
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 6971) 	unsigned long flags;
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 6972) 
8ed3699682be7 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:39 +0100 6973) 	spin_lock_irqsave(&task_group_lock, flags);
6f505b16425a5 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:30 +0100 6974) 	list_add_rcu(&tg->list, &task_groups);
f473aa5e025bc kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200 6975) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6976) 	/* Root should already exist: */
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6977) 	WARN_ON(!parent);
f473aa5e025bc kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200 6978) 
f473aa5e025bc kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200 6979) 	tg->parent = parent;
f473aa5e025bc kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200 6980) 	INIT_LIST_HEAD(&tg->children);
09f2724a786f7 kernel/sched.c      (Zhang, Yanmin              2030-08-14 15:56:40 +0800 6981) 	list_add_rcu(&tg->siblings, &parent->children);
8ed3699682be7 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:39 +0100 6982) 	spin_unlock_irqrestore(&task_group_lock, flags);
8663e24d56dc1 kernel/sched/core.c (Peter Zijlstra             2016-06-22 14:58:02 +0200 6983) 
8663e24d56dc1 kernel/sched/core.c (Peter Zijlstra             2016-06-22 14:58:02 +0200 6984) 	online_fair_sched_group(tg);
29f59db3a74b0 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:07 +0200 6985) }
29f59db3a74b0 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:07 +0200 6986) 
9b5b77512dce2 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:09 +0200 6987) /* rcu callback to free various structures associated with a task group */
2f5177f0fd7e5 kernel/sched/core.c (Peter Zijlstra             2016-03-16 16:22:45 +0100 6988) static void sched_free_group_rcu(struct rcu_head *rhp)
29f59db3a74b0 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:07 +0200 6989) {
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6990) 	/* Now it should be safe to free those cfs_rqs: */
2f5177f0fd7e5 kernel/sched/core.c (Peter Zijlstra             2016-03-16 16:22:45 +0100 6991) 	sched_free_group(container_of(rhp, struct task_group, rcu));
29f59db3a74b0 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:07 +0200 6992) }
29f59db3a74b0 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:07 +0200 6993) 
4cf86d77f5942 kernel/sched.c      (Ingo Molnar                2007-10-15 17:00:14 +0200 6994) void sched_destroy_group(struct task_group *tg)
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 6995) {
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 6996) 	/* Wait for possible concurrent references to cfs_rqs complete: */
2f5177f0fd7e5 kernel/sched/core.c (Peter Zijlstra             2016-03-16 16:22:45 +0100 6997) 	call_rcu(&tg->rcu, sched_free_group_rcu);
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 6998) }
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 6999) 
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 7000) void sched_offline_group(struct task_group *tg)
29f59db3a74b0 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:07 +0200 7001) {
8ed3699682be7 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:39 +0100 7002) 	unsigned long flags;
29f59db3a74b0 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:07 +0200 7003) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 7004) 	/* End participation in shares distribution: */
6fe1f348b3dd1 kernel/sched/core.c (Peter Zijlstra             2016-01-21 22:24:16 +0100 7005) 	unregister_fair_sched_group(tg);
3d4b47b4b040c kernel/sched.c      (Peter Zijlstra             2010-11-15 15:47:01 -0800 7006) 
3d4b47b4b040c kernel/sched.c      (Peter Zijlstra             2010-11-15 15:47:01 -0800 7007) 	spin_lock_irqsave(&task_group_lock, flags);
6f505b16425a5 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:30 +0100 7008) 	list_del_rcu(&tg->list);
f473aa5e025bc kernel/sched.c      (Peter Zijlstra             2008-04-19 19:45:00 +0200 7009) 	list_del_rcu(&tg->siblings);
8ed3699682be7 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:39 +0100 7010) 	spin_unlock_irqrestore(&task_group_lock, flags);
29f59db3a74b0 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:07 +0200 7011) }
29f59db3a74b0 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:07 +0200 7012) 
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7013) static void sched_change_group(struct task_struct *tsk, int type)
29f59db3a74b0 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:07 +0200 7014) {
8323f26ce3425 kernel/sched/core.c (Peter Zijlstra             2012-06-22 13:36:05 +0200 7015) 	struct task_group *tg;
29f59db3a74b0 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:07 +0200 7016) 
f7b8a47da17c9 kernel/sched/core.c (Kirill Tkhai               2014-10-28 08:24:34 +0300 7017) 	/*
f7b8a47da17c9 kernel/sched/core.c (Kirill Tkhai               2014-10-28 08:24:34 +0300 7018) 	 * All callers are synchronized by task_rq_lock(); we do not use RCU
f7b8a47da17c9 kernel/sched/core.c (Kirill Tkhai               2014-10-28 08:24:34 +0300 7019) 	 * which is pointless here. Thus, we pass "true" to task_css_check()
f7b8a47da17c9 kernel/sched/core.c (Kirill Tkhai               2014-10-28 08:24:34 +0300 7020) 	 * to prevent lockdep warnings.
f7b8a47da17c9 kernel/sched/core.c (Kirill Tkhai               2014-10-28 08:24:34 +0300 7021) 	 */
f7b8a47da17c9 kernel/sched/core.c (Kirill Tkhai               2014-10-28 08:24:34 +0300 7022) 	tg = container_of(task_css_check(tsk, cpu_cgrp_id, true),
8323f26ce3425 kernel/sched/core.c (Peter Zijlstra             2012-06-22 13:36:05 +0200 7023) 			  struct task_group, css);
8323f26ce3425 kernel/sched/core.c (Peter Zijlstra             2012-06-22 13:36:05 +0200 7024) 	tg = autogroup_task_group(tsk, tg);
8323f26ce3425 kernel/sched/core.c (Peter Zijlstra             2012-06-22 13:36:05 +0200 7025) 	tsk->sched_task_group = tg;
8323f26ce3425 kernel/sched/core.c (Peter Zijlstra             2012-06-22 13:36:05 +0200 7026) 
810b38179e9e4 kernel/sched.c      (Peter Zijlstra             2008-02-29 15:21:01 -0500 7027) #ifdef CONFIG_FAIR_GROUP_SCHED
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7028) 	if (tsk->sched_class->task_change_group)
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7029) 		tsk->sched_class->task_change_group(tsk, type);
b2b5ce022acf5 kernel/sched.c      (Peter Zijlstra             2010-10-15 15:24:15 +0200 7030) 	else
810b38179e9e4 kernel/sched.c      (Peter Zijlstra             2008-02-29 15:21:01 -0500 7031) #endif
b2b5ce022acf5 kernel/sched.c      (Peter Zijlstra             2010-10-15 15:24:15 +0200 7032) 		set_task_rq(tsk, task_cpu(tsk));
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7033) }
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7034) 
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7035) /*
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7036)  * Change task's runqueue when it moves between groups.
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7037)  *
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7038)  * The caller of this function should have put the task in its new group by
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7039)  * now. This function just updates tsk->se.cfs_rq and tsk->se.parent to reflect
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7040)  * its new group.
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7041)  */
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7042) void sched_move_task(struct task_struct *tsk)
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7043) {
7a57f32a4d5c8 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:47:02 +0100 7044) 	int queued, running, queue_flags =
7a57f32a4d5c8 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:47:02 +0100 7045) 		DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7046) 	struct rq_flags rf;
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7047) 	struct rq *rq;
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7048) 
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7049) 	rq = task_rq_lock(tsk, &rf);
1b1d62254df0f kernel/sched/core.c (Peter Zijlstra             2017-01-23 16:05:55 +0100 7050) 	update_rq_clock(rq);
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7051) 
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7052) 	running = task_current(rq, tsk);
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7053) 	queued = task_on_rq_queued(tsk);
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7054) 
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7055) 	if (queued)
7a57f32a4d5c8 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:47:02 +0100 7056) 		dequeue_task(rq, tsk, queue_flags);
bb3bac2ca9a3a kernel/sched/core.c (Steven Rostedt (VMware)    2017-02-06 11:04:26 -0500 7057) 	if (running)
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7058) 		put_prev_task(rq, tsk);
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7059) 
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7060) 	sched_change_group(tsk, TASK_MOVE_GROUP);
810b38179e9e4 kernel/sched.c      (Peter Zijlstra             2008-02-29 15:21:01 -0500 7061) 
da0c1e65b51a2 kernel/sched/core.c (Kirill Tkhai               2014-08-20 13:47:32 +0400 7062) 	if (queued)
7a57f32a4d5c8 kernel/sched/core.c (Peter Zijlstra             2017-02-21 14:47:02 +0100 7063) 		enqueue_task(rq, tsk, queue_flags);
2a4b03ffc69f2 kernel/sched/core.c (Vincent Guittot            2020-01-14 15:13:56 +0100 7064) 	if (running) {
03b7fad167efc kernel/sched/core.c (Peter Zijlstra             2019-05-29 20:36:41 +0000 7065) 		set_next_task(rq, tsk);
2a4b03ffc69f2 kernel/sched/core.c (Vincent Guittot            2020-01-14 15:13:56 +0100 7066) 		/*
2a4b03ffc69f2 kernel/sched/core.c (Vincent Guittot            2020-01-14 15:13:56 +0100 7067) 		 * After changing group, the running task may have joined a
2a4b03ffc69f2 kernel/sched/core.c (Vincent Guittot            2020-01-14 15:13:56 +0100 7068) 		 * throttled one but it's still the running task. Trigger a
2a4b03ffc69f2 kernel/sched/core.c (Vincent Guittot            2020-01-14 15:13:56 +0100 7069) 		 * resched to make sure that task can still run.
2a4b03ffc69f2 kernel/sched/core.c (Vincent Guittot            2020-01-14 15:13:56 +0100 7070) 		 */
2a4b03ffc69f2 kernel/sched/core.c (Vincent Guittot            2020-01-14 15:13:56 +0100 7071) 		resched_curr(rq);
2a4b03ffc69f2 kernel/sched/core.c (Vincent Guittot            2020-01-14 15:13:56 +0100 7072) 	}
29f59db3a74b0 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:07 +0200 7073) 
eb58075149b7f kernel/sched/core.c (Peter Zijlstra             2015-07-31 21:28:18 +0200 7074) 	task_rq_unlock(rq, tsk, &rf);
29f59db3a74b0 kernel/sched.c      (Srivatsa Vaddagiri         2007-10-15 17:00:07 +0200 7075) }
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7076) 
a7c6d554aa012 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:23 -0400 7077) static inline struct task_group *css_tg(struct cgroup_subsys_state *css)
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7078) {
a7c6d554aa012 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:23 -0400 7079) 	return css ? container_of(css, struct task_group, css) : NULL;
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7080) }
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7081) 
eb95419b023ab kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:23 -0400 7082) static struct cgroup_subsys_state *
eb95419b023ab kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:23 -0400 7083) cpu_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7084) {
eb95419b023ab kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:23 -0400 7085) 	struct task_group *parent = css_tg(parent_css);
eb95419b023ab kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:23 -0400 7086) 	struct task_group *tg;
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7087) 
eb95419b023ab kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:23 -0400 7088) 	if (!parent) {
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7089) 		/* This is early initialization for the top cgroup */
07e06b011db2b kernel/sched.c      (Yong Zhang                 2011-01-07 15:17:36 +0800 7090) 		return &root_task_group.css;
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7091) 	}
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7092) 
ec7dc8ac73e4a kernel/sched.c      (Dhaval Giani               2008-04-19 19:44:59 +0200 7093) 	tg = sched_create_group(parent);
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7094) 	if (IS_ERR(tg))
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7095) 		return ERR_PTR(-ENOMEM);
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7096) 
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7097) 	return &tg->css;
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7098) }
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7099) 
96b777452d888 kernel/sched/core.c (Konstantin Khlebnikov      2017-02-08 14:27:27 +0300 7100) /* Expose task group only after completing cgroup initialization */
96b777452d888 kernel/sched/core.c (Konstantin Khlebnikov      2017-02-08 14:27:27 +0300 7101) static int cpu_cgroup_css_online(struct cgroup_subsys_state *css)
96b777452d888 kernel/sched/core.c (Konstantin Khlebnikov      2017-02-08 14:27:27 +0300 7102) {
96b777452d888 kernel/sched/core.c (Konstantin Khlebnikov      2017-02-08 14:27:27 +0300 7103) 	struct task_group *tg = css_tg(css);
96b777452d888 kernel/sched/core.c (Konstantin Khlebnikov      2017-02-08 14:27:27 +0300 7104) 	struct task_group *parent = css_tg(css->parent);
96b777452d888 kernel/sched/core.c (Konstantin Khlebnikov      2017-02-08 14:27:27 +0300 7105) 
96b777452d888 kernel/sched/core.c (Konstantin Khlebnikov      2017-02-08 14:27:27 +0300 7106) 	if (parent)
96b777452d888 kernel/sched/core.c (Konstantin Khlebnikov      2017-02-08 14:27:27 +0300 7107) 		sched_online_group(tg, parent);
7226017ad37a8 kernel/sched/core.c (Qais Yousef                2019-12-24 11:54:04 +0000 7108) 
7226017ad37a8 kernel/sched/core.c (Qais Yousef                2019-12-24 11:54:04 +0000 7109) #ifdef CONFIG_UCLAMP_TASK_GROUP
7226017ad37a8 kernel/sched/core.c (Qais Yousef                2019-12-24 11:54:04 +0000 7110) 	/* Propagate the effective uclamp value for the new group */
7226017ad37a8 kernel/sched/core.c (Qais Yousef                2019-12-24 11:54:04 +0000 7111) 	cpu_util_update_eff(css);
7226017ad37a8 kernel/sched/core.c (Qais Yousef                2019-12-24 11:54:04 +0000 7112) #endif
7226017ad37a8 kernel/sched/core.c (Qais Yousef                2019-12-24 11:54:04 +0000 7113) 
96b777452d888 kernel/sched/core.c (Konstantin Khlebnikov      2017-02-08 14:27:27 +0300 7114) 	return 0;
96b777452d888 kernel/sched/core.c (Konstantin Khlebnikov      2017-02-08 14:27:27 +0300 7115) }
96b777452d888 kernel/sched/core.c (Konstantin Khlebnikov      2017-02-08 14:27:27 +0300 7116) 
2f5177f0fd7e5 kernel/sched/core.c (Peter Zijlstra             2016-03-16 16:22:45 +0100 7117) static void cpu_cgroup_css_released(struct cgroup_subsys_state *css)
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 7118) {
eb95419b023ab kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:23 -0400 7119) 	struct task_group *tg = css_tg(css);
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 7120) 
2f5177f0fd7e5 kernel/sched/core.c (Peter Zijlstra             2016-03-16 16:22:45 +0100 7121) 	sched_offline_group(tg);
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 7122) }
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 7123) 
eb95419b023ab kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:23 -0400 7124) static void cpu_cgroup_css_free(struct cgroup_subsys_state *css)
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7125) {
eb95419b023ab kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:23 -0400 7126) 	struct task_group *tg = css_tg(css);
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7127) 
2f5177f0fd7e5 kernel/sched/core.c (Peter Zijlstra             2016-03-16 16:22:45 +0100 7128) 	/*
2f5177f0fd7e5 kernel/sched/core.c (Peter Zijlstra             2016-03-16 16:22:45 +0100 7129) 	 * Relies on the RCU grace period between css_released() and this.
2f5177f0fd7e5 kernel/sched/core.c (Peter Zijlstra             2016-03-16 16:22:45 +0100 7130) 	 */
2f5177f0fd7e5 kernel/sched/core.c (Peter Zijlstra             2016-03-16 16:22:45 +0100 7131) 	sched_free_group(tg);
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 7132) }
ace783b9bbfa2 kernel/sched/core.c (Li Zefan                   2013-01-24 14:30:48 +0800 7133) 
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7134) /*
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7135)  * This is called before wake_up_new_task(), therefore we really only
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7136)  * have to set its group bits, all the other stuff does not apply.
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7137)  */
b53202e630893 kernel/sched/core.c (Oleg Nesterov              2015-12-03 10:24:08 -0500 7138) static void cpu_cgroup_fork(struct task_struct *task)
eeb61e53ea19b kernel/sched/core.c (Kirill Tkhai               2014-10-27 14:18:25 +0400 7139) {
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7140) 	struct rq_flags rf;
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7141) 	struct rq *rq;
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7142) 
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7143) 	rq = task_rq_lock(task, &rf);
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7144) 
80f5c1b84baa8 kernel/sched/core.c (Peter Zijlstra             2016-10-03 16:28:37 +0200 7145) 	update_rq_clock(rq);
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7146) 	sched_change_group(task, TASK_SET_GROUP);
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7147) 
ea86cb4b7621e kernel/sched/core.c (Vincent Guittot            2016-06-17 13:38:55 +0200 7148) 	task_rq_unlock(rq, task, &rf);
eeb61e53ea19b kernel/sched/core.c (Kirill Tkhai               2014-10-27 14:18:25 +0400 7149) }
eeb61e53ea19b kernel/sched/core.c (Kirill Tkhai               2014-10-27 14:18:25 +0400 7150) 
1f7dd3e5a6e4f kernel/sched/core.c (Tejun Heo                  2015-12-03 10:18:21 -0500 7151) static int cpu_cgroup_can_attach(struct cgroup_taskset *tset)
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7152) {
bb9d97b6dffa1 kernel/sched.c      (Tejun Heo                  2011-12-12 18:12:21 -0800 7153) 	struct task_struct *task;
1f7dd3e5a6e4f kernel/sched/core.c (Tejun Heo                  2015-12-03 10:18:21 -0500 7154) 	struct cgroup_subsys_state *css;
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7155) 	int ret = 0;
bb9d97b6dffa1 kernel/sched.c      (Tejun Heo                  2011-12-12 18:12:21 -0800 7156) 
1f7dd3e5a6e4f kernel/sched/core.c (Tejun Heo                  2015-12-03 10:18:21 -0500 7157) 	cgroup_taskset_for_each(task, css, tset) {
b68aa2300cabe kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 7158) #ifdef CONFIG_RT_GROUP_SCHED
eb95419b023ab kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:23 -0400 7159) 		if (!sched_rt_can_attach(css_tg(css), task))
bb9d97b6dffa1 kernel/sched.c      (Tejun Heo                  2011-12-12 18:12:21 -0800 7160) 			return -EINVAL;
b68aa2300cabe kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 7161) #endif
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7162) 		/*
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7163) 		 * Serialize against wake_up_new_task() such that if its
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7164) 		 * running, we're sure to observe its full state.
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7165) 		 */
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7166) 		raw_spin_lock_irq(&task->pi_lock);
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7167) 		/*
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7168) 		 * Avoid calling sched_move_task() before wake_up_new_task()
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7169) 		 * has happened. This would lead to problems with PELT, due to
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7170) 		 * move wanting to detach+attach while we're not attached yet.
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7171) 		 */
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7172) 		if (task->state == TASK_NEW)
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7173) 			ret = -EINVAL;
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7174) 		raw_spin_unlock_irq(&task->pi_lock);
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7175) 
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7176) 		if (ret)
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7177) 			break;
bb9d97b6dffa1 kernel/sched.c      (Tejun Heo                  2011-12-12 18:12:21 -0800 7178) 	}
7dc603c9028ea kernel/sched/core.c (Peter Zijlstra             2016-06-16 13:29:28 +0200 7179) 	return ret;
be367d0992702 kernel/sched.c      (Ben Blum                   2009-09-23 15:56:31 -0700 7180) }
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7181) 
1f7dd3e5a6e4f kernel/sched/core.c (Tejun Heo                  2015-12-03 10:18:21 -0500 7182) static void cpu_cgroup_attach(struct cgroup_taskset *tset)
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7183) {
bb9d97b6dffa1 kernel/sched.c      (Tejun Heo                  2011-12-12 18:12:21 -0800 7184) 	struct task_struct *task;
1f7dd3e5a6e4f kernel/sched/core.c (Tejun Heo                  2015-12-03 10:18:21 -0500 7185) 	struct cgroup_subsys_state *css;
bb9d97b6dffa1 kernel/sched.c      (Tejun Heo                  2011-12-12 18:12:21 -0800 7186) 
1f7dd3e5a6e4f kernel/sched/core.c (Tejun Heo                  2015-12-03 10:18:21 -0500 7187) 	cgroup_taskset_for_each(task, css, tset)
bb9d97b6dffa1 kernel/sched.c      (Tejun Heo                  2011-12-12 18:12:21 -0800 7188) 		sched_move_task(task);
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7189) }
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7190) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7191) #ifdef CONFIG_UCLAMP_TASK_GROUP
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7192) static void cpu_util_update_eff(struct cgroup_subsys_state *css)
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7193) {
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7194) 	struct cgroup_subsys_state *top_css = css;
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7195) 	struct uclamp_se *uc_parent = NULL;
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7196) 	struct uclamp_se *uc_se = NULL;
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7197) 	unsigned int eff[UCLAMP_CNT];
0413d7f33e607 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:11 +0100 7198) 	enum uclamp_id clamp_id;
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7199) 	unsigned int clamps;
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7200) 
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7201) 	css_for_each_descendant_pre(css, top_css) {
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7202) 		uc_parent = css_tg(css)->parent
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7203) 			? css_tg(css)->parent->uclamp : NULL;
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7204) 
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7205) 		for_each_clamp_id(clamp_id) {
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7206) 			/* Assume effective clamps matches requested clamps */
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7207) 			eff[clamp_id] = css_tg(css)->uclamp_req[clamp_id].value;
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7208) 			/* Cap effective clamps with parent's effective clamps */
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7209) 			if (uc_parent &&
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7210) 			    eff[clamp_id] > uc_parent[clamp_id].value) {
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7211) 				eff[clamp_id] = uc_parent[clamp_id].value;
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7212) 			}
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7213) 		}
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7214) 		/* Ensure protection is always capped by limit */
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7215) 		eff[UCLAMP_MIN] = min(eff[UCLAMP_MIN], eff[UCLAMP_MAX]);
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7216) 
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7217) 		/* Propagate most restrictive effective clamps */
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7218) 		clamps = 0x0;
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7219) 		uc_se = css_tg(css)->uclamp;
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7220) 		for_each_clamp_id(clamp_id) {
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7221) 			if (eff[clamp_id] == uc_se[clamp_id].value)
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7222) 				continue;
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7223) 			uc_se[clamp_id].value = eff[clamp_id];
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7224) 			uc_se[clamp_id].bucket_id = uclamp_bucket_id(eff[clamp_id]);
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7225) 			clamps |= (0x1 << clamp_id);
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7226) 		}
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 7227) 		if (!clamps) {
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7228) 			css = css_rightmost_descendant(css);
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 7229) 			continue;
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 7230) 		}
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 7231) 
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 7232) 		/* Immediately update descendants RUNNABLE tasks */
babbe170e053c kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:10 +0100 7233) 		uclamp_update_active_tasks(css, clamps);
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7234) 	}
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7235) }
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7236) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7237) /*
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7238)  * Integer 10^N with a given N exponent by casting to integer the literal "1eN"
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7239)  * C expression. Since there is no way to convert a macro argument (N) into a
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7240)  * character constant, use two levels of macros.
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7241)  */
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7242) #define _POW10(exp) ((unsigned int)1e##exp)
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7243) #define POW10(exp) _POW10(exp)
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7244) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7245) struct uclamp_request {
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7246) #define UCLAMP_PERCENT_SHIFT	2
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7247) #define UCLAMP_PERCENT_SCALE	(100 * POW10(UCLAMP_PERCENT_SHIFT))
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7248) 	s64 percent;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7249) 	u64 util;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7250) 	int ret;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7251) };
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7252) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7253) static inline struct uclamp_request
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7254) capacity_from_percent(char *buf)
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7255) {
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7256) 	struct uclamp_request req = {
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7257) 		.percent = UCLAMP_PERCENT_SCALE,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7258) 		.util = SCHED_CAPACITY_SCALE,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7259) 		.ret = 0,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7260) 	};
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7261) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7262) 	buf = strim(buf);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7263) 	if (strcmp(buf, "max")) {
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7264) 		req.ret = cgroup_parse_float(buf, UCLAMP_PERCENT_SHIFT,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7265) 					     &req.percent);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7266) 		if (req.ret)
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7267) 			return req;
b562d14064996 kernel/sched/core.c (Qais Yousef                2020-01-14 21:09:47 +0000 7268) 		if ((u64)req.percent > UCLAMP_PERCENT_SCALE) {
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7269) 			req.ret = -ERANGE;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7270) 			return req;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7271) 		}
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7272) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7273) 		req.util = req.percent << SCHED_CAPACITY_SHIFT;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7274) 		req.util = DIV_ROUND_CLOSEST_ULL(req.util, UCLAMP_PERCENT_SCALE);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7275) 	}
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7276) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7277) 	return req;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7278) }
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7279) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7280) static ssize_t cpu_uclamp_write(struct kernfs_open_file *of, char *buf,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7281) 				size_t nbytes, loff_t off,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7282) 				enum uclamp_id clamp_id)
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7283) {
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7284) 	struct uclamp_request req;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7285) 	struct task_group *tg;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7286) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7287) 	req = capacity_from_percent(buf);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7288) 	if (req.ret)
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7289) 		return req.ret;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7290) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7291) 	mutex_lock(&uclamp_mutex);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7292) 	rcu_read_lock();
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7293) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7294) 	tg = css_tg(of_css(of));
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7295) 	if (tg->uclamp_req[clamp_id].value != req.util)
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7296) 		uclamp_se_set(&tg->uclamp_req[clamp_id], req.util, false);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7297) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7298) 	/*
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7299) 	 * Because of not recoverable conversion rounding we keep track of the
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7300) 	 * exact requested value
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7301) 	 */
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7302) 	tg->uclamp_pct[clamp_id] = req.percent;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7303) 
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7304) 	/* Update effective clamps to track the most restrictive value */
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7305) 	cpu_util_update_eff(of_css(of));
0b60ba2dd3420 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:07 +0100 7306) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7307) 	rcu_read_unlock();
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7308) 	mutex_unlock(&uclamp_mutex);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7309) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7310) 	return nbytes;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7311) }
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7312) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7313) static ssize_t cpu_uclamp_min_write(struct kernfs_open_file *of,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7314) 				    char *buf, size_t nbytes,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7315) 				    loff_t off)
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7316) {
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7317) 	return cpu_uclamp_write(of, buf, nbytes, off, UCLAMP_MIN);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7318) }
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7319) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7320) static ssize_t cpu_uclamp_max_write(struct kernfs_open_file *of,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7321) 				    char *buf, size_t nbytes,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7322) 				    loff_t off)
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7323) {
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7324) 	return cpu_uclamp_write(of, buf, nbytes, off, UCLAMP_MAX);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7325) }
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7326) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7327) static inline void cpu_uclamp_print(struct seq_file *sf,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7328) 				    enum uclamp_id clamp_id)
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7329) {
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7330) 	struct task_group *tg;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7331) 	u64 util_clamp;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7332) 	u64 percent;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7333) 	u32 rem;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7334) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7335) 	rcu_read_lock();
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7336) 	tg = css_tg(seq_css(sf));
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7337) 	util_clamp = tg->uclamp_req[clamp_id].value;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7338) 	rcu_read_unlock();
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7339) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7340) 	if (util_clamp == SCHED_CAPACITY_SCALE) {
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7341) 		seq_puts(sf, "max\n");
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7342) 		return;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7343) 	}
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7344) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7345) 	percent = tg->uclamp_pct[clamp_id];
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7346) 	percent = div_u64_rem(percent, POW10(UCLAMP_PERCENT_SHIFT), &rem);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7347) 	seq_printf(sf, "%llu.%0*u\n", percent, UCLAMP_PERCENT_SHIFT, rem);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7348) }
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7349) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7350) static int cpu_uclamp_min_show(struct seq_file *sf, void *v)
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7351) {
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7352) 	cpu_uclamp_print(sf, UCLAMP_MIN);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7353) 	return 0;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7354) }
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7355) 
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7356) static int cpu_uclamp_max_show(struct seq_file *sf, void *v)
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7357) {
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7358) 	cpu_uclamp_print(sf, UCLAMP_MAX);
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7359) 	return 0;
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7360) }
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7361) #endif /* CONFIG_UCLAMP_TASK_GROUP */
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7362) 
052f1dc7eb023 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 7363) #ifdef CONFIG_FAIR_GROUP_SCHED
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7364) static int cpu_shares_write_u64(struct cgroup_subsys_state *css,
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7365) 				struct cftype *cftype, u64 shareval)
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7366) {
5b61d50ab4ef5 kernel/sched/core.c (Konstantin Khlebnikov      2019-02-27 11:10:18 +0300 7367) 	if (shareval > scale_load_down(ULONG_MAX))
5b61d50ab4ef5 kernel/sched/core.c (Konstantin Khlebnikov      2019-02-27 11:10:18 +0300 7368) 		shareval = MAX_SHARES;
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7369) 	return sched_group_set_shares(css_tg(css), scale_load(shareval));
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7370) }
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7371) 
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7372) static u64 cpu_shares_read_u64(struct cgroup_subsys_state *css,
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7373) 			       struct cftype *cft)
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7374) {
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7375) 	struct task_group *tg = css_tg(css);
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7376) 
c8b281161dfa4 kernel/sched.c      (Nikhil Rao                 2011-05-18 14:37:48 -0700 7377) 	return (u64) scale_load_down(tg->shares);
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7378) }
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7379) 
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7380) #ifdef CONFIG_CFS_BANDWIDTH
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7381) static DEFINE_MUTEX(cfs_constraints_mutex);
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7382) 
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7383) const u64 max_cfs_quota_period = 1 * NSEC_PER_SEC; /* 1s */
b1546edcf2aab kernel/sched/core.c (YueHaibing                 2019-04-18 22:47:13 +0800 7384) static const u64 min_cfs_quota_period = 1 * NSEC_PER_MSEC; /* 1ms */
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7385) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7386) static int __cfs_schedulable(struct task_group *tg, u64 period, u64 runtime);
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7387) 
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7388) static int tg_set_cfs_bandwidth(struct task_group *tg, u64 period, u64 quota)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7389) {
56f570e512eeb kernel/sched.c      (Paul Turner                2011-11-07 20:26:33 -0800 7390) 	int i, ret = 0, runtime_enabled, runtime_was_enabled;
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 7391) 	struct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7392) 
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7393) 	if (tg == &root_task_group)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7394) 		return -EINVAL;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7395) 
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7396) 	/*
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7397) 	 * Ensure we have at some amount of bandwidth every period.  This is
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7398) 	 * to prevent reaching a state of large arrears when throttled via
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7399) 	 * entity_tick() resulting in prolonged exit starvation.
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7400) 	 */
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7401) 	if (quota < min_cfs_quota_period || period < min_cfs_quota_period)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7402) 		return -EINVAL;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7403) 
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7404) 	/*
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7405) 	 * Likewise, bound things on the otherside by preventing insane quota
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7406) 	 * periods.  This also allows us to normalize in computing quota
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7407) 	 * feasibility.
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7408) 	 */
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7409) 	if (period > max_cfs_quota_period)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7410) 		return -EINVAL;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7411) 
0e59bdaea75f1 kernel/sched/core.c (Kirill Tkhai               2014-06-25 12:19:42 +0400 7412) 	/*
0e59bdaea75f1 kernel/sched/core.c (Kirill Tkhai               2014-06-25 12:19:42 +0400 7413) 	 * Prevent race between setting of cfs_rq->runtime_enabled and
0e59bdaea75f1 kernel/sched/core.c (Kirill Tkhai               2014-06-25 12:19:42 +0400 7414) 	 * unthrottle_offline_cfs_rqs().
0e59bdaea75f1 kernel/sched/core.c (Kirill Tkhai               2014-06-25 12:19:42 +0400 7415) 	 */
0e59bdaea75f1 kernel/sched/core.c (Kirill Tkhai               2014-06-25 12:19:42 +0400 7416) 	get_online_cpus();
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7417) 	mutex_lock(&cfs_constraints_mutex);
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7418) 	ret = __cfs_schedulable(tg, period, quota);
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7419) 	if (ret)
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7420) 		goto out_unlock;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7421) 
58088ad0152ba kernel/sched.c      (Paul Turner                2011-07-21 09:43:31 -0700 7422) 	runtime_enabled = quota != RUNTIME_INF;
56f570e512eeb kernel/sched.c      (Paul Turner                2011-11-07 20:26:33 -0800 7423) 	runtime_was_enabled = cfs_b->quota != RUNTIME_INF;
1ee14e6c8cdde kernel/sched/core.c (Ben Segall                 2013-10-16 11:16:12 -0700 7424) 	/*
1ee14e6c8cdde kernel/sched/core.c (Ben Segall                 2013-10-16 11:16:12 -0700 7425) 	 * If we need to toggle cfs_bandwidth_used, off->on must occur
1ee14e6c8cdde kernel/sched/core.c (Ben Segall                 2013-10-16 11:16:12 -0700 7426) 	 * before making related changes, and on->off must occur afterwards
1ee14e6c8cdde kernel/sched/core.c (Ben Segall                 2013-10-16 11:16:12 -0700 7427) 	 */
1ee14e6c8cdde kernel/sched/core.c (Ben Segall                 2013-10-16 11:16:12 -0700 7428) 	if (runtime_enabled && !runtime_was_enabled)
1ee14e6c8cdde kernel/sched/core.c (Ben Segall                 2013-10-16 11:16:12 -0700 7429) 		cfs_bandwidth_usage_inc();
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7430) 	raw_spin_lock_irq(&cfs_b->lock);
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7431) 	cfs_b->period = ns_to_ktime(period);
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7432) 	cfs_b->quota = quota;
58088ad0152ba kernel/sched.c      (Paul Turner                2011-07-21 09:43:31 -0700 7433) 
a9cf55b286105 kernel/sched.c      (Paul Turner                2011-07-21 09:43:32 -0700 7434) 	__refill_cfs_bandwidth_runtime(cfs_b);
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 7435) 
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 7436) 	/* Restart the period timer (if active) to handle new period expiry: */
77a4d1a1b9a12 kernel/sched/core.c (Peter Zijlstra             2015-04-15 11:41:57 +0200 7437) 	if (runtime_enabled)
77a4d1a1b9a12 kernel/sched/core.c (Peter Zijlstra             2015-04-15 11:41:57 +0200 7438) 		start_cfs_bandwidth(cfs_b);
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 7439) 
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7440) 	raw_spin_unlock_irq(&cfs_b->lock);
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7441) 
0e59bdaea75f1 kernel/sched/core.c (Kirill Tkhai               2014-06-25 12:19:42 +0400 7442) 	for_each_online_cpu(i) {
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7443) 		struct cfs_rq *cfs_rq = tg->cfs_rq[i];
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 7444) 		struct rq *rq = cfs_rq->rq;
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 7445) 		struct rq_flags rf;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7446) 
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 7447) 		rq_lock_irq(rq, &rf);
58088ad0152ba kernel/sched.c      (Paul Turner                2011-07-21 09:43:31 -0700 7448) 		cfs_rq->runtime_enabled = runtime_enabled;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7449) 		cfs_rq->runtime_remaining = 0;
671fd9dabe523 kernel/sched.c      (Paul Turner                2011-07-21 09:43:34 -0700 7450) 
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 7451) 		if (cfs_rq->throttled)
671fd9dabe523 kernel/sched.c      (Paul Turner                2011-07-21 09:43:34 -0700 7452) 			unthrottle_cfs_rq(cfs_rq);
8a8c69c327788 kernel/sched/core.c (Peter Zijlstra             2016-10-04 16:04:35 +0200 7453) 		rq_unlock_irq(rq, &rf);
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7454) 	}
1ee14e6c8cdde kernel/sched/core.c (Ben Segall                 2013-10-16 11:16:12 -0700 7455) 	if (runtime_was_enabled && !runtime_enabled)
1ee14e6c8cdde kernel/sched/core.c (Ben Segall                 2013-10-16 11:16:12 -0700 7456) 		cfs_bandwidth_usage_dec();
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7457) out_unlock:
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7458) 	mutex_unlock(&cfs_constraints_mutex);
0e59bdaea75f1 kernel/sched/core.c (Kirill Tkhai               2014-06-25 12:19:42 +0400 7459) 	put_online_cpus();
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7460) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7461) 	return ret;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7462) }
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7463) 
b1546edcf2aab kernel/sched/core.c (YueHaibing                 2019-04-18 22:47:13 +0800 7464) static int tg_set_cfs_quota(struct task_group *tg, long cfs_quota_us)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7465) {
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7466) 	u64 quota, period;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7467) 
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 7468) 	period = ktime_to_ns(tg->cfs_bandwidth.period);
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7469) 	if (cfs_quota_us < 0)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7470) 		quota = RUNTIME_INF;
1a8b4540db732 kernel/sched/core.c (Konstantin Khlebnikov      2019-02-27 11:10:20 +0300 7471) 	else if ((u64)cfs_quota_us <= U64_MAX / NSEC_PER_USEC)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7472) 		quota = (u64)cfs_quota_us * NSEC_PER_USEC;
1a8b4540db732 kernel/sched/core.c (Konstantin Khlebnikov      2019-02-27 11:10:20 +0300 7473) 	else
1a8b4540db732 kernel/sched/core.c (Konstantin Khlebnikov      2019-02-27 11:10:20 +0300 7474) 		return -EINVAL;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7475) 
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7476) 	return tg_set_cfs_bandwidth(tg, period, quota);
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7477) }
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7478) 
b1546edcf2aab kernel/sched/core.c (YueHaibing                 2019-04-18 22:47:13 +0800 7479) static long tg_get_cfs_quota(struct task_group *tg)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7480) {
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7481) 	u64 quota_us;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7482) 
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 7483) 	if (tg->cfs_bandwidth.quota == RUNTIME_INF)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7484) 		return -1;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7485) 
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 7486) 	quota_us = tg->cfs_bandwidth.quota;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7487) 	do_div(quota_us, NSEC_PER_USEC);
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7488) 
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7489) 	return quota_us;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7490) }
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7491) 
b1546edcf2aab kernel/sched/core.c (YueHaibing                 2019-04-18 22:47:13 +0800 7492) static int tg_set_cfs_period(struct task_group *tg, long cfs_period_us)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7493) {
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7494) 	u64 quota, period;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7495) 
1a8b4540db732 kernel/sched/core.c (Konstantin Khlebnikov      2019-02-27 11:10:20 +0300 7496) 	if ((u64)cfs_period_us > U64_MAX / NSEC_PER_USEC)
1a8b4540db732 kernel/sched/core.c (Konstantin Khlebnikov      2019-02-27 11:10:20 +0300 7497) 		return -EINVAL;
1a8b4540db732 kernel/sched/core.c (Konstantin Khlebnikov      2019-02-27 11:10:20 +0300 7498) 
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7499) 	period = (u64)cfs_period_us * NSEC_PER_USEC;
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 7500) 	quota = tg->cfs_bandwidth.quota;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7501) 
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7502) 	return tg_set_cfs_bandwidth(tg, period, quota);
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7503) }
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7504) 
b1546edcf2aab kernel/sched/core.c (YueHaibing                 2019-04-18 22:47:13 +0800 7505) static long tg_get_cfs_period(struct task_group *tg)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7506) {
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7507) 	u64 cfs_period_us;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7508) 
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 7509) 	cfs_period_us = ktime_to_ns(tg->cfs_bandwidth.period);
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7510) 	do_div(cfs_period_us, NSEC_PER_USEC);
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7511) 
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7512) 	return cfs_period_us;
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7513) }
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7514) 
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7515) static s64 cpu_cfs_quota_read_s64(struct cgroup_subsys_state *css,
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7516) 				  struct cftype *cft)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7517) {
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7518) 	return tg_get_cfs_quota(css_tg(css));
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7519) }
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7520) 
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7521) static int cpu_cfs_quota_write_s64(struct cgroup_subsys_state *css,
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7522) 				   struct cftype *cftype, s64 cfs_quota_us)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7523) {
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7524) 	return tg_set_cfs_quota(css_tg(css), cfs_quota_us);
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7525) }
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7526) 
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7527) static u64 cpu_cfs_period_read_u64(struct cgroup_subsys_state *css,
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7528) 				   struct cftype *cft)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7529) {
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7530) 	return tg_get_cfs_period(css_tg(css));
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7531) }
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7532) 
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7533) static int cpu_cfs_period_write_u64(struct cgroup_subsys_state *css,
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7534) 				    struct cftype *cftype, u64 cfs_period_us)
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7535) {
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7536) 	return tg_set_cfs_period(css_tg(css), cfs_period_us);
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7537) }
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7538) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7539) struct cfs_schedulable_data {
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7540) 	struct task_group *tg;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7541) 	u64 period, quota;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7542) };
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7543) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7544) /*
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7545)  * normalize group quota/period to be quota/max_period
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7546)  * note: units are usecs
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7547)  */
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7548) static u64 normalize_cfs_quota(struct task_group *tg,
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7549) 			       struct cfs_schedulable_data *d)
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7550) {
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7551) 	u64 quota, period;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7552) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7553) 	if (tg == d->tg) {
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7554) 		period = d->period;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7555) 		quota = d->quota;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7556) 	} else {
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7557) 		period = tg_get_cfs_period(tg);
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7558) 		quota = tg_get_cfs_quota(tg);
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7559) 	}
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7560) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7561) 	/* note: these should typically be equivalent */
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7562) 	if (quota == RUNTIME_INF || quota == -1)
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7563) 		return RUNTIME_INF;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7564) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7565) 	return to_ratio(period, quota);
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7566) }
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7567) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7568) static int tg_cfs_schedulable_down(struct task_group *tg, void *data)
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7569) {
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7570) 	struct cfs_schedulable_data *d = data;
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 7571) 	struct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7572) 	s64 quota = 0, parent_quota = -1;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7573) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7574) 	if (!tg->parent) {
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7575) 		quota = RUNTIME_INF;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7576) 	} else {
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 7577) 		struct cfs_bandwidth *parent_b = &tg->parent->cfs_bandwidth;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7578) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7579) 		quota = normalize_cfs_quota(tg, d);
9c58c79a8a76c kernel/sched/core.c (Zhihui Zhang               2014-09-20 21:24:36 -0400 7580) 		parent_quota = parent_b->hierarchical_quota;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7581) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7582) 		/*
c53593e5cb693 kernel/sched/core.c (Tejun Heo                  2018-01-22 11:26:18 -0800 7583) 		 * Ensure max(child_quota) <= parent_quota.  On cgroup2,
c53593e5cb693 kernel/sched/core.c (Tejun Heo                  2018-01-22 11:26:18 -0800 7584) 		 * always take the min.  On cgroup1, only inherit when no
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 7585) 		 * limit is set:
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7586) 		 */
c53593e5cb693 kernel/sched/core.c (Tejun Heo                  2018-01-22 11:26:18 -0800 7587) 		if (cgroup_subsys_on_dfl(cpu_cgrp_subsys)) {
c53593e5cb693 kernel/sched/core.c (Tejun Heo                  2018-01-22 11:26:18 -0800 7588) 			quota = min(quota, parent_quota);
c53593e5cb693 kernel/sched/core.c (Tejun Heo                  2018-01-22 11:26:18 -0800 7589) 		} else {
c53593e5cb693 kernel/sched/core.c (Tejun Heo                  2018-01-22 11:26:18 -0800 7590) 			if (quota == RUNTIME_INF)
c53593e5cb693 kernel/sched/core.c (Tejun Heo                  2018-01-22 11:26:18 -0800 7591) 				quota = parent_quota;
c53593e5cb693 kernel/sched/core.c (Tejun Heo                  2018-01-22 11:26:18 -0800 7592) 			else if (parent_quota != RUNTIME_INF && quota > parent_quota)
c53593e5cb693 kernel/sched/core.c (Tejun Heo                  2018-01-22 11:26:18 -0800 7593) 				return -EINVAL;
c53593e5cb693 kernel/sched/core.c (Tejun Heo                  2018-01-22 11:26:18 -0800 7594) 		}
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7595) 	}
9c58c79a8a76c kernel/sched/core.c (Zhihui Zhang               2014-09-20 21:24:36 -0400 7596) 	cfs_b->hierarchical_quota = quota;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7597) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7598) 	return 0;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7599) }
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7600) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7601) static int __cfs_schedulable(struct task_group *tg, u64 period, u64 quota)
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7602) {
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700 7603) 	int ret;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7604) 	struct cfs_schedulable_data data = {
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7605) 		.tg = tg,
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7606) 		.period = period,
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7607) 		.quota = quota,
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7608) 	};
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7609) 
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7610) 	if (quota != RUNTIME_INF) {
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7611) 		do_div(data.period, NSEC_PER_USEC);
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7612) 		do_div(data.quota, NSEC_PER_USEC);
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7613) 	}
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7614) 
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700 7615) 	rcu_read_lock();
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700 7616) 	ret = walk_tg_tree(tg_cfs_schedulable_down, tg_nop, &data);
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700 7617) 	rcu_read_unlock();
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700 7618) 
8277434ef1202 kernel/sched.c      (Paul Turner                2011-07-21 09:43:35 -0700 7619) 	return ret;
a790de99599a2 kernel/sched.c      (Paul Turner                2011-07-21 09:43:29 -0700 7620) }
e8da1b18b3206 kernel/sched.c      (Nikhil Rao                 2011-07-21 09:43:40 -0700 7621) 
a1f7164c7b8b0 kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:18 -0700 7622) static int cpu_cfs_stat_show(struct seq_file *sf, void *v)
e8da1b18b3206 kernel/sched.c      (Nikhil Rao                 2011-07-21 09:43:40 -0700 7623) {
2da8ca822d49c kernel/sched/core.c (Tejun Heo                  2013-12-05 12:28:04 -0500 7624) 	struct task_group *tg = css_tg(seq_css(sf));
029632fbb7b7c kernel/sched.c      (Peter Zijlstra             2011-10-25 10:00:11 +0200 7625) 	struct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;
e8da1b18b3206 kernel/sched.c      (Nikhil Rao                 2011-07-21 09:43:40 -0700 7626) 
44ffc75ba9a63 kernel/sched/core.c (Tejun Heo                  2013-12-05 12:28:01 -0500 7627) 	seq_printf(sf, "nr_periods %d\n", cfs_b->nr_periods);
44ffc75ba9a63 kernel/sched/core.c (Tejun Heo                  2013-12-05 12:28:01 -0500 7628) 	seq_printf(sf, "nr_throttled %d\n", cfs_b->nr_throttled);
44ffc75ba9a63 kernel/sched/core.c (Tejun Heo                  2013-12-05 12:28:01 -0500 7629) 	seq_printf(sf, "throttled_time %llu\n", cfs_b->throttled_time);
e8da1b18b3206 kernel/sched.c      (Nikhil Rao                 2011-07-21 09:43:40 -0700 7630) 
3d6c50c27bd64 kernel/sched/core.c (Yun Wang                   2018-07-04 11:27:27 +0800 7631) 	if (schedstat_enabled() && tg != &root_task_group) {
3d6c50c27bd64 kernel/sched/core.c (Yun Wang                   2018-07-04 11:27:27 +0800 7632) 		u64 ws = 0;
3d6c50c27bd64 kernel/sched/core.c (Yun Wang                   2018-07-04 11:27:27 +0800 7633) 		int i;
3d6c50c27bd64 kernel/sched/core.c (Yun Wang                   2018-07-04 11:27:27 +0800 7634) 
3d6c50c27bd64 kernel/sched/core.c (Yun Wang                   2018-07-04 11:27:27 +0800 7635) 		for_each_possible_cpu(i)
3d6c50c27bd64 kernel/sched/core.c (Yun Wang                   2018-07-04 11:27:27 +0800 7636) 			ws += schedstat_val(tg->se[i]->statistics.wait_sum);
3d6c50c27bd64 kernel/sched/core.c (Yun Wang                   2018-07-04 11:27:27 +0800 7637) 
3d6c50c27bd64 kernel/sched/core.c (Yun Wang                   2018-07-04 11:27:27 +0800 7638) 		seq_printf(sf, "wait_sum %llu\n", ws);
3d6c50c27bd64 kernel/sched/core.c (Yun Wang                   2018-07-04 11:27:27 +0800 7639) 	}
3d6c50c27bd64 kernel/sched/core.c (Yun Wang                   2018-07-04 11:27:27 +0800 7640) 
e8da1b18b3206 kernel/sched.c      (Nikhil Rao                 2011-07-21 09:43:40 -0700 7641) 	return 0;
e8da1b18b3206 kernel/sched.c      (Nikhil Rao                 2011-07-21 09:43:40 -0700 7642) }
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7643) #endif /* CONFIG_CFS_BANDWIDTH */
6d6bc0ad867c4 kernel/sched.c      (Dhaval Giani               2008-05-30 14:23:45 +0200 7644) #endif /* CONFIG_FAIR_GROUP_SCHED */
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7645) 
052f1dc7eb023 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 7646) #ifdef CONFIG_RT_GROUP_SCHED
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7647) static int cpu_rt_runtime_write(struct cgroup_subsys_state *css,
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7648) 				struct cftype *cft, s64 val)
6f505b16425a5 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:30 +0100 7649) {
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7650) 	return sched_group_set_rt_runtime(css_tg(css), val);
6f505b16425a5 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:30 +0100 7651) }
6f505b16425a5 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:30 +0100 7652) 
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7653) static s64 cpu_rt_runtime_read(struct cgroup_subsys_state *css,
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7654) 			       struct cftype *cft)
6f505b16425a5 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:30 +0100 7655) {
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7656) 	return sched_group_rt_runtime(css_tg(css));
6f505b16425a5 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:30 +0100 7657) }
d0b27fa77854b kernel/sched.c      (Peter Zijlstra             2008-04-19 19:44:57 +0200 7658) 
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7659) static int cpu_rt_period_write_uint(struct cgroup_subsys_state *css,
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7660) 				    struct cftype *cftype, u64 rt_period_us)
d0b27fa77854b kernel/sched.c      (Peter Zijlstra             2008-04-19 19:44:57 +0200 7661) {
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7662) 	return sched_group_set_rt_period(css_tg(css), rt_period_us);
d0b27fa77854b kernel/sched.c      (Peter Zijlstra             2008-04-19 19:44:57 +0200 7663) }
d0b27fa77854b kernel/sched.c      (Peter Zijlstra             2008-04-19 19:44:57 +0200 7664) 
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7665) static u64 cpu_rt_period_read_uint(struct cgroup_subsys_state *css,
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7666) 				   struct cftype *cft)
d0b27fa77854b kernel/sched.c      (Peter Zijlstra             2008-04-19 19:44:57 +0200 7667) {
182446d087906 kernel/sched/core.c (Tejun Heo                  2013-08-08 20:11:24 -0400 7668) 	return sched_group_rt_period(css_tg(css));
d0b27fa77854b kernel/sched.c      (Peter Zijlstra             2008-04-19 19:44:57 +0200 7669) }
6d6bc0ad867c4 kernel/sched.c      (Dhaval Giani               2008-05-30 14:23:45 +0200 7670) #endif /* CONFIG_RT_GROUP_SCHED */
6f505b16425a5 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:30 +0100 7671) 
a1f7164c7b8b0 kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:18 -0700 7672) static struct cftype cpu_legacy_files[] = {
052f1dc7eb023 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 7673) #ifdef CONFIG_FAIR_GROUP_SCHED
fe5c7cc22897b kernel/sched.c      (Paul Menage                2007-10-29 21:18:11 +0100 7674) 	{
fe5c7cc22897b kernel/sched.c      (Paul Menage                2007-10-29 21:18:11 +0100 7675) 		.name = "shares",
f4c753b7eacc2 kernel/sched.c      (Paul Menage                2008-04-29 00:59:56 -0700 7676) 		.read_u64 = cpu_shares_read_u64,
f4c753b7eacc2 kernel/sched.c      (Paul Menage                2008-04-29 00:59:56 -0700 7677) 		.write_u64 = cpu_shares_write_u64,
fe5c7cc22897b kernel/sched.c      (Paul Menage                2007-10-29 21:18:11 +0100 7678) 	},
052f1dc7eb023 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 7679) #endif
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7680) #ifdef CONFIG_CFS_BANDWIDTH
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7681) 	{
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7682) 		.name = "cfs_quota_us",
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7683) 		.read_s64 = cpu_cfs_quota_read_s64,
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7684) 		.write_s64 = cpu_cfs_quota_write_s64,
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7685) 	},
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7686) 	{
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7687) 		.name = "cfs_period_us",
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7688) 		.read_u64 = cpu_cfs_period_read_u64,
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7689) 		.write_u64 = cpu_cfs_period_write_u64,
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7690) 	},
e8da1b18b3206 kernel/sched.c      (Nikhil Rao                 2011-07-21 09:43:40 -0700 7691) 	{
e8da1b18b3206 kernel/sched.c      (Nikhil Rao                 2011-07-21 09:43:40 -0700 7692) 		.name = "stat",
a1f7164c7b8b0 kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:18 -0700 7693) 		.seq_show = cpu_cfs_stat_show,
e8da1b18b3206 kernel/sched.c      (Nikhil Rao                 2011-07-21 09:43:40 -0700 7694) 	},
ab84d31e15502 kernel/sched.c      (Paul Turner                2011-07-21 09:43:28 -0700 7695) #endif
052f1dc7eb023 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 7696) #ifdef CONFIG_RT_GROUP_SCHED
6f505b16425a5 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:30 +0100 7697) 	{
9f0c1e560c433 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:39 +0100 7698) 		.name = "rt_runtime_us",
06ecb27cfbf53 kernel/sched.c      (Paul Menage                2008-04-29 01:00:06 -0700 7699) 		.read_s64 = cpu_rt_runtime_read,
06ecb27cfbf53 kernel/sched.c      (Paul Menage                2008-04-29 01:00:06 -0700 7700) 		.write_s64 = cpu_rt_runtime_write,
6f505b16425a5 kernel/sched.c      (Peter Zijlstra             2008-01-25 21:08:30 +0100 7701) 	},
d0b27fa77854b kernel/sched.c      (Peter Zijlstra             2008-04-19 19:44:57 +0200 7702) 	{
d0b27fa77854b kernel/sched.c      (Peter Zijlstra             2008-04-19 19:44:57 +0200 7703) 		.name = "rt_period_us",
f4c753b7eacc2 kernel/sched.c      (Paul Menage                2008-04-29 00:59:56 -0700 7704) 		.read_u64 = cpu_rt_period_read_uint,
f4c753b7eacc2 kernel/sched.c      (Paul Menage                2008-04-29 00:59:56 -0700 7705) 		.write_u64 = cpu_rt_period_write_uint,
d0b27fa77854b kernel/sched.c      (Peter Zijlstra             2008-04-19 19:44:57 +0200 7706) 	},
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7707) #endif
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7708) #ifdef CONFIG_UCLAMP_TASK_GROUP
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7709) 	{
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7710) 		.name = "uclamp.min",
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7711) 		.flags = CFTYPE_NOT_ON_ROOT,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7712) 		.seq_show = cpu_uclamp_min_show,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7713) 		.write = cpu_uclamp_min_write,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7714) 	},
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7715) 	{
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7716) 		.name = "uclamp.max",
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7717) 		.flags = CFTYPE_NOT_ON_ROOT,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7718) 		.seq_show = cpu_uclamp_max_show,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7719) 		.write = cpu_uclamp_max_write,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7720) 	},
052f1dc7eb023 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 7721) #endif
d1ccc66df8bfe kernel/sched/core.c (Ingo Molnar                2017-02-01 11:46:42 +0100 7722) 	{ }	/* Terminate */
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7723) };
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7724) 
d41bf8c9deaed kernel/sched/core.c (Tejun Heo                  2017-10-23 16:18:27 -0700 7725) static int cpu_extra_stat_show(struct seq_file *sf,
d41bf8c9deaed kernel/sched/core.c (Tejun Heo                  2017-10-23 16:18:27 -0700 7726) 			       struct cgroup_subsys_state *css)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7727) {
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7728) #ifdef CONFIG_CFS_BANDWIDTH
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7729) 	{
d41bf8c9deaed kernel/sched/core.c (Tejun Heo                  2017-10-23 16:18:27 -0700 7730) 		struct task_group *tg = css_tg(css);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7731) 		struct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7732) 		u64 throttled_usec;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7733) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7734) 		throttled_usec = cfs_b->throttled_time;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7735) 		do_div(throttled_usec, NSEC_PER_USEC);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7736) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7737) 		seq_printf(sf, "nr_periods %d\n"
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7738) 			   "nr_throttled %d\n"
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7739) 			   "throttled_usec %llu\n",
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7740) 			   cfs_b->nr_periods, cfs_b->nr_throttled,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7741) 			   throttled_usec);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7742) 	}
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7743) #endif
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7744) 	return 0;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7745) }
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7746) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7747) #ifdef CONFIG_FAIR_GROUP_SCHED
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7748) static u64 cpu_weight_read_u64(struct cgroup_subsys_state *css,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7749) 			       struct cftype *cft)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7750) {
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7751) 	struct task_group *tg = css_tg(css);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7752) 	u64 weight = scale_load_down(tg->shares);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7753) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7754) 	return DIV_ROUND_CLOSEST_ULL(weight * CGROUP_WEIGHT_DFL, 1024);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7755) }
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7756) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7757) static int cpu_weight_write_u64(struct cgroup_subsys_state *css,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7758) 				struct cftype *cft, u64 weight)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7759) {
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7760) 	/*
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7761) 	 * cgroup weight knobs should use the common MIN, DFL and MAX
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7762) 	 * values which are 1, 100 and 10000 respectively.  While it loses
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7763) 	 * a bit of range on both ends, it maps pretty well onto the shares
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7764) 	 * value used by scheduler and the round-trip conversions preserve
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7765) 	 * the original value over the entire range.
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7766) 	 */
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7767) 	if (weight < CGROUP_WEIGHT_MIN || weight > CGROUP_WEIGHT_MAX)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7768) 		return -ERANGE;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7769) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7770) 	weight = DIV_ROUND_CLOSEST_ULL(weight * 1024, CGROUP_WEIGHT_DFL);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7771) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7772) 	return sched_group_set_shares(css_tg(css), scale_load(weight));
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7773) }
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7774) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7775) static s64 cpu_weight_nice_read_s64(struct cgroup_subsys_state *css,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7776) 				    struct cftype *cft)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7777) {
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7778) 	unsigned long weight = scale_load_down(css_tg(css)->shares);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7779) 	int last_delta = INT_MAX;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7780) 	int prio, delta;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7781) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7782) 	/* find the closest nice value to the current weight */
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7783) 	for (prio = 0; prio < ARRAY_SIZE(sched_prio_to_weight); prio++) {
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7784) 		delta = abs(sched_prio_to_weight[prio] - weight);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7785) 		if (delta >= last_delta)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7786) 			break;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7787) 		last_delta = delta;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7788) 	}
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7789) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7790) 	return PRIO_TO_NICE(prio - 1 + MAX_RT_PRIO);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7791) }
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7792) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7793) static int cpu_weight_nice_write_s64(struct cgroup_subsys_state *css,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7794) 				     struct cftype *cft, s64 nice)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7795) {
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7796) 	unsigned long weight;
7281c8dec8a87 kernel/sched/core.c (Peter Zijlstra             2018-04-20 14:29:51 +0200 7797) 	int idx;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7798) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7799) 	if (nice < MIN_NICE || nice > MAX_NICE)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7800) 		return -ERANGE;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7801) 
7281c8dec8a87 kernel/sched/core.c (Peter Zijlstra             2018-04-20 14:29:51 +0200 7802) 	idx = NICE_TO_PRIO(nice) - MAX_RT_PRIO;
7281c8dec8a87 kernel/sched/core.c (Peter Zijlstra             2018-04-20 14:29:51 +0200 7803) 	idx = array_index_nospec(idx, 40);
7281c8dec8a87 kernel/sched/core.c (Peter Zijlstra             2018-04-20 14:29:51 +0200 7804) 	weight = sched_prio_to_weight[idx];
7281c8dec8a87 kernel/sched/core.c (Peter Zijlstra             2018-04-20 14:29:51 +0200 7805) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7806) 	return sched_group_set_shares(css_tg(css), scale_load(weight));
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7807) }
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7808) #endif
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7809) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7810) static void __maybe_unused cpu_period_quota_print(struct seq_file *sf,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7811) 						  long period, long quota)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7812) {
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7813) 	if (quota < 0)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7814) 		seq_puts(sf, "max");
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7815) 	else
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7816) 		seq_printf(sf, "%ld", quota);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7817) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7818) 	seq_printf(sf, " %ld\n", period);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7819) }
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7820) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7821) /* caller should put the current value in *@periodp before calling */
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7822) static int __maybe_unused cpu_period_quota_parse(char *buf,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7823) 						 u64 *periodp, u64 *quotap)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7824) {
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7825) 	char tok[21];	/* U64_MAX */
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7826) 
4c47acd824aaa kernel/sched/core.c (Konstantin Khlebnikov      2019-03-06 20:11:42 +0300 7827) 	if (sscanf(buf, "%20s %llu", tok, periodp) < 1)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7828) 		return -EINVAL;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7829) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7830) 	*periodp *= NSEC_PER_USEC;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7831) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7832) 	if (sscanf(tok, "%llu", quotap))
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7833) 		*quotap *= NSEC_PER_USEC;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7834) 	else if (!strcmp(tok, "max"))
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7835) 		*quotap = RUNTIME_INF;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7836) 	else
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7837) 		return -EINVAL;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7838) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7839) 	return 0;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7840) }
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7841) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7842) #ifdef CONFIG_CFS_BANDWIDTH
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7843) static int cpu_max_show(struct seq_file *sf, void *v)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7844) {
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7845) 	struct task_group *tg = css_tg(seq_css(sf));
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7846) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7847) 	cpu_period_quota_print(sf, tg_get_cfs_period(tg), tg_get_cfs_quota(tg));
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7848) 	return 0;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7849) }
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7850) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7851) static ssize_t cpu_max_write(struct kernfs_open_file *of,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7852) 			     char *buf, size_t nbytes, loff_t off)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7853) {
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7854) 	struct task_group *tg = css_tg(of_css(of));
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7855) 	u64 period = tg_get_cfs_period(tg);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7856) 	u64 quota;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7857) 	int ret;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7858) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7859) 	ret = cpu_period_quota_parse(buf, &period, &quota);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7860) 	if (!ret)
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7861) 		ret = tg_set_cfs_bandwidth(tg, period, quota);
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7862) 	return ret ?: nbytes;
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7863) }
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7864) #endif
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7865) 
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7866) static struct cftype cpu_files[] = {
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7867) #ifdef CONFIG_FAIR_GROUP_SCHED
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7868) 	{
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7869) 		.name = "weight",
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7870) 		.flags = CFTYPE_NOT_ON_ROOT,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7871) 		.read_u64 = cpu_weight_read_u64,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7872) 		.write_u64 = cpu_weight_write_u64,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7873) 	},
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7874) 	{
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7875) 		.name = "weight.nice",
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7876) 		.flags = CFTYPE_NOT_ON_ROOT,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7877) 		.read_s64 = cpu_weight_nice_read_s64,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7878) 		.write_s64 = cpu_weight_nice_write_s64,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7879) 	},
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7880) #endif
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7881) #ifdef CONFIG_CFS_BANDWIDTH
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7882) 	{
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7883) 		.name = "max",
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7884) 		.flags = CFTYPE_NOT_ON_ROOT,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7885) 		.seq_show = cpu_max_show,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7886) 		.write = cpu_max_write,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7887) 	},
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7888) #endif
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7889) #ifdef CONFIG_UCLAMP_TASK_GROUP
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7890) 	{
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7891) 		.name = "uclamp.min",
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7892) 		.flags = CFTYPE_NOT_ON_ROOT,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7893) 		.seq_show = cpu_uclamp_min_show,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7894) 		.write = cpu_uclamp_min_write,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7895) 	},
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7896) 	{
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7897) 		.name = "uclamp.max",
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7898) 		.flags = CFTYPE_NOT_ON_ROOT,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7899) 		.seq_show = cpu_uclamp_max_show,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7900) 		.write = cpu_uclamp_max_write,
2480c093130f6 kernel/sched/core.c (Patrick Bellasi            2019-08-22 14:28:06 +0100 7901) 	},
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7902) #endif
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7903) 	{ }	/* terminate */
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7904) };
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7905) 
073219e995b4a kernel/sched/core.c (Tejun Heo                  2014-02-08 10:36:58 -0500 7906) struct cgroup_subsys cpu_cgrp_subsys = {
92fb97487a7e4 kernel/sched/core.c (Tejun Heo                  2012-11-19 08:13:38 -0800 7907) 	.css_alloc	= cpu_cgroup_css_alloc,
96b777452d888 kernel/sched/core.c (Konstantin Khlebnikov      2017-02-08 14:27:27 +0300 7908) 	.css_online	= cpu_cgroup_css_online,
2f5177f0fd7e5 kernel/sched/core.c (Peter Zijlstra             2016-03-16 16:22:45 +0100 7909) 	.css_released	= cpu_cgroup_css_released,
92fb97487a7e4 kernel/sched/core.c (Tejun Heo                  2012-11-19 08:13:38 -0800 7910) 	.css_free	= cpu_cgroup_css_free,
d41bf8c9deaed kernel/sched/core.c (Tejun Heo                  2017-10-23 16:18:27 -0700 7911) 	.css_extra_stat_show = cpu_extra_stat_show,
eeb61e53ea19b kernel/sched/core.c (Kirill Tkhai               2014-10-27 14:18:25 +0400 7912) 	.fork		= cpu_cgroup_fork,
bb9d97b6dffa1 kernel/sched.c      (Tejun Heo                  2011-12-12 18:12:21 -0800 7913) 	.can_attach	= cpu_cgroup_can_attach,
bb9d97b6dffa1 kernel/sched.c      (Tejun Heo                  2011-12-12 18:12:21 -0800 7914) 	.attach		= cpu_cgroup_attach,
a1f7164c7b8b0 kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:18 -0700 7915) 	.legacy_cftypes	= cpu_legacy_files,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7916) 	.dfl_cftypes	= cpu_files,
b38e42e962dbc kernel/sched/core.c (Tejun Heo                  2016-02-23 10:00:50 -0500 7917) 	.early_init	= true,
0d5936344f30a kernel/sched/core.c (Tejun Heo                  2017-09-25 09:00:19 -0700 7918) 	.threaded	= true,
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7919) };
68318b8e0b61f kernel/sched.c      (Srivatsa Vaddagiri         2007-10-18 23:41:03 -0700 7920) 
052f1dc7eb023 kernel/sched.c      (Peter Zijlstra             2008-02-13 15:45:40 +0100 7921) #endif	/* CONFIG_CGROUP_SCHED */
d842de871c8c5 kernel/sched.c      (Srivatsa Vaddagiri         2007-12-02 20:04:49 +0100 7922) 
b637a328bd4f4 kernel/sched/core.c (Paul E. McKenney           2012-09-19 16:58:38 -0700 7923) void dump_cpu_task(int cpu)
b637a328bd4f4 kernel/sched/core.c (Paul E. McKenney           2012-09-19 16:58:38 -0700 7924) {
b637a328bd4f4 kernel/sched/core.c (Paul E. McKenney           2012-09-19 16:58:38 -0700 7925) 	pr_info("Task dump for CPU %d:\n", cpu);
b637a328bd4f4 kernel/sched/core.c (Paul E. McKenney           2012-09-19 16:58:38 -0700 7926) 	sched_show_task(cpu_curr(cpu));
b637a328bd4f4 kernel/sched/core.c (Paul E. McKenney           2012-09-19 16:58:38 -0700 7927) }
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7928) 
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7929) /*
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7930)  * Nice levels are multiplicative, with a gentle 10% change for every
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7931)  * nice level changed. I.e. when a CPU-bound task goes from nice 0 to
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7932)  * nice 1, it will get ~10% less CPU time than another CPU-bound task
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7933)  * that remained on nice 0.
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7934)  *
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7935)  * The "10% effect" is relative and cumulative: from _any_ nice level,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7936)  * if you go up 1 level, it's -10% CPU usage, if you go down 1 level
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7937)  * it's +10% CPU usage. (to achieve that we use a multiplier of 1.25.
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7938)  * If a task goes up by ~10% and another task goes down by ~10% then
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7939)  * the relative distance between them is ~25%.)
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7940)  */
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7941) const int sched_prio_to_weight[40] = {
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7942)  /* -20 */     88761,     71755,     56483,     46273,     36291,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7943)  /* -15 */     29154,     23254,     18705,     14949,     11916,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7944)  /* -10 */      9548,      7620,      6100,      4904,      3906,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7945)  /*  -5 */      3121,      2501,      1991,      1586,      1277,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7946)  /*   0 */      1024,       820,       655,       526,       423,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7947)  /*   5 */       335,       272,       215,       172,       137,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7948)  /*  10 */       110,        87,        70,        56,        45,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7949)  /*  15 */        36,        29,        23,        18,        15,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7950) };
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7951) 
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7952) /*
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7953)  * Inverse (2^32/x) values of the sched_prio_to_weight[] array, precalculated.
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7954)  *
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7955)  * In cases where the weight does not change often, we can use the
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7956)  * precalculated inverse to speed up arithmetics by turning divisions
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7957)  * into multiplications:
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7958)  */
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7959) const u32 sched_prio_to_wmult[40] = {
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7960)  /* -20 */     48388,     59856,     76040,     92818,    118348,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7961)  /* -15 */    147320,    184698,    229616,    287308,    360437,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7962)  /* -10 */    449829,    563644,    704093,    875809,   1099582,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7963)  /*  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7964)  /*   0 */   4194304,   5237765,   6557202,   8165337,  10153587,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7965)  /*   5 */  12820798,  15790321,  19976592,  24970740,  31350126,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7966)  /*  10 */  39045157,  49367440,  61356676,  76695844,  95443717,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7967)  /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,
ed82b8a1ff76e kernel/sched/core.c (Andi Kleen                 2015-11-29 20:59:43 -0800 7968) };
14a7405b2e814 kernel/sched/core.c (Ingo Molnar                2018-03-03 16:32:24 +0100 7969) 
14a7405b2e814 kernel/sched/core.c (Ingo Molnar                2018-03-03 16:32:24 +0100 7970) #undef CREATE_TRACE_POINTS
